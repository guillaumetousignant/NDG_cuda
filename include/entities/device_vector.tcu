#include <stdexcept>
#include <thrust/swap.h>

template <typename T>
template<typename TI>
__host__ __device__
SEM::Entities::device_vector<T>::device_vector(TI size) : size_(size) {
    cudaMalloc(&data_, size_ * sizeof(T));
}

template <typename T>
template<typename TI>
__host__
SEM::Entities::device_vector<T>::device_vector(TI size, const cudaStream_t& stream) : size_(size) {
    cudaMallocAsync(&data_, size_ * sizeof(T), stream);
}

template <typename T>
__host__ __device__
SEM::Entities::device_vector<T>::device_vector() : size_(0) {
    data_ = nullptr;
}

template <typename T>
__host__ __device__
SEM::Entities::device_vector<T>::~device_vector() {
    if (data_ != nullptr) {
        destroy();
    }
    cudaFree(data_);
}

template <typename T>
__host__ __device__
SEM::Entities::device_vector<T>::device_vector(const device_vector<T>& other) : size_(other.size()) { // copy constructor
    cudaMalloc(&data_, size_ * sizeof(T));
    #ifdef  __CUDA_ARCH__
    memcpy(data_, other.data(), size_ * sizeof(T));
    #else
    cudaMemcpy(data_, other.data(), size_ * sizeof(T), cudaMemcpyDeviceToDevice); // Apparently slower than using a kernel
    #endif
}

template <typename T>
__host__
SEM::Entities::device_vector<T>::device_vector(const device_vector<T>& other, const cudaStream_t& stream) : size_(other.size()) { // copy constructor
    cudaMallocAsync(&data_, size_ * sizeof(T), stream);
    cudaMemcpyAsync(data_, other.data(), size_ * sizeof(T), cudaMemcpyDeviceToDevice, stream); // Apparently slower than using a kernel
}

template <typename T>
__host__
SEM::Entities::device_vector<T>::device_vector(const std::vector<T>& other) : size_(other.size()) { // copy constructor
    cudaMalloc(&data_, size_ * sizeof(T));
    cudaMemcpy(data_, other.data(), size_ * sizeof(T), cudaMemcpyHostToDevice);
}

template <typename T>
__host__
SEM::Entities::device_vector<T>::device_vector(const std::vector<T>& other, const cudaStream_t& stream) : size_(other.size()) { // copy constructor
    cudaMallocAsync(&data_, size_ * sizeof(T), stream);
    cudaMemcpyAsync(data_, other.data(), size_ * sizeof(T), cudaMemcpyHostToDevice, stream);
}

template <typename T>
__host__ __device__
SEM::Entities::device_vector<T>::device_vector(device_vector<T>&& other) noexcept : size_(other.size()) { // move constructor
    data_ = other.data();
    other.data_ = nullptr;
}

template <typename T>
__host__ __device__
auto SEM::Entities::device_vector<T>::operator=(const device_vector<T>& other) -> device_vector& { // copy assignment
    if (&other == this) {
        return *this;
    }
    
    if (size_ != other.size()) {
        cudaFree(data_);
        size_ = other.size();
        cudaMalloc(&data_, size_ * sizeof(T));
    }
    #ifdef  __CUDA_ARCH__
    memcpy(data_, other.data(), size_ * sizeof(T));
    #else
    cudaMemcpy(data_, other.data(), size_ * sizeof(T), cudaMemcpyDeviceToDevice); // Apparently slower than using a kernel
    #endif

    return *this;
}

template <typename T>
__host__
auto SEM::Entities::device_vector<T>::operator=(const std::vector<T>& other) -> device_vector& { // copy assignment
    if (size_ != other.size()) {
        cudaFree(data_);
        size_ = other.size();
        cudaMalloc(&data_, size_ * sizeof(T));
    }

    cudaMemcpy(data_, other.data(), size_ * sizeof(T), cudaMemcpyHostToDevice);

    return *this;
}

template <typename T>
__host__ __device__
auto SEM::Entities::device_vector<T>::operator=(device_vector<T>&& other) noexcept -> device_vector& { // move assignment
    #ifdef  __CUDA_ARCH__
    thrust::swap(data_, other.data_);
    thrust::swap(size_, other.size_);
    #else
    std::swap(data_, other.data_);
    std::swap(size_, other.size_);
    #endif

    return *this;
}

template <typename T>
template<typename TI>
__device__
auto SEM::Entities::device_vector<T>::operator[](TI index) -> T& {
    return data_[index];
}

template <typename T>
template<typename TI>
__device__
auto SEM::Entities::device_vector<T>::operator[](TI index) const -> const T& {
    return data_[index];
}

template <typename T>
__host__ __device__
auto SEM::Entities::device_vector<T>::size() const -> size_t {
    return size_;
}

template <typename T>
__host__ __device__
auto SEM::Entities::device_vector<T>::data() -> T* {
    return data_;
}

template <typename T>
__host__ __device__
auto SEM::Entities::device_vector<T>::data() const -> const T* {
    return data_;
}

template <typename T>
__host__
auto SEM::Entities::device_vector<T>::copy_from(const std::vector<T>& host_vector) -> void {
    if (size_ != host_vector.size()) {
        throw std::length_error("Host and device vectors must have the same length.");
    }

    cudaMemcpy(data_, host_vector.data(), size_ * sizeof(T), cudaMemcpyHostToDevice);
}

template <typename T>
__host__
auto SEM::Entities::device_vector<T>::copy_from(const std::vector<T>& host_vector, const cudaStream_t& stream) -> void {
    if (size_ != host_vector.size()) {
        throw std::length_error("Host and device vectors must have the same length.");
    }

    cudaMemcpyAsync(data_, host_vector.data(), size_ * sizeof(T), cudaMemcpyHostToDevice, stream);
}

template <typename T>
__host__
auto SEM::Entities::device_vector<T>::copy_to(std::vector<T>& host_vector) const -> void {
    if (size_ != host_vector.size()) {
        throw std::length_error("Host and device vectors must have the same length.");
    }

    cudaMemcpy(host_vector.data(), data_, size_ * sizeof(T), cudaMemcpyDeviceToHost);
}

template <typename T>
__host__
auto SEM::Entities::device_vector<T>::copy_to(std::vector<T>& host_vector, const cudaStream_t& stream) const -> void {
    if (size_ != host_vector.size()) {
        throw std::length_error("Host and device vectors must have the same length.");
    }

    cudaMemcpyAsync(host_vector.data(), data_, size_ * sizeof(T), cudaMemcpyDeviceToHost, stream);
}

template <typename T>
__host__
auto SEM::Entities::device_vector<T>::copy_from(const device_vector<T>& device_vector) -> void {
    if (size_ != device_vector.size()) {
        throw std::length_error("Both vectors must have the same length.");
    }

    cudaMemcpy(data_, device_vector.data(), size_ * sizeof(T), cudaMemcpyDeviceToDevice); // Apparently slower than using a kernel
}

template <typename T>
__host__
auto SEM::Entities::device_vector<T>::copy_from(const device_vector<T>& device_vector, const cudaStream_t& stream) -> void {
    if (size_ != device_vector.size()) {
        throw std::length_error("Both vectors must have the same length.");
    }

    cudaMemcpyAsync(data_, device_vector.data(), size_ * sizeof(T), cudaMemcpyDeviceToDevice, stream); // Apparently slower than using a kernel
}

template <typename T>
__host__
auto SEM::Entities::device_vector<T>::copy_to(device_vector<T>& device_vector) const -> void {
    if (size_ != device_vector.size()) {
        throw std::length_error("Both vectors must have the same length.");
    }

    cudaMemcpy(device_vector.data(), data_, size_ * sizeof(T), cudaMemcpyDeviceToDevice); // Apparently slower than using a kernel
}

template <typename T>
__host__
auto SEM::Entities::device_vector<T>::copy_to(device_vector<T>& device_vector, const cudaStream_t& stream) const -> void {
    if (size_ != device_vector.size()) {
        throw std::length_error("Both vectors must have the same length.");
    }

    cudaMemcpyAsync(device_vector.data(), data_, size_ * sizeof(T), cudaMemcpyDeviceToDevice, stream); // Apparently slower than using a kernel
}

template <typename T>
__host__ __device__
auto SEM::Entities::device_vector<T>::clear() -> void {
    if (data_ != nullptr) {
        destroy();
    }
    cudaFree(data_);
    data_ = nullptr;
    size_ = 0;
}

template <typename T>
__host__ __device__
auto SEM::Entities::device_vector<T>::clear(const cudaStream_t& stream) -> void {
    if (data_ != nullptr) {
        destroy(stream);
    }
    cudaFreeAsync(data_, stream);
    data_ = nullptr;
    size_ = 0;
}

template<typename T>
__global__
auto SEM::Entities::empty_device_vector(size_t size, T* data) -> void {
    const int index = blockIdx.x * blockDim.x + threadIdx.x;
    const int stride = blockDim.x * gridDim.x;

    for (size_t i = index; i < size; i += stride) {
        data[i].~T();
    }
}
