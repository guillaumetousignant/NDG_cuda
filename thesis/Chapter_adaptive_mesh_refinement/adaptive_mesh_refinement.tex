\chapter{Adaptive Mesh Refinement} \label{chapter:adaptive_mesh_refinement} 
% Something about move semantics
% No coarsening
% Adaptivity triggers
% Mortar here

Computers constantly increase in power, thanks to incremental progress made on known processes and
new architectures such as that described in Chapter~\ref{chapter:graphics_processing_units}.
However, processing power and memory is still limited and the size of problems studied has
increased in step with the available resources. It is still necessary to carefully manage those
limited resources in order to maximise the efficiency of simulations. Some flow regions may be more
interesting or harder to compute, benefiting from an increase in resolution. On the other hand, some
flow regions may have less happening in them or be easier to compute, and a decrease in resolution
may be acceptable.

\begin{figure}[H]
	\centering
	\subfloat[Mesh before refining]
	{\includegraphics[width=0.45\textwidth]{Chapter_adaptive_mesh_refinement/media/mesh_initial} \label{fig:mesh_initial}}
	\hfill
	\subfloat[Mesh after refining]
	{\includegraphics[width=0.45\textwidth]{Chapter_adaptive_mesh_refinement/media/mesh_refinement} \label{fig:mesh_after_refinement}}
	\caption{Adaptive mesh refinement: The elements have split near the wave, where the solution is steeper. (a) Before refining (b) After refining}.
	\label{fig:mesh_refinement}
\end{figure}

It is possible to increase the number of elements and/or the polynomial order of an entire mesh
before solving the problem. This increases resolution in important areas of the flow, but also
increases resolution everywhere else in the domain, where the increased computation cost provides no
benefit. It is sometimes possible to predict where to refine before solving the problem, such as
around static shock waves in predictable locations. In these cases, the mesh can be refined in those
areas before computation has started. However, it is not always possible to know these areas
beforehand, or these areas may move as time advances if the problem is transient. The error may also
be higher in unforeseen areas that are not apparent.

Adaptive mesh refinement is the process of mesh resolution as the computation goes, where the effect
of that increase in resolution is most needed. To find out where to refine, the solution error must
be estimated, and a refinement method must be chosen. The error can be estimated by extrapolating
the value of the next mode, and the slope of the last few modes can guide the choice of the
refinement method. The two refinement methods studied here are h-refinement and p-refinement,
increasing the number of elements and the polynomial order, respectively. 

This whole process must be executed in parallel and with as much as the work as possible executed on
the GPU. GPUs are more efficient at working with fixed workloads where all threads of a block
execute the same instruction. Refining the mesh moves memory around and can introduce additional
branching if the elements have different polynomial orders or non-conforming interfaces. The
adaptivity process itself is also hard to implement on GPUs, as elements must either move, change
their polynomial order or split into multiple elements in parallel. If the different threads are not
perfectly coordinated, they could do these operations on the same destination memory location,
creating race conditions.

Once the mesh has been refined, the element boundaries can become geometrically or functionally
non-conforming. To compute fluxes between the elements, we use the mortar element method. Mortar, in
the form of faces, is added between the elements, and the boundary solution is projected from the
elements to the face's own collocation points. 

When solving problems in parallel, the mesh will be split into blocks, each assigned to a process
and its GPU. As the mesh is refined, it is possible that the blocks are refined unequally. This can
lead to a load imbalance between the different GPUs. As the different GPUs need to synchronise at
each timestep, lightly-loaded GPUs will always be waiting after heavily-loaded GPUs. The computation
time will therefore be driven by the most heavily-loaded GPU, and the speedup incurred by
parallelizing the program will be reduced. To improve this side effect of mesh refinement, a dynamic
load balancing algorithm will be implemented in Chapter~\ref{chapter:load_balancing}.

\section{Strategies} \label{section:adaptive_mesh_refinement:adaptivity_strategies}

\subsection{p-adaptivity} \label{section:adaptive_mesh_refinement:adaptivity_strategies:p-adaptivity}

\begin{figure}[H]
	\centering
	\includesvg[width=0.6\textwidth]{Chapter_adaptive_mesh_refinement/media/p-adaptivity_N4_N6}
	\caption{2D p-adaptivity: The element's polynomial order is increased from 4 to 6, with 5 to 7 collocation points}
	\label{fig:p-adaptivity}
\end{figure}

The first refinement strategy is p-adaptivity. When refining this way, elements increase their
polynomial order to increase their resolution. The lower-order solution is projected to the
higher-order collocation points. In this work, the polynomial order in increased in steps of 2, and
is required to be identical in the x and y directions. 

\subsection{h-adaptivity} \label{section:adaptive_mesh_refinement:adaptivity_strategies:h-adaptivity}

\begin{figure}[H]
	\centering
	\includesvg[width=0.6\textwidth]{Chapter_adaptive_mesh_refinement/media/h-adaptivity_N4}
	\caption{2D h-adaptivity: The element splits into 4 elements}
	\label{fig:h-adaptivity}
\end{figure}

The second refinement strategy is h-adaptivity. h-adaptivity splits single elements into several
elements occupying the same space. In this work we exclusively deal with quadrilaterals, and these
elements split in 4 smaller elements when h-refining. The solution is projected from the initial big
element to each of the 4 offspring elements.

\subsection{hp-adaptivity} \label{section:adaptive_mesh_refinement:adaptivity_strategies:hp-adaptivity}

In order to maximise computation efficiency, both p-adaptivity and h-adaptivity are used together.
An hp-adaptive program uses either p-adaptivity or h-adaptivity on an element-by-element basis,
depending on which is expected to reduce error the most. The choice between the two methods is
detailed in Section~\ref{section:adaptive_mesh_refinement:refinement_criteria}.

\begin{figure}[H]
	\centering
	\includesvg[width=0.6\textwidth]{Chapter_adaptive_mesh_refinement/media/hp-adaptivity_N4_N6}
	\caption{2D hp-adaptivity: Elements refining create non-conforming interfaces}
	\label{fig:hp-adaptivity}
\end{figure}

Once the mesh has been refined, it is possible that the interfaces between elements become
non-conforming. Each interface is made up of two element edges, in bold black, and a face, in blue.
Recall from Chapter~\ref{chapter:spectral_element_method} that fluxes are computed between the
elements from the extrapolated values of the solution on collocation points corresponding to the
polynomial order of the elements. These would be sitting on the bold black lines in the figure
above. This works directly if two neighbouring elements have the same polynomial order and they
completely share an edge. However, after p-adaptivity, interfaces can become functionally
non-conforming, as seen with the horizontal blue line in the figure above. The nodes won't match
anymore since one element has more points than the other. After h-adapting, interfaces can become
geometrically non-conforming, as seen with the topmost blue line in the figure above. The nodes do
not match because one element edge is shorter than the other. Finally, interfaces can be both
geometrically and functionally non-conforming, as seen with the bottom blue line in the figure
above. We will have to find a way to connect those elements in a way that retains the spectral
convergence of the method. Such a method is described in
Section~\ref{section:adaptive_mesh_refinement:mortar_element_method}.

\begin{figure}[H]
	\centering
	\subfloat[Mesh before refining]
	{\includegraphics[width=0.45\textwidth]{Chapter_adaptive_mesh_refinement/media/load_imbalance_initial} \label{fig:mesh_imbalance_initial}}
	\hfill
	\subfloat[Mesh after refining]
	{\includegraphics[width=0.45\textwidth]{Chapter_adaptive_mesh_refinement/media/load_imbalance} \label{fig:mesh_imbalance_after_refinement}}
	\caption{Load imbalance: The elements have split unequally in the two worker GPUs, one having a higher computational load. (a) Before refining (b) After refining}.
	\label{fig:load_imbalance}
\end{figure}

One last potential issue with AMR is the appearing of load imbalance. For example, the mesh from
Figure~\ref{fig:mesh_refinement} is split in two blocks for two GPUs to work in parallel, as shown
on Figure~\ref{fig:load_imbalance}. Initially, both the left and right GPUs have the same number of
elements in their domain. After refining, the left GPU has more elements as the wave is in its
domain and more elements have split there. Elements will have to be sent from the left GPU to the
right GPU in order to even out the computational load, while ensuring that there are as few
interfaces as possible between the GPUs. Interfaces need to be minimised because the GPUs need to
communicate every timestep when computing fluxes on these interfaces. This involves transfers from
from the GPUs to their CPU, between CPUs, and finally back from the CPUs to their GPU for both
directions. This is an expensive process, and improving locality of the elements will help reduce
the runtime of that part of the computation. This is dynamic load balancing, and
Chapter~\ref{chapter:load_balancing} is dedicated to solving this problem.

\section{Error Estimation} \label{section:adaptive_mesh_refinement:error_estimation}

We can't simply refine everywhere in the domain at every occasion, else this is no better than
having a finer mesh to start with. The mesh must only be refined where the expected error is higher.
We must estimate the error of each element of the mesh, then refine those elements.

As seen in Chapter~\ref{chapter:spectral_element_method}, we represent the solution as an infinite
series of polynomials. In this case we use the Legendre polynomials $L_n(x)$ of degree $n$, with the
coefficients $a_n$ being the modes of the solution.

\begin{equation} \label{equ:infinite_sum}
	u(x) = \sum_{n = 0}^{\infty}a_n L_n(x)
\end{equation}

It is of course impossible to use an infinite sum on computers, therefore we truncate this sum to
$N$, the polynomial order of the solution. The remaining terms, denoted by $\tau$, are the
truncation error of the solution.

\begin{equation} \label{equ:solution_approximation}
	\begin{aligned}
		u(x) &= \sum_{n = 0}^{\infty }a_n L_n(x) \\
		&= \sum_{n = 0}^{N}a_n L_n(x) + \sum_{n = N + 1}^{\infty }a_n L_n(x) \\ 
		&= \sum_{n = 0}^{N}a_n L_n(x) + \tau \\
		&\approx \sum_{n = 0}^{N}a_n L_n(x)
	\end{aligned}
\end{equation}

We want the norm of that error to estimate the quality of the solution. To find it, we will need
the modes $a_n$ beyond $N$. We will extrapolate the known modes $a_n$. The polynomials we use are
orthogonal, therefore the following relation gives the inner product of two of the polynomials.

\begin{equation} \label{equ:polynomial_orthogonality}
	\begin{aligned}
		\left ( L_n, L_m \right ) &= \int_{-1}^{1}L_n(r)L_m(r)dr \\
		&= \frac{2}{2n + 1}\delta _{nm}
	\end{aligned}
\end{equation}

with $\delta_{nm}$ being the Kronecker delta. We start by computing the known modes by taking the
inner product of the approximate solution $u_h(x)$ and  the corresponding Legendre polynomial
$L_n(x)$.

\begin{equation} \label{equ:modes_inner_product}
	\begin{aligned}
		\left ( u_h, L_n \right ) &= \int_{-1}^{1} u_h(x(r)) L_n(r) dr \\
		&= \sum_{k = 0}^{N} a_k \int_{-1}^{1} L_k(r) L_n(r)dr \\
		&= \frac{2}{2n + 1} a_n
	\end{aligned}
\end{equation}

Rearranging, we get the modes $a_N$.

\begin{equation} \label{equ:modes}
	a_n = \frac{2n + 1}{2}\int_{-1}^{1}u_h(x(r))L_n(r)dr
\end{equation}

We now get predicted modes $\widetilde{a}_n$ from extrapolating our known nodes. The $L2$ norm of
the truncation error can then be approximated.

\begin{equation} \label{equ:error_norm}
	\begin{aligned}
		\left \| \tau \right \| &= \sqrt{\int_{-1}^{1} \sum_{n = N + 1}^{\infty }\left ( \widetilde{a}_n L_n(r) \right )^2 dr} \\
		&= \sqrt{\sum_{n = N + 1}^{\infty }\widetilde{a}_n^2 \int_{-1}^{1} L_n(r) L_n(r) dr}
	\end{aligned}
\end{equation}

We use the orthogonality of the Legendre polynomials shown in
Equation~\ref{equ:polynomial_orthogonality} to obtain a simpler form of the truncation error norm.

\begin{equation} \label{equ:error_norm_1D}
	\left \| \tau \right \| = \sqrt{\sum_{n = N + 1}^{\infty } \frac{\widetilde{a}_{n}^2}{\frac{(2n + 1)}{2}}}
\end{equation}

In 2D, the solution is a tensor product of polynomials, the truncation error changes accordingly. 

\begin{equation} \label{equ:error_norm_2D}
	\left \| \tau \right \| = \sqrt{\sum_{n = N + 1}^{\infty } \sum_{m = M + 1}^{\infty}\frac{\widetilde{a}_{nm}^2}{\frac{(2n + 1)(2m + 1)}{2^2}}}
\end{equation}

As per~\cite{Mavriplis1990}, we examine the decay rate of the modes as a way to estimate the local
error. This method indicates that for a sufficiently high polynomial order, $p \geq 5$, the modes
$\widetilde{a}_n$ can have an exponential decay. We obtain these modes by extrapolating the known
modes. 

\begin{equation} \label{equ:exponential_decay}
	\widetilde{a}_n = Ce^{-\sigma n}
\end{equation}

In order to fit a straight line to the data by linear least-squares, we want a linear relation
between the decay rate $\sigma$ and the modes. We take the logarithm of both sides and get our
linear relation.

\begin{equation} \label{equ:exponential_decay_linear}
	ln(\widetilde{a}_n) = ln(C) - \sigma n
\end{equation}

We apply a linear least-squares fit to the last points of the known modes $a_n$. The slope of the
line, $\sigma$, is the error indicator. It will help choose a refinement strategy in
Section~\ref{section:adaptive_mesh_refinement:refinement_criteria}. The linear least-squares finds a
straight line by minimising the square of the distance between the fitted points and the line. The
resulting line is given by:

\begin{equation}
	y = \alpha + \beta x,
\end{equation}

and its coefficients are computer by:

\begin{gather}
	\beta = \frac{\sum_{i = 1}^{n}(x_i - \overline{x})(y_i - \overline{y})}{\sum_{i = 1}^{n}(x_i -\overline{x})^2}, \\
	\alpha = \overline{y} - \beta \overline{x},
\end{gather}

with $\overline{x}$ and $\overline{y}$ being the $x$ and $y$ averages of the data. The error
indicator becomes $\sigma = - \beta$, and the constant becomes $C = e^\alpha$.

Back to the error estimation, we can replace the sums of \ref{equ:error_norm_1D} and
\ref{equ:error_norm_2D} by integrals and omit the denominators to provide an upper bound.

\begin{align}
	&1D: \quad \tau = \sqrt{\sum_{n = N + 1}^{\infty } \frac{\widetilde{a}_n^2}{\frac{2n + 1}{2}}}, \label{equ:error_1D_sums}
	\\
	&2D: \quad \tau = \sqrt{\sum_{n = N + 1}^{\infty }\sum_{m = M + 1}^{\infty}\frac{\widetilde{a}^2_{nm}}{\frac{(2n + 1)(2m + 1)}{2^2}}} \label{equ:error_2D_sums}
	\\
	&1D: \quad \tau = \sqrt{\int_{N + 1}^{\infty } a_n^2 dn}, \label{equ:error_1D_integrals}
	\\
	&2D: \quad \tau = \sqrt{\int_{M+1}^{\infty }\int_{N + 1 }^{\infty}(a_{nm})^2 dn dm} \label{equ:error_2D_integrals}
\end{align}

By adding the extrapolated modes from Equation~\ref{equ:exponential_decay}, the 1D case can be
evaluated directly.

\begin{equation}
	\begin{aligned}
		\tau_{est} &= \sqrt{\int_{N + 1}^{\infty } a_n^2 dn} \\
		&= \sqrt{\int_{N + 1}^{\infty } \left ( Ce^{-\sigma n} \right )^2 dn} \\
		&= \sqrt{\frac{C^2}{2\sigma }}e^{-\sigma (N + 1)}. 
	\end{aligned}
\end{equation}

The same method cannot be applied to the 2D case, as the modes in Equation
~\ref{equ:error_2D_integrals} are defined in two dimensions. We will need to shape these modes in 1D
to find the error estimate as in the 1D case. We use the same polynomial order in both $x$ and $y$
directions, and create a 1D equivalent modes $\overline{a}_n$.

\begin{equation}
	\overline{a}_n = \left| a_{n,n} \right| + \sum_{i = 0}^{n-1} \left( \left| a_{i, n} \right| + \left| a_{n, i} \right| \right)
\end{equation}

We use that equivalent modes to estimate the error.

\begin{equation}
	\begin{aligned}
		\tau_{est} &= \sqrt{\int_{M+1}^{\infty }\int_{N + 1 }^{\infty}(a_{nm})^2 dn dm} \\
		&= \sqrt{\int_{N + 1}^{\infty }\overline{a}_p^2 dp} \\
		&= \sqrt{\int_{N + 1}^{\infty} Ce^{-2\sigma p}dp} \\
		&= \sqrt{\frac{C}{2\sigma}e^{-2\sigma(N + 1)}} \\
		&= \sqrt{\frac{C}{2\sigma}} e^{-\sigma (N + 1)}
	\end{aligned}
\end{equation}

The error indicator $\sigma$ and $C$ are computed from the 1D equivalent modes $\overline{a}_n$.

It is now possible to refine when the estimated error is past a certain threshold. We still need to
choose one of the refinement strategies from
Section~\ref{section:adaptive_mesh_refinement:adaptivity_strategies}.
Section~\ref{section:adaptive_mesh_refinement:refinement_criteria} describes a method to choose the
most appropriate strategy using the error indicator $\sigma$ we computed previously.

\section{Refinement Criteria} \label{section:adaptive_mesh_refinement:refinement_criteria}

Once we computed the error approximation on every element and know which elements we want to refine,
we must choose in what way to refine them. Recall from
Section~\ref{section:adaptive_mesh_refinement:adaptivity_strategies} that we can choose
p-refinement, increasing the polynomial order $N$ of an element, or h-refinement, splitting the
element in smaller elements to increase the total number of elements $K$.

The method~\cite{Mavriplis1990} used here is to look at the decay rate of the modes $a_n$ of the
solution. The error indicator $\sigma$ we computed in
Section~\ref{section:adaptive_mesh_refinement:error_estimation} is the slope of the last few modes
of the solution. A well-resolved solution should have a fast decay of the modes, with $\sigma > 1$.
On the other hand, $\sigma \leqslant 1$ implies the solution is not well resolved as is.

\begin{figure}[H]
	\centering
	\subfloat[Badly resolved solution]
	{\includegraphics[width=0.4\textwidth]{Chapter_adaptive_mesh_refinement/media/decaying_modes_slow} \label{fig:decaying_modes_slow}}
	\hfill
	\subfloat[Well resolved solution]
	{\includegraphics[width=0.4\textwidth]{Chapter_adaptive_mesh_refinement/media/decaying_modes_fast} \label{fig:decaying_modes_fast}}
	\caption{Modes decaying: How the modes decay in an element, and the fitted line. (a) $\sigma \leqslant 1$ (b) $\sigma > 1$}.
	\label{fig:decaying_modes}
\end{figure}

The truncation error is related to the value of the extrapolated nodes. If the slope of the modes is
steep, each added mode is expected to reduce the error a lot. This is because the truncated modes
have a smaller value, therefore the truncated part of the solution is also small. On the other hand,
a flat slope indicates that the value of the modes does not decay fats, and adding additional modes
is not expected to reduce the truncation error much. This is often the case in non-smooth parts of
the solution.

We choose the slope of the modes, $\sigma$, as our error indicator. If $\sigma > 1$, adding
polynomial modes is expected to reduce the error a lot, so we use p-adaptivity and increase the
polynomial order $N$ of the element. If $\sigma \leqslant 1$, adding more modes is not predicted to
reduce the error by much, therefore we use h-adaptivity and split the element into smaller elements,
increasing $K$.

\section{Mortar Element Method} \label{section:adaptive_mesh_refinement:mortar_element_method}

\cite{Maday1989}

\section{Implementation} \label{section:adaptive_mesh_refinement:implementation}

As detailed in Chapter~\ref{chapter:graphics_processing_units}, the grid is stored on the GPU as a
flat array of elements, each element having pointers to its solution arrays, stored in GPU dynamic
memory. This ensures the elements have a fixed size regardless of their polynomial order.