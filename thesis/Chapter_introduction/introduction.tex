\chapter{Introduction}
Fluid flows are an important part of everyday life. Fluid mechanics is a whole discipline dedicated
to studying these flows. More and more industrial and scientific applications surface everyday, from
great scales like climate studies encompassing the whole earth over millennia, to the study of
microscopic cells flowing through tiny blood vessels. The aerospace industry is probably the most
evident example of how prevalent aerodynamics are in today's world. Before the advent of computers,
there was really only two methods used in engineering fluid dynamics: experimental and theoretical.
Since the widespread use of computers became the norm, a third method appeared:
\textit{computational fluid dynamics \gls{acr:CFD}}. This is a numerical approach to solving the equations
governing those processes.

Usage of CFD has been steadily rising in recent years~\cite{Slotnick2014}. It is easy to imagine
why, when experimental results are so costly to obtain, and a theoretical approach is difficult to
apply to complex cases. Nonetheless, CFD is not meant to replace those methods, but to be combined
with them. Reducing the need to perform experiments, while comparing results with the experiments
that do happen to verify and validate them~\cite{Stern2001}.

Even with increasing computing power, some problems are difficult to solve with CFD. Problems of
high-dimensionality and complex flows still cannot be solved economically. NASA's CFD Vision 2030
study~\cite{Slotnick2014} states that turbulent flow separation is one area that still cannot be
predicted accurately. In order to simulate these problems, we will need increased processing power,
more efficient use of that power, and higher resolution numerical methods.

% DG-SEM
Spectral methods are an answer to that need of more accurate numerical methods. Unlike traditional
methods like finite differences methods, finite element methods and finite volume methods, spectral
methods are high-order methods and converge exponentially fast. Instead of modelling the solution as
single points, or a single to double order function within a space subdivision, spectral methods
represent a function as a truncated series:

\begin{equation}
	u(x) \approx u_N(x) = \sum_{n = 0}^{N} \widehat{u}_n \phi _n(x)
\end{equation}

Where $\phi _n(x)$ are basis functions. We use this representation into the governing equations to
compute the unknowns $\widehat{u}_n$. Spectral methods are usually divided in two
categories~\cite{Karniadakis2005}: collocation methods and Galerkin methods. Galerkin methods solve
governing equations in integral form over the domain~\cite{Reed1973}, which is broken up into
elements. This method, in combination with the spectral approximation, will be used in this work. It
is called the \textit{discontinuous Galerkin spectral element method (DG-SEM)}. The method is
discontinuous because it allows the solution to be discontinuous at element boundaries, instead
using fluxes between elements to stabilise the scheme. This method combines the accuracy and
exponential convergence of spectral methods with the geometric flexibility of finite element
methods. 

% GPUs
With the stagnation of traditional computer \textit{central processing units (CPU)} operating
frequencies over the last years~\cite{Parkhurst2006}, computer systems have evolved to use more
parallel architectures. CPUs are now composed of several computing cores~\cite{Nayfeh1997} executing
tasks together or in parallel, and contemporary \textit{high performance computing (HPC)} platforms
consist of several whole computers networked together executing tasks in parallel. Amidst those
changes, an inherently parallel architecture has started to be used in scientific computing.
\textit{Graphics processing units (GPU)} are computer chips that were initially used in computer
graphics, a massively parallel workload. GPUs can now be programmed similarly to traditional
CPUs~\cite{Owens2008}, offering their thousands of simpler cores to many kids of computation. This
architecture is optimised for maximal bandwidth, to process great amounts of data as fast as
possible. Recently, these processors have been incorporated into HPC platforms~\cite{Fan2004},
enabling massively parallel workloads with theoretical processing power much greater than using
traditional CPUs.

We will use GPUs for our computations in hope to increase the available processing power to solve
complex problems. This will not come free, as GPU architectures are more optimised for static
workloads executing the exact same code on all GPU cores. We use the CUDA parallel computing
platform~\cite{Garland2008}, which enables programming GPUs using the C++ language with some
extensions, much like CPU programming.

% Adaptivity
Even with highly accurate methods and the processing power of GPU-enables HPC platforms, the
available resources must be spent judiciously. To get the accurate results we want, meshes need to
be extremely fine. A mesh this fine throughout the whole domain can become impractical to compute
even on the largest HPC systems. The limited resources must be spent where they will count most.
Creating grids that are more refined in areas of interest is possible if those areas are known
beforehand. This is a time-consuming process that must be started over for every new problem. It is
sometimes not possible to predict where areas of interest will be, such as when modeling chaotic
turbulent flows. It is also possible that the mathematically important areas are not easy to
identify. 

\textit{Adaptive mesh refinement (AMR)} methods have been studied in order to solve this problem.
These methods aim to identify the areas of the mesh that need refinement, and refine those areas to
obtain more accurate results while not wasting resources on areas of less interest.
~\cite{Berger1984}

% Load balancing

% Problem