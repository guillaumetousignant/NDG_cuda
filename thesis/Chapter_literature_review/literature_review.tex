\chapter{Literature Review}\label{chapter:literature_review} 
% SEM
% Large scale SEM (Nek)
% Adaptivity
% GPUs
% Load balancing

The issues that this work aims to overcome are not new, and as such a lot of work has been performed
to overcome them. This section highlights the current state of the art in this domain, and where
this work fits within it.

\section{Spectral Element Methods}\label{section:literature_review:sem}<

In this work, we will use the \acrfull{acr:DG-SEM}, first described by Reed and
Hill~\cite{Reed1973}. Many references are available nowadays on the method, such as textbooks by
Warburton and Hesthaven~\cite{Hesthaven2007} or Kopriva~\cite{Kopriva2009}, which describes a suite
of algorithms that can be used to assemble a full solver using this method. This method is used in a
wide array of domains, including aerodynamics, nanophotonics~\cite{Busch2011} and water
simulation~\cite{Gassner2016}.

Large scale spectral element solvers have existed for quite a while now. One such solver is Nek5000,
an open-source Navier Stokes spectral element solver that runs on \acrshortpl{acr:CPU}. It has been
proven to scale well to above a million processors~\cite{Offermans2017}. This solver has been used
to solve problems like simulation of micro-scale blood flow~\cite{Obabko2017}, \acrfull{acr:DNS} of
flows in engines~\cite{Ameen2020}, and nearly exascale simulation of flows in nuclear
reactors~\cite{Merzari2020}. Other solvers exist, such as the Nektar++~\cite{Cantwell2015} library.
It is a spectral finite element library used to create purpose-built solvers for different kinds of
problems. Example applications are aerodynamic \acrfull{acr:LES} of sport cars~\cite{Mengaldo2020},
Formula One cars~\cite{Cantwell2015}, and \acrshort{acr:DNS} of flows over various airfoils.

These are the kind of solid foundations we wish to build upon in this work. These solvers
traditionally use \acrshortpl{acr:CPU} to compute their solutions. The next logical step is to
perform these computations using \acrshortpl{acr:GPU} in order to increase their performance. These
methods will need to be ported to this new architecture. Additionally, none of these solvers use
\acrfull{acr:AMR}. This will have to be implemented in order to increase the efficiency of
computations, using the limited processing ressources at our disposal where it counts most. This
creates the double challenge of using \acrshortpl{acr:GPU} for high-order methods and adaptivity,
which is especially challenging on \acrshortpl{acr:GPU} as they are more tailored for static
workload. \Acrshortpl{acr:GPU} excel at solving very parallel problems where every core of the
\acrshort{acr:GPU} executes the exact same computation on neatly organised data. \Acrshort{acr:AMR}
is not a naturally parallel process, and the resulting meshes incur divergence in the execution on
different cores. Nonetheless, if \Acrshort{acr:AMR} implemented correctly should decrease the
execution time of the program while reducing ressource usage.

\section{Graphics Processing Units}\label{section:literature_review:gpu}

~\cite{Klockner2009}

In recent years, the scientific computing world has started using \acrshortpl{acr:GPU} to perform
computations. Some traditionally \acrshort{acr:CPU}-based commercial solvers were upgraded to run
entirely on \acrshortpl{acr:GPU}, or have some parts of the code \acrshort{acr:GPU}-accelerated.
Krawezik and Poole present a new \acrshort{acr:GPU}-accelerated direct sparse solver for Ansys
Fluent, boasting a \(2.9 \times \) speedup compared to the \acrshort{acr:CPU}-only solver.
Open-source solutions like OpenFOAM~\cite{Alonazi2015} have also been extended to work on hybrid
\acrshort{acr:CPU} and \acrshort{acr:GPU} architectures. Another example is
RapidCFD~\cite{SimFlow2020}, a collection of OpenFOAM solvers ported to Nvidia \acrshort{acr:CUDA}.

Some brand new solvers have been created for \acrshortpl{acr:GPU}, such as the AmgX solver, a
distributed algebraic multigrid solver. This solver has been shown to have a \(2 \times \) to \(5
\times \) speedup compared to a \acrshort{acr:CPU} implementation~\cite{Naumov2015}.

In the realm of spectral methods, there have been a few advances using \acrshortpl{acr:GPU}.
Notably, NekRS is a Navier Stokes solver designed to run on diverse hardware, including
\acrshortpl{acr:CPU} and \acrshortpl{acr:GPU}~\cite{Fischer2021}. It is related to the traditional
Nek5000 solver. It uses the OCCA~\cite{Medina2014}, a parallel programming language that provides
code that can be compiled to different architectures, notably OpenMP \acrshort{acr:CPU} code,
\acrshort{acr:CUDA} \acrshort{acr:GPU} code and OpenCL code. NekRS shows a parallel efficiency of
\(80 \% \), and \(80 \% \) to \(90 \% \) of the realizable peak performance of \acrshort{acr:HPC}
systems for large problems~\cite{Fischer2021}.

This indicates that \acrshort{acr:SEM} solvers can have very good performance on the
\acrshort{acr:GPU} architecture. This gives a promising foundation for this work, as the core solver
is expected to be fast. These solvers use static meshes, meaning that the mesh does not change
during the simulation. This can be problematic for complex problems, when uniformly increasing the
mesh resolution to capture fine detail in the solution makes the problem prohibitively expensive to
compute. The mesh can be refined beforehand in areas of the flow expected to need refinement, which
is not always possible if these areas cannot be predicted in advance, change position in time, or
are not easy to predict visually. We will therefore want to implement \acrlong{acr:AMR}, which is
not available in any of those well-known solvers.

\section{Adaptive Mesh Refinement}\label{section:literature_review:amr}

\Acrshort{acr:AMR} has been proved to speedup significantly 

~\cite{Plewa2005} % AMR book

% something about not being ideal for GPUs

~\cite{Schive2018} % GPU amr
~\cite{Giuliani2019} % GPU amr gas dynamics

\section{Dynamic Load Balancing}\label{section:literature_review:load_balancing}