\chapter{Literature Review}\label{chapter:literature_review} 
% SEM
% Large scale SEM (Nek)
% Adaptivity
% GPUs
% Load balancing

The issues that this work aims to overcome are not new, and as such a lot of work has been performed
to overcome them. This section highlights the current state of the art in this domain, and where
this work fits within it.

\section{Spectral Element Methods}\label{section:literature_review:sem}

In this work, we will use the \acrfull{acr:DG-SEM}, first described by Reed and
Hill~\cite{Reed1973}. Many references are available nowadays on the method, such as textbooks by
Warburton and Hesthaven~\cite{Hesthaven2007} or Kopriva~\cite{Kopriva2009}, which describes a suite
of algorithms that can be used to assemble a full solver using this method. This method is used in a
wide array of domains, including aerodynamics, nanophotonics~\cite{Busch2011} and water
simulation~\cite{Gassner2016}.

Large scale spectral element solvers have existed for quite a while now. One such solver is Nek5000,
an open-source Navier Stokes spectral element solver that runs on \acrshortpl{acr:CPU}. It has been
proven to scale well to above a million processors~\cite{Offermans2017}. This solver has been used
to solve problems like simulation of micro-scale blood flow~\cite{Obabko2017}, \acrfull{acr:DNS} of
flows in engines~\cite{Ameen2020}, and nearly exascale simulation of flows in nuclear
reactors~\cite{Merzari2020}. Other solvers exist, such as the Nektar++~\cite{Cantwell2015} library.
It is a spectral finite element library used to create purpose-built solvers for different kinds of
problems. Example applications are aerodynamic \acrfull{acr:LES} of sport cars~\cite{Mengaldo2020},
Formula One cars~\cite{Cantwell2015}, and \acrshort{acr:DNS} of flows over various airfoils.

These are the kind of solid foundations we wish to build upon in this work. These solvers
traditionally use \acrshortpl{acr:CPU} to compute their solutions. The next logical step is to
perform these computations using \acrshortpl{acr:GPU} in order to increase their performance. These
methods will need to be ported to this new architecture. Additionally, none of these solvers use
\acrfull{acr:AMR}. This will have to be implemented in order to increase the efficiency of
computations, using the limited processing ressources at our disposal where it counts most. This
creates the double challenge of using \acrshortpl{acr:GPU} for high-order methods and adaptivity,
which is especially challenging on \acrshortpl{acr:GPU} as they are more tailored for static
workload. \Acrshortpl{acr:GPU} excel at solving very parallel problems where every core of the
\acrshort{acr:GPU} executes the exact same computation on neatly organised data. \Acrshort{acr:AMR}
is not a naturally parallel process, and the resulting meshes incur divergence in the execution on
different cores. Nonetheless, if \Acrshort{acr:AMR} implemented correctly should decrease the
execution time of the program while reducing ressource usage.

\section{Graphics Processing Units}\label{section:literature_review:gpu}

~\cite{Klockner2009}

In recent years, the scientific computing world has started using \acrshortpl{acr:GPU} to perform
computations. Some traditionally \acrshort{acr:CPU}-based commercial solvers were upgraded to run
entirely on \acrshortpl{acr:GPU}, or have some parts of the code \acrshort{acr:GPU}-accelerated.
Krawezik and Poole present a new \acrshort{acr:GPU}-accelerated direct sparse solver for Ansys
Fluent, boasting a \(2.9 \times \) speedup compared to the \acrshort{acr:CPU}-only solver.
Open-source solutions like OpenFOAM~\cite{Alonazi2015} have also been extended to work on hybrid
\acrshort{acr:CPU} and \acrshort{acr:GPU} architectures. Another example is
RapidCFD~\cite{SimFlow2020}, a collection of OpenFOAM solvers ported to Nvidia \acrshort{acr:CUDA}.
These solvers see an improvement of up to \(2 \times \) and a parallel speedup of \(3 \times \)
when using eight \acrshortpl{acr:GPU} instead of a single one.

Some brand new solvers have been created for \acrshortpl{acr:GPU}, such as the AmgX solver, a
distributed algebraic multigrid solver. This solver has been shown to have a \(2 \times \) to \(5
\times \) speedup compared to a \acrshort{acr:CPU} implementation~\cite{Naumov2015}.

In the realm of spectral methods, there have been a few advances using \acrshortpl{acr:GPU}. In
2009, Klöckner et al.\ demonstrated a \acrshort{acr:DG-SEM} solver running on a consumer level
\acrshort{acr:GPU} that demonstrated a \(50 \times \) speedup compared to the machine's
\acrshort{acr:CPU}. The next year, Gödel et al.\ presented a \acrshort{acr:GPU}-accelerated discontinuous
Galerkin solver for electric fields, capable of running on multiple \acrshortpl{acr:GPU} in
parallel. The Nek5000 solver gained a \acrshort{acr:GPU} compatible branch for Nekbone, a simplified
version of the program that implements its core solver.

As purpose-build \acrshort{acr:GPU} solvers, NekRS is a Navier Stokes solver designed to run on
diverse hardware, including \acrshortpl{acr:CPU} and \acrshortpl{acr:GPU}~\cite{Fischer2021}. It is
related to the traditional Nek5000 solver. It uses the OCCA~\cite{Medina2014}, a parallel
programming language that provides code that can be compiled to different architectures, notably
OpenMP \acrshort{acr:CPU} code, \acrshort{acr:CUDA} \acrshort{acr:GPU} code and OpenCL code. NekRS
shows a parallel efficiency of \(80 \% \), and \(80 \% \) to \(90 \% \) of the realizable peak
performance of \acrshort{acr:HPC} systems for large problems~\cite{Fischer2021}.

This indicates that \acrshort{acr:SEM} solvers can have very good performance on the
\acrshort{acr:GPU} architecture. This gives a promising foundation for this work, as the core solver
is expected to be fast. These solvers use static meshes, meaning that the mesh does not change
during the simulation. This can be problematic for complex problems, when uniformly increasing the
mesh resolution to capture fine detail in the solution makes the problem prohibitively expensive to
compute. The mesh can be refined beforehand in areas of the flow expected to need refinement, which
is not always possible if these areas cannot be predicted in advance, change position in time, or
are not easy to predict visually. We will therefore want to implement \acrlong{acr:AMR}, which is
not available in any of those well-known solvers. This is due to the fact that \acrlong{acr:AMR}
does not map easily to the \acrshort{acr:GPU} architecture, as stated in
Section~\ref{section:literature_review:sem}. 

\section{Adaptive Mesh Refinement}\label{section:literature_review:amr}

\Acrshort{acr:AMR} has been proved to speedup significantly the computation of fluid flow problems.
Chalmers et al.\ report a \(3 \times \) speedup as well as an improved error when using
\acrshort{acr:AMR}~\cite{Chalmers2019}. The processing time and memory saving will be especially
welcome when considering that \acrshortpl{acr:GPU} have less memory than \acrshortpl{acr:CPU}.

\Acrshort{acr:AMR} implementations on \acrshortpl{acr:CPU} date from quite some time. Berger and
Oliger~\cite{Berger1984} present the method using overset grids, and Khokhlov~\cite{Khokhlov1998}
describes a tree-structured method that can be accessed and modified in parallel.

Some software solutions exist to integrate into existing code and implement \acrlong{acr:AMR}. The
PARAMESH toolkit~\cite{MacNeice2000} is such an example. It indicates that it is possible to obtain
speedups above the order of \( 10 \times \) for high refinement levels, when compared to using an
uniformly refined mesh. It also enables users to work with the meshes it creates in parallel. The
p4est library~\cite{Burstedde2011} is another example, it provides parallel \acrshort{acr:AMR} to
programs by implementing forest of octrees algorithms.It has been proven to scale well up to 220,000
\acrshort{acr:CPU} cores.

Traditional \acrshort{acr:CPU} algorithms are well understood, and full books have been written on
the subject, such as by Plewa et al.~\cite{Plewa2005}. Support for \acrshort{acr:AMR} has even
started to be added to certain well-established solvers like Nek5000~\cite{Offermans2019}, where the
parallel scaling of the method has been studied up to 32,768 \acrshort{acr:CPU}
cores~\cite{Peplinski2016}. 

These are good results, indicating that if the challenges in implementing \acrshort{acr:AMR} on
\acrshortpl{acr:GPU} are overcome, significant gains should be expected. These challenges are not
small, as the non-conforming meshes produces by \acrshort{acr:AMR} increase divergence between
\acrshort{acr:GPU} threads, can create load imbalance when the problem is solved in parallel and the
mesh is not refined equally in each block. The refinement process itself is also not naturally
parallel, therefore care should be taken when implementing it to have as many parts of the process
executed in parallel on \acrshortpl{acr:GPU} as possible. Finally, adaptive meshes complicate the
memory layout on \acrshortpl{acr:GPU}, which are more optimised to perform computations on static
packed data.

\Acrlong{acr:AMR} is not unique to fluid dynamics. The field of astrophysics has numerous
\acrshort{acr:AMR} solvers. First, Enzo~\cite{Bryan2014} is an open-source solver dedicated to
solving astrophysical fluid flows, notably dark matter dynamics and heating and cooling of flows.
This solver is \acrshort{acr:GPU}-accelerated, resulting in a \(5 \times \) speedup over its
\acrshort{acr:CPU}-only implementation. Then there are the GAMER~\cite{Schive2010} and
GAMER-2~\cite{Schive2018} solvers, both open-source \acrshort{acr:GPU}-accelerated
\acrshort{acr:AMR} solvers tailored to astrophysical simulations, including hydrodynamics,
magnetohydrodynamics, self-gravity and particle flows. The latter has been shown to scale up to 4096
\acrshortpl{acr:GPU} and 65,536 \acrshort{acr:CPU} cores. In the field of data visualisation, a lot
of work has to be done in order to directly represent the data generated by \acrshort{acr:AMR}
programs. Wang et al.~\cite{Wang2020} propose algorithm to accurately render tree-based
\acrshort{acr:AMR} data in difficult regions such as across discontinuities in the refinement level.
Their method uses ray tracing and is computed on \acrshortpl{acr:CPU}, and can render data without
having to interpolate it to a single refinement level.

In the gas dynamics field,
~\cite{Giuliani2019} % GPU amr gas dynamics

\section{Dynamic Load Balancing}\label{section:literature_review:load_balancing}

% cite Shiqi

~\cite{Karypis1997} % METIS
~\cite{Karypis1997P} % PARMETIS

~\cite{MacNeice2000} % PARAMESH also uses peano-hilbert curves for load balancing
~\cite{Peplinski2016} % they also use load imbalance tolerance to guide load balancing. They use
                        % parmetis for partitioning
~\cite{Schive2018} % gamer-2 also uses a hilbert curve