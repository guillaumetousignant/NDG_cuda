@inproceedings{Mavriplis1990,
   abstract = {Aspects of adaptive spectral element methods are presented with emphasis on the a posteriori error estimators used in the automatic mesh refinement process. The nonconforming formulation of the method is reviewed in an effort to illustrate the various mesh refinement options available. Single mesh a posteriori error estimators are developed for the spectral element method. These estimate the actual error incurred by the discretization on a local per element basis and predict the convergence behaviour of the numerical solution as well. As a result the error estimators serve as criteria in the choice of refinement options. The usefulness of the estimators is demonstrated through examples of Navier-Stokes calculations.},
   author = {Catherine Mavriplis},
   city = {Wiesbaden},
   editor = {Pieter Wesseling},
   isbn = {978-3-663-13975-1},
   booktitle = {Proceedings of the Eighth GAMM-Conference on Numerical Methods in Fluid Mechanics},
   pages = {333-342},
   publisher = {Vieweg+Teubner Verlag},
   title = {A Posteriori Error Estimators for Adaptive Spectral Element Techniques},
   year = {1990},
}

@inproceedings{Maday1989,
   author = {Yvon Maday and Cathy Mavriplis and Anthony T Patera},
   booktitle = {Domain Decomposition Methods},
   keywords = {Decomposition,Discrete Functions,Domains,Finite Element Method,Fluid Mechanics and Heat Transfer,Grid Generation (Mathematics),Navier-Stokes Equation,Partial Differential Equations,Spectral Methods,Weighting Functions},
   month = {1},
   pages = {392-418},
   title = {Nonconforming mortar element methods - Application to spectral discretizations},
   year = {1989},
}

@book{Kopriva2009,
   abstract = {This book offers a systematic and self-contained approach to solvepartial differential
equations numerically using single and multidomain spectralmethods. It contains detailed
algorithms in pseudocode for the applicationof spectral approximations to both one
and two dimensional PDEsof mathematical physics describing potentials,transport, and
wave propagation. David Kopriva, a well-known researcherin the field with extensive
practical experience, shows how only a fewfundamental algorithms form the building
blocks of any spectral code, evenfor problems with complex geometries. The book addresses
computationaland applications scientists, as it emphasizes thepractical derivation
and implementation of spectral methods over abstract mathematics. It is divided into
two parts: First comes a primer on spectralapproximation and the basic algorithms,
including FFT algorithms, Gaussquadrature algorithms, and how to approximate derivatives.
The secondpart shows how to use those algorithms to solve steady and time dependent
PDEs in one and two space dimensions. Exercises and questions at theend of each chapter
encourage the reader to experiment with thealgorithms.},
   author = {David A Kopriva},
   edition = {1st},
   isbn = {9048122600},
   publisher = {Springer Publishing Company, Incorporated},
   title = {Implementing Spectral Methods for Partial Differential Equations: Algorithms for Scientists and Engineers},
   year = {2009},
}

@article{Harris2007,
   author = {Mark Harris and others},
   issue = {4},
   journal = {Nvidia developer technology},
   pages = {70},
   publisher = {NVIDIA Corp. California},
   title = {Optimizing parallel reduction in CUDA},
   volume = {2},
   year = {2007},
}

@online{Nvidia2021,
  author = {Nvidia},
  title = {CUDA C++ Programming Guide},
  month = {11},
  year = {2021},
  url = {https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html},
  urldate = {2021-11-29}
}

@article{Giuliani2019,
   author = {Andrew Giuliani and Lilia Krivodonova},
   doi = {10.1016/j.jcp.2018.12.019},
   journal = {Journal of Computational Physics},
   month = {11},
   title = {Adaptive mesh refinement on graphics processing units for applications in gas dynamics},
   volume = {381},
   year = {2019},
}

@online{Clucas1999,
   author = {Jean Clucas and Pam Walatka},
   month = {8},
   title = {FAST User Guide},
   url = {https://www.nas.nasa.gov/Software/FAST/RND-93-010.walatka-clucas/htmldocs/chp_16.surferu.html},
   year = {1999},
   urldate = {2021-12-13}
}

@article{Hilbert1891,
   author = {David Hilbert},
   issue = {38},
   journal = {Mathematische Annalen},
   pages = {459-460},
   title = {Über die stetige Abbildung einer Linie auf ein Flächenstück.},
   url = {http://www.digizeitschriften.de/dms/img/?PPN=PPN235181684_0038&DMDID=dmdlog40},
   year = {1891},
}

@article{Peano1890,
   author = {Giuseppe Peano},
   issue = {36},
   journal = {Mathematische Annalen},
   pages = {157-160},
   title = {Sur une courbe, qui remplit toute une aire plane.},
   url = {http://www.digizeitschriften.de/dms/img/?PPN=PPN235181684_0036&DMDID=dmdlog13},
   year = {1890},
}

@article{Haverkort2011,
   author = {Herman J Haverkort},
   journal = {CoRR},
   title = {An inventory of three-dimensional Hilbert space-filling curves},
   volume = {abs/1109.2323},
   url = {http://arxiv.org/abs/1109.2323},
   year = {2011},
}

@article{Karypis1998,
   author = {George Karypis and Vipin Kumar},
   journal = {SIAM Journal on Scientific Computing},
   pages = {359-392},
   title = {A fast and high quality multilevel scheme for partitioning irregular graphs},
   volume = {20},
   year = {1998},
}

@article{Pinar2004,
   abstract = {The one-dimensional decomposition of nonuniform workload arrays with optimal load balancing is investigated. The problem has been studied in the literature as the “chains-on-chains partitioning” problem. Despite the rich literature on exact algorithms, heuristics are still used in parallel computing community with the “hope” of good decompositions and the “myth” of exact algorithms being hard to implement and not runtime efficient. We show that exact algorithms yield significant improvements in load balance over heuristics with negligible overhead. Detailed pseudocodes of the proposed algorithms are provided for reproducibility. We start with a literature review and propose improvements and efficient implementation tips for these algorithms. We also introduce novel algorithms that are asymptotically and runtime efficient. Our experiments on sparse matrix and direct volume rendering datasets verify that balance can be significantly improved by using exact algorithms. The proposed exact algorithms are 100 times faster than a single sparse-matrix vector multiplication for 64-way decompositions on the average. We conclude that exact algorithms with proposed efficient implementations can effectively replace heuristics.},
   author = {Ali Pınar and Cevdet Aykanat},
   doi = {https://doi.org/10.1016/j.jpdc.2004.05.003},
   issn = {0743-7315},
   issue = {8},
   journal = {Journal of Parallel and Distributed Computing},
   keywords = {Chains-on-chains partitioning,Dynamic programming,Image-space parallel volume rendering,Iterative refinement,One-dimensional partitioning,Optimal load balancing,Parallel sparse matrix vector multiplication,Parametric search},
   pages = {974-996},
   title = {Fast optimal load balancing algorithms for 1D partitioning},
   volume = {64},
   url = {https://www.sciencedirect.com/science/article/pii/S0743731504000851},
   year = {2004},
}

@article{Patera1984,
   abstract = {A spectral element method that combines the generality of the finite element method with the accuracy of spectral techniques is proposed for the numerical solution of the incompressible Navier-Stokes equations. In the spectral element discretization, the computational domain is broken into a series of elements, and the velocity in each element is represented as a high-order Lagrangian interpolant through Chebyshev collocation points. The hyperbolic piece of the governing equations is then treated with an explicit collocation scheme, while the pressure and viscous contributions are treated implicitly with a projection operator derived from a variational principle. The implementation of the technique is demonstrated on a one-dimensional inflow-outflow advection-diffusion equation, and the method is then applied to laminar two-dimensional (separated) flow in a channel expansion. Comparisons are made with experiment and previous numerical work.},
   author = {Anthony T Patera},
   doi = {https://doi.org/10.1016/0021-9991(84)90128-1},
   issn = {0021-9991},
   issue = {3},
   journal = {Journal of Computational Physics},
   pages = {468-488},
   title = {A spectral element method for fluid dynamics: Laminar flow in a channel expansion},
   volume = {54},
   url = {https://www.sciencedirect.com/science/article/pii/0021999184901281},
   year = {1984},
}

@book{Gottlieb1977,
   author = {David Gottlieb and Steven A Orszag},
   publisher = {SIAM},
   title = {Numerical analysis of spectral methods: theory and applications},
   year = {1977},
}

@inbook{Toro2009,
   author = {Eleuterio Toro},
   doi = {10.1007/b79761},
   journal = {Riemann Solvers and Numerical Methods for Fluid Dynamics},
   month = {1},
   title = {Riemann Solvers and Numerical Methods for Fluid Dynamics: A Practical Introduction},
   year = {2009},
}

@article{Williamson1980,
   abstract = {All second-order, many third-order, and a few fourth-order Runge-Kutta schemes can be arranged to require only two storage locations per variable, compared with three needed by Gill's method.},
   author = {J H Williamson},
   doi = {https://doi.org/10.1016/0021-9991(80)90033-9},
   issn = {0021-9991},
   issue = {1},
   journal = {Journal of Computational Physics},
   pages = {48-56},
   title = {Low-storage Runge-Kutta schemes},
   volume = {35},
   url = {https://www.sciencedirect.com/science/article/pii/0021999180900339},
   year = {1980},
}

@article{Gottlieb2001,
   author = {Sigal Gottlieb and Chi-Wang Shu and Eitan Tadmor},
   doi = {10.1137/S003614450036757X},
   journal = {SIAM Review},
   month = {1},
   title = {Strong Stability-Preserving High-Order Time Discretization Methods},
   volume = {43},
   year = {2001},
}

@report{Slotnick2014,
   author = {Jeffrey Slotnick and Abdollah Khodadoust and Juan Alonso and William Gropp and Dimitri Mavriplis},
   title = {CFD Vision 2030 Study: A Path to Revolutionary Computational Aerosciences},
   url = {http://www.sti.nasa.gov},
   year = {2014},
}

@article{Stern2001,
   author = {Fred Stern and Robert V Wilson and Hugh W Coleman and Eric G Paterson},
   issue = {4},
   journal = {J. Fluids Eng.},
   pages = {793-802},
   title = {Comprehensive approach to verification and validation of CFD simulations—part 1: methodology and procedures},
   volume = {123},
   year = {2001},
}

@inbook{Karniadakis2005,
   author = {George Karniadakis and Spencer Sherwin},
   doi = {10.1093/acprof:oso/9780198528692.001.0001},
   month = {1},
   title = {Spectral/HP Element Methods for Computational Fluid Dynamics},
   year = {2005},
}

@inproceedings{Fan2004,
   abstract = {Inspired by the attractive Flops/dollar ratio and the incredible growth in the speed of modern graphics processing units (GPUs), we propose to use a cluster of GPUs for high performance scientific computing. As an example application, we have developed a parallel flow simulation using the lattice Boltzmann model (LBM) on a GPU cluster and have simulated the dispersion of airborne contaminants in the Times Square area of New York City. Using 30 GPU nodes, our simulation can compute a 480x400x80 LBM in 0.31 second/step, a speed which is 4.6 times faster than that of our CPU cluster implementation. Besides the LBM, we also discuss other potential applications of the GPU cluster, such as cellular automata, PDE solvers, and FEM.},
   author = {Zhe Fan and Feng Qiu and A Kaufman and S Yoakum-Stover},
   doi = {10.1109/SC.2004.26},
   journal = {SC '04: Proceedings of the 2004 ACM/IEEE Conference on Supercomputing},
   month = {11},
   pages = {47},
   title = {GPU Cluster for High Performance Computing},
   year = {2004},
}

@article{Owens2008,
   abstract = {The graphics processing unit (GPU) has become an integral part of today's mainstream computing systems. Over the past six years, there has been a marked increase in the performance and capabilities of GPUs. The modern GPU is not only a powerful graphics engine but also a highly parallel programmable processor featuring peak arithmetic and memory bandwidth that substantially outpaces its CPU counterpart. The GPU's rapid increase in both programmability and capability has spawned a research community that has successfully mapped a broad range of computationally demanding, complex problems to the GPU. This effort in general-purpose computing on the GPU, also known as GPU computing, has positioned the GPU as a compelling alternative to traditional microprocessors in high-performance computer systems of the future. We describe the background, hardware, and programming model for GPU computing, summarize the state of the art in tools and techniques, and present four GPU computing successes in game physics and computational biophysics that deliver order-of-magnitude performance gains over optimized CPU applications.},
   author = {John D Owens and Mike Houston and David Luebke and Simon Green and John E Stone and James C Phillips},
   doi = {10.1109/JPROC.2008.917757},
   issn = {1558-2256},
   issue = {5},
   journal = {Proceedings of the IEEE},
   month = {5},
   pages = {879-899},
   title = {GPU Computing},
   volume = {96},
   year = {2008},
}

@inproceedings{Lee2010,
   abstract = {Recent advances in computing have led to an explosion in the amount of data being generated. Processing the ever-growing data in a timely manner has made throughput computing an important aspect for emerging applications. Our analysis of a set of important throughput computing kernels shows that there is an ample amount of parallelism in these kernels which makes them suitable for today's multi-core CPUs and GPUs. In the past few years there have been many studies claiming GPUs deliver substantial speedups (between 10X and 1000X) over multi-core CPUs on these kernels. To understand where such large performance difference comes from, we perform a rigorous performance analysis and find that after applying optimizations appropriate for both CPUs and GPUs the performance gap between an Nvidia GTX280 processor and the Intel Core i7-960 processor narrows to only 2.5x on average. In this paper, we discuss optimization techniques for both CPU and GPU, analyze what architecture features contributed to performance differences between the two architectures, and recommend a set of architectural features which provide significant improvement in architectural efficiency for throughput kernels.},
   author = {Victor W Lee and Changkyu Kim and Jatin Chhugani and Michael Deisher and Daehyun Kim and Anthony D Nguyen and Nadathur Satish and Mikhail Smelyanskiy and Srinivas Chennupaty and Per Hammarlund and Ronak Singhal and Pradeep Dubey},
   city = {New York, NY, USA},
   doi = {10.1145/1815961.1816021},
   isbn = {9781450300537},
   journal = {Proceedings of the 37th Annual International Symposium on Computer Architecture},
   keywords = {cpu architecture,gpu architecture,performance analysis,performance measurement,software optimization,throughput computing},
   pages = {451-460},
   publisher = {Association for Computing Machinery},
   title = {Debunking the 100X GPU vs. CPU Myth: An Evaluation of Throughput Computing on CPU and GPU},
   url = {https://doi.org/10.1145/1815961.1816021},
   year = {2010},
}

@article{Nayfeh1997,
   abstract = {Presents the case for billion-transistor processor architectures that will consist of chip multiprocessors (CMPs): multiple (four to 16) simple, fast processors on one chip. In their proposal, each processor is tightly coupled to a small, fast, level-one cache, and all processors share a larger level-two cache. The processors may collaborate on a parallel job or run independent tasks (as in the SMT proposal). The CMP architecture lends itself to simpler design, faster validation, cleaner functional partitioning, and higher theoretical peak performance. However for this architecture to realize its performance potential, either programmers or compilers will have to make code explicitly parallel. Old ISAs will be incompatible with this architecture (although they could run slowly on one of the small processors).},
   author = {B A Nayfeh and K Olukotun},
   doi = {10.1109/2.612253},
   issn = {1558-0814},
   issue = {9},
   journal = {Computer},
   month = {9},
   pages = {79-85},
   title = {A single-chip multiprocessor},
   volume = {30},
   year = {1997},
}

@article{Barroso2005,
   abstract = {In the late 1990s, our research group at DEC was one of a growing number of teams advocating the CMP (chip multiprocessor) as an alternative to highly complex single-threaded CPUs. We were designing the Piranha system,1 which was a radical point in the CMP design space in that we used very simple cores (similar to the early RISC designs of the late ’80s) to provide a higher level of thread-level parallelism. Our main goal was to achieve the best commercial workload performance for a given silicon budget. Today, in developing Google’s computing infrastructure, our focus is broader than performance alone. The merits of a particular architecture are measured by answering the following question: Are you able to afford the computational capacity you need? The high-computational demands that are inherent in most of Google’s services have led us to develop a deep understanding of the overall cost of computing, and continually to look for hardware/software designs that optimize performance per unit of cost.},
   author = {Luiz André Barroso},
   city = {New York, NY, USA},
   doi = {10.1145/1095408.1095420},
   issn = {1542-7730},
   issue = {7},
   journal = {Queue},
   month = {9},
   pages = {48-53},
   publisher = {Association for Computing Machinery},
   title = {The Price of Performance: An Economic Case for Chip Multiprocessing},
   volume = {3},
   url = {https://doi.org/10.1145/1095408.1095420},
   year = {2005},
}

@inproceedings{Parkhurst2006,
   abstract = {In the past, processor design trends were dominated by increasingly complex feature sets, higher clock speeds, growing thermal envelopes and increasing power dissipation. Recently, clock speeds have tapered and thermal and power dissipation envelopes have remained flat. However, the demand for increasing performance continues which has fueled the move to integrated multiple processor (multi-core) designs. This paper discusses this trend towards multi-core processor designs, the design challenges that accompany it and a view of the research required to support it.},
   author = {Jeff Parkhurst and John Darringer and Bill Grundmann},
   city = {New York, NY, USA},
   doi = {10.1145/1233501.1233516},
   isbn = {1595933891},
   journal = {Proceedings of the 2006 IEEE/ACM International Conference on Computer-Aided Design},
   pages = {67-72},
   publisher = {Association for Computing Machinery},
   title = {From Single Core to Multi-Core: Preparing for a New Exponential},
   url = {https://doi.org/10.1145/1233501.1233516},
   year = {2006},
}

@article{Garland2008,
   abstract = {The CUDA programming model provides a straightforward means of describing inherently parallel computations, and NVIDIA's Tesla GPU architecture delivers high computational throughput on massively parallel problems. This article surveys experiences gained in applying CUDA to a diverse set of problems and the parallel speedups over sequential codes running on traditional CPU architectures attained by executing key computations on the GPU.},
   author = {Michael Garland and Scott Le Grand and John Nickolls and Joshua Anderson and Jim Hardwick and Scott Morton and Everett Phillips and Yao Zhang and Vasily Volkov},
   doi = {10.1109/MM.2008.57},
   issn = {1937-4143},
   issue = {4},
   journal = {IEEE Micro},
   month = {7},
   pages = {13-27},
   title = {Parallel Computing Experiences with CUDA},
   volume = {28},
   year = {2008},
}

@article{Deville2003,
   author = {Michel Deville and P Fischer and E H Mund},
   doi = {10.1115/1.1566402},
   journal = {Applied Mechanics Reviews},
   month = {1},
   title = {High-Order Methods for Incompressible Fluid Flow},
   volume = {56},
   year = {2003},
}

@thesis{Mavriplis1989,
   author = {Cathy Mavriplis},
   institution = {Massachusetts Institute of Technology},
   title = {Nonconforming discretizations and a posteriori error estimators for adaptive spectral element techniques},
   year = {1989},
}

@report{Reed1973,
   author = {William H Reed and Thomas R Hill},
   institution = {Los Alamos Scientific Lab., N. Mex.(USA)},
   title = {Triangular mesh methods for the neutron transport equation},
   year = {1973},
}

@inbook{Hesthaven2007,
   author = {Jan Hesthaven and Tim Warburton},
   month = {2},
   title = {Nodal Discontinuous Galerkin Methods: Algorithms, Analysis, and Applications},
   volume = {54},
   year = {2007},
}

@inproceedings{Amdahl1967,
   abstract = {For over a decade prophets have voiced the contention that the organization of a single computer has reached its limits and that truly significant advances can be made only by interconnection of a multiplicity of computers in such a manner as to permit cooperative solution. Variously the proper direction has been pointed out as general purpose computers with a generalized interconnection of memories, or as specialized computers with geometrically related memory interconnections and controlled by one or more instruction streams.},
   author = {Gene M Amdahl},
   city = {New York, NY, USA},
   doi = {10.1145/1465482.1465560},
   isbn = {9781450378956},
   journal = {Proceedings of the April 18-20, 1967, Spring Joint Computer Conference},
   pages = {483-485},
   publisher = {Association for Computing Machinery},
   title = {Validity of the Single Processor Approach to Achieving Large Scale Computing Capabilities},
   url = {https://doi.org/10.1145/1465482.1465560},
   year = {1967},
}

@article{Gustafson1988,
   author = {John L Gustafson},
   city = {New York, NY, USA},
   doi = {10.1145/42411.42415},
   issn = {0001-0782},
   issue = {5},
   journal = {Commun. ACM},
   month = {5},
   pages = {532-533},
   publisher = {Association for Computing Machinery},
   title = {Reevaluating Amdahl's Law},
   volume = {31},
   url = {https://doi.org/10.1145/42411.42415},
   year = {1988},
}

@article{Berger1984,
   abstract = {An adaptive method based on the idea of multiple component grids for the solution of hyperbolic partial differential equations using finite difference techniques is presented. Based upon Richardson-type estimates of the truncation error, refined grids are created or existing ones removed to attain a given accuracy for a minimum amount of work. The approach is recursive in that fine grids can contain even finer grids. The grids with finer mesh width in space also have a smaller mesh width in time, making this a mesh refinement algorithm in time and space. We present the algorithm, error estimation procedure, and the data structures, and conclude with numerical examples in one and two space dimensions.},
   author = {Marsha J Berger and Joseph Oliger},
   doi = {https://doi.org/10.1016/0021-9991(84)90073-1},
   issn = {0021-9991},
   issue = {3},
   journal = {Journal of Computational Physics},
   pages = {484-512},
   title = {Adaptive mesh refinement for hyperbolic partial differential equations},
   volume = {53},
   url = {https://www.sciencedirect.com/science/article/pii/0021999184900731},
   year = {1984},
}

@article{Khokhlov1998,
   abstract = {A fully threaded tree (FTT) for adaptive mesh refinement (AMR) of regular meshes is described. By using a tree threaded at all levels, tree traversals for finding nearest neighbors are avoided. All operations on a tree including tree modifications areO(N), whereNis a number of cells and can be performed in parallel. An implementation of the tree requires 2Nwords of memory. In this paper, FTT is applied to the integration of the Euler equations of fluid dynamics. The integration on a tree can utilize flux evaluation algorithms used for grids, but requires a different time-stepping strategy to be computationally efficient. An adaptive-mesh time-stepping algorithm is described in which different time steps are used at different levels of the tree. Time stepping and mesh refinement are interleaved to avoid extensive buffer layers of fine mesh which were otherwise required ahead of moving shocks. A filtering algorithm for removing high-frequency noise during mesh refinement is described. Test examples are presented, and the FTT performance is evaluated.},
   author = {A M Khokhlov},
   doi = {https://doi.org/10.1006/jcph.1998.9998},
   issn = {0021-9991},
   issue = {2},
   journal = {Journal of Computational Physics},
   pages = {519-543},
   title = {Fully Threaded Tree Algorithms for Adaptive Refinement Fluid Dynamics Simulations},
   volume = {143},
   url = {https://www.sciencedirect.com/science/article/pii/S0021999198999983},
   year = {1998},
}

@article{Cantwell2015,
   author = {C D Cantwell and D Moxey and A Comerford and A. Bolis and G Rocco and G Mengaldo and D De Grazia and S Yakovlev and J-E. Lombard and D Ekelschot and B. Jordi and H Xu and Y Mohamied and C Eskilsson and B Nelson and P Vos and C Biotto and R M Kirby and S J Sherwin},
   doi = {10.1016/j.cpc.2015.02.008},
   issn = {0010-4655},
   journal = {COMPUTER PHYSICS COMMUNICATIONS},
   month = {7},
   pages = {205-219},
   publisher = {ELSEVIER SCIENCE BV},
   title = {Nektar plus plus : An open-source spectral/hp element framework},
   volume = {192},
   url = {http://gateway.webofknowledge.com/gateway/Gateway.cgi?GWVersion=2&SrcApp=PARTNER_APP&SrcAuth=LinksAMR&KeyUT=WOS:000354141100020&DestLinkType=FullRecord&DestApp=ALL_WOS&UsrCustomerID=1ba7043ffcc86c417c072aa74d649202},
   year = {2015},
}

@online{Siemens2020,
   author = {Siemens},
   journal = {Siemens Simcenter},
   month = {1},
   title = {Whistle while you mesh: Simcenter STAR-CCM+ model-driven adaptive mesh refinement (AMR)},
   year = {2020},
   url = {https://blogs.sw.siemens.com/simcenter/whistle-while-you-mesh-simcenter-star-ccm-model-driven-adaptive-mesh-refinement-amr},
   urldate = {2022-02-11}
}

@article{Schive2018,
   abstract = {We present gamer-2, a GPU-accelerated adaptive mesh refinement (AMR) code for astrophysics. It provides a rich set of features, including adaptive time-stepping, several hydrodynamic schemes, magnetohydrodynamics, self-gravity, particles, star formation, chemistry, and radiative processes with grackle, data analysis with yt, and memory pool for efficient object allocation. gamer-2 is fully bitwise reproducible. For the performance optimization, it adopts hybrid OpenMP/MPI/GPU parallelization and utilizes overlapping CPU computation, GPU computation, and CPU–GPU communication. Load balancing is achieved using a Hilbert space-filling curve on a level-by-level basis without the need to duplicate the entire AMR hierarchy on each MPI process. To provide convincing demonstrations of the accuracy and performance of gamer-2, we directly compare with enzo on isolated disc galaxy simulations and with flash on galaxy cluster merger simulations. We show that the physical results obtained by different codes are in very good agreement, and gamer-2 outperforms enzo and flash by nearly one and two orders of magnitude, respectively, on the Blue Waters supercomputers using 1–256 nodes. More importantly, gamer-2 exhibits similar or even better parallel scalability compared to the other two codes. We also demonstrate good weak and strong scaling using up to 4096 GPUs and 65 536 CPU cores, and achieve a uniform resolution as high as \\10\\, 240^3\\ cells. Furthermore, gamer-2 can be adopted as an AMR + GPUs framework and has been extensively used for the wave dark matter simulations. gamer-2 is open source (available at https://github.com/gamer-project/gamer) and new contributions are welcome.},
   author = {Hsi-Yu Schive and John A ZuHone and Nathan J Goldbaum and Matthew J Turk and Massimo Gaspari and Chin-Yu Cheng},
   doi = {10.1093/mnras/sty2586},
   issn = {0035-8711},
   issue = {4},
   journal = {Monthly Notices of the Royal Astronomical Society},
   month = {2},
   pages = {4815-4840},
   title = {gamer-2: a GPU-accelerated adaptive mesh refinement code – accuracy, performance, and scalability},
   volume = {481},
   url = {https://doi.org/10.1093/mnras/sty2586},
   year = {2018},
}

@article{Offermans2017,
   author = {Nicolas Offermans and Oana Marin and Michel Schanen and Jing Gong and Paul F Fischer and Philipp Schlatter and Aleks Obabko and Adam Peplinski and Maxwell Hutchinson and Elia Merzari},
   journal = {CoRR},
   title = {On the Strong Scaling of the Spectral Element Solver Nek5000 on Petascale Systems},
   volume = {abs/1706.02970},
   url = {http://arxiv.org/abs/1706.02970},
   year = {2017},
   urldate = {2022-02-21},
}

@article{Fischer2021,
   author = {Paul Fischer and Stefan Kerkemeier and Misun Min and Yu-Hsiang Lan and Malachi Phillips and Thilina Rathnayake and Elia Merzari and Ananias Tomboulides and Ali Karakus and Noel Chalmers and Tim Warburton},
   journal = {CoRR},
   title = {NekRS, a GPU-Accelerated Spectral Element Navier-Stokes Solver},
   volume = {abs/2104.05829},
   url = {https://arxiv.org/abs/2104.05829},
   year = {2021},
   urldate = {2022-02-21},
}

@article{Medina2014,
   author = {David S Medina and Amik St.-Cyr and Timothy Warburton},
   journal = {CoRR},
   title = {OCCA: A unified approach to multi-threading languages},
   volume = {abs/1403.0968},
   url = {http://arxiv.org/abs/1403.0968},
   year = {2014},
   urldate = {2022-02-21},
}

@inproceedings{Krawezik2009,
   author = {Geraud Krawezik and Gene Poole},
   month = {2},
   title = {Accelerating the ANSYS Direct Sparse Solver with GPUs},
   year = {2009},
}

@article{Naumov2015,
   abstract = { The solution of large sparse linear systems arises in many applications, such as computational fluid dynamics and oil reservoir simulation. In realistic cases the matrices are often so large that they require large scale distributed parallel computing to obtain the solution of interest in a reasonable time. In this paper we discuss the design and implementation of the AmgX library, which provides drop-in GPU acceleration of distributed algebraic multigrid (AMG) and preconditioned iterative methods. The AmgX library implements both classical and aggregation-based AMG methods with different selector and interpolation strategies, along with a variety of smoothers and preconditioners, including block-Jacobi, Gauss–Seidel, and incomplete-LU factorization. The library contains many of the standard and flexible preconditioned Krylov subspace iterative methods, which can be combined with any of the available multigrid methods or simpler preconditioners. The parallelism in the aggregation scheme exploits parallel graph matching techniques, while the smoothers and preconditioners often rely on parallel graph coloring algorithms. The AMG algorithm implemented in the AmgX library achieves $2$–$5$ speedup on a single GPU against a competitive implementation on the CPU. As will be shown in the numerical experiments section, both setup and solve phases scale well across multiple nodes, sustaining this performance advantage. },
   author = {M Naumov and M Arsaev and P Castonguay and J Cohen and J Demouth and J Eaton and S Layton and N Markovskiy and I Reguly and N Sakharnykh and V Sellappan and R Strzodka},
   doi = {10.1137/140980260},
   issue = {5},
   journal = {SIAM Journal on Scientific Computing},
   pages = {S602-S626},
   title = {AmgX: A Library for GPU Accelerated Algebraic Multigrid and Preconditioned Iterative Methods},
   volume = {37},
   url = {https://doi.org/10.1137/140980260},
   year = {2015},
}

@article{Alonazi2015,
   author = {Amani Alonazi and David Keyes and Alexey Lastovetsky and Vladimir Rychkov},
   month = {2},
   title = {Design and Optimization of OpenFOAM-based CFD Applications for Hybrid and Heterogeneous HPC Platforms},
   year = {2015},
}

@online{SimFlow2020,
   author = {SimFlow},
   journal = {SimFlow},
   pages = {1-1},
   title = {RapidCFD GPU - OpenFOAM on GPU},
   year = {2020},
   url = {https://sim-flow.com/rapid-cfd-gpu/},
   urldate = {2022-02-21}
}

@thesis{He2021,
   author = {Shiqi He},
   city = {75 Laurier Ave. E, Ottawa},
   doi = {http://dx.doi.org/10.20381/ruor-26256},
   institution = {University of Ottawa},
   month = {2},
   title = {Dynamic Load Balancing for a hp-adaptive Discontinuous Galerkin Wave Equation Solver via Spacing-Filling Curve and Advanced Data Structure},
   year = {2021},
}

@article{Plewa2005,
   author = {Tomasz Plewa and Timur Linde and V Gregory Weirs and others},
   publisher = {Springer},
   title = {Adaptive mesh refinement-theory and applications},
   year = {2005},
}

@inproceedings{Obabko2017,
   author = {Aleksandr Obabko and Eduard Tsyrulnykov and Robert Rainsberger and Alvaro V Torreira and Hassan Nagib and Anil Agarwal and Paul F Fischer},
   journal = {APS Division of Fluid Dynamics Meeting Abstracts},
   pages = {A11–008},
   title = {Large-Eddy Simulation of Flows Through a Novel Vascular Access Device for Hemodialysis Access},
   year = {2017},
}

@article{Ameen2020,
   author = {Muhsin Ameen and Saumil Patel and Juan Colmenares and Tanmoy Chatterjee and Jackie Chen},
   journal = {DOE Vehicle Technologies Office Annual Merit Review},
   pages = {1-4},
   title = {Direct Numerical Simulation (DNS) and high-fidelity large-eddy simulations for improved prediction of in-cylinder flow and combustion processes},
   year = {2020},
}

@article{Merzari2020,
   author = {Elia Merzari and Paul Fischer and Misun Min and Stefan Kerkemeier and Aleksandr Obabko and Dillon Shaver and Haomin Yuan and Yiqi Yu and Javier Martinez and Landon Brockmeyer and others},
   issue = {9},
   journal = {Nuclear Technology},
   pages = {1308-1324},
   publisher = {Taylor & Francis},
   title = {Toward exascale: overview of large eddy simulations and direct numerical simulations of nuclear reactor flows with the spectral element method in Nek5000},
   volume = {206},
   year = {2020},
}

@article{Mengaldo2020,
   author = {Gianmarco Mengaldo and David Moxey and Michael Turner and Rodrigo Costa Moura and Ayad Jassim and Mark Taylor and Joaquim Peiró and Spencer J Sherwin},
   journal = {CoRR},
   title = {Industry-Relevant Implicit Large-Eddy Simulation of a High-Performance
Road Car via Spectral/hp Element Methods},
   volume = {abs/2009.10178},
   url = {https://arxiv.org/abs/2009.10178},
   year = {2020},
}

@article{Busch2011,
   abstract = {Abstract This article reviews the state of the recently developed discontinuous Galerkin finite element method for the efficient numerical treatment of nanophotonic systems. This approach combines the accurate and flexible spatial discretisation of classical finite elements with efficient time stepping capabilities. We describe in detail the underlying principles of the discontinuous Galerkin technique and its application to the simulation of complex nanophotonic structures. In addition, formulations for both time- and frequency-domain solvers are provided and specific advantages and limitations of the technique are discussed. The potential of the discontinuous Galerkin approach is illustrated by modelling and simulating several experimentally relevant systems.},
   author = {K Busch and M König and J Niegemann},
   doi = {https://doi.org/10.1002/lpor.201000045},
   issue = {6},
   journal = {Laser \& Photonics Reviews},
   keywords = {Maxwell's equations,Nanophotonics,discontinuous Galerkin methods.,frequency-domain simulations,light-matter interaction,plasmonics,time-domain simulations},
   pages = {773-809},
   title = {Discontinuous Galerkin methods in nanophotonics},
   volume = {5},
   url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/lpor.201000045},
   year = {2011},
}

@article{Klockner2009,
   abstract = {Discontinuous Galerkin (DG) methods for the numerical solution of partial differential equations have enjoyed considerable success because they are both flexible and robust: They allow arbitrary unstructured geometries and easy control of accuracy without compromising simulation stability. Lately, another property of DG has been growing in importance: The majority of a DG operator is applied in an element-local way, with weak penalty-based element-to-element coupling. The resulting locality in memory access is one of the factors that enables DG to run on off-the-shelf, massively parallel graphics processors (GPUs). In addition, DG’s high-order nature lets it require fewer data points per represented wavelength and hence fewer memory accesses, in exchange for higher arithmetic intensity. Both of these factors work significantly in favor of a GPU implementation of DG. Using a single US400 Nvidia GTX 280 GPU, we accelerate a solver for Maxwell’s equations on a general 3D unstructured grid by a factor of around 50 relative to a serial computation on a current-generation CPU. In many cases, our algorithms exhibit full use of the device’s available memory bandwidth. Example computations achieve and surpass 200gigaflops/s of net application-level floating point work. In this article, we describe and derive the techniques used to reach this level of performance. In addition, we present comprehensive data on the accuracy and runtime behavior of the method.},
   author = {A Klöckner and T Warburton and J Bridge and J S Hesthaven},
   doi = {https://doi.org/10.1016/j.jcp.2009.06.041},
   issn = {0021-9991},
   issue = {21},
   journal = {Journal of Computational Physics},
   keywords = {Discontinuous Galerkin,GPU,High order,Many-core,Maxwell’s equations,Parallel computation},
   pages = {7863-7882},
   title = {Nodal discontinuous Galerkin methods on graphics processors},
   volume = {228},
   url = {https://www.sciencedirect.com/science/article/pii/S0021999109003647},
   year = {2009},
}

@article{Gassner2016,
   abstract = {In this work, we design an arbitrary high order accurate nodal discontinuous Galerkin spectral element type method for the one dimensional shallow water equations. The novel method uses a skew-symmetric formulation of the continuous problem. We prove that this discretisation exactly preserves the local mass and momentum. Furthermore, we show that combined with a special numerical interface flux function, the method exactly preserves the entropy, which is also the total energy for the shallow water equations. Finally, we prove that the surface fluxes, the skew-symmetric volume integrals, and the source term are well balanced. Numerical tests are performed to demonstrate the theoretical findings.},
   author = {Gregor J Gassner and Andrew R Winters and David A Kopriva},
   doi = {https://doi.org/10.1016/j.amc.2015.07.014},
   issn = {0096-3003},
   journal = {Applied Mathematics and Computation},
   keywords = {Discontinuous Galerkin spectral element method,Entropy conservation,Gauss–Lobatto Legendre,Skew-symmetric shallow water equations,Summation-by-parts,Well balanced},
   note = {Recent Advances in Numerical Methods for Hyperbolic Partial Differential Equations},
   pages = {291-308},
   title = {A well balanced and entropy conservative discontinuous Galerkin spectral element method for the shallow water equations},
   volume = {272},
   url = {https://www.sciencedirect.com/science/article/pii/S0096300315009261},
   year = {2016},
}

@article{Godel2010,
   abstract = {A multirate Adams-Bashforth (AB) scheme for simulation of electromagnetic wave propagation using the discontinuous Galerkin finite element method (DG-FEM) is presented. The algorithm is adapted such that single-instruction multiple-thread (SIMT) characteristic for the implementation on a graphics processing unit (GPU) is preserved. A domain decomposition strategy respecting the multirate classification for computation on multiple GPUs is presented. Accuracy and performance is analyzed with help of suitable benchmarks.},
   author = {Nico Gödel and Steffen Schomann and Tim Warburton and Markus Clemens},
   doi = {10.1109/TMAG.2010.2043655},
   issn = {1941-0069},
   issue = {8},
   journal = {IEEE Transactions on Magnetics},
   month = {8},
   pages = {2735-2738},
   title = {GPU Accelerated Adams–Bashforth Multirate Discontinuous Galerkin FEM Simulation of High-Frequency Electromagnetic Fields},
   volume = {46},
   year = {2010},
}

@inproceedings{Peplinski2016,
   abstract = {We discuss parallel performance of h-type Adaptive Mesh Refinement (AMR) developed for the high-order spectral element solver Nek5000 within CRESTA project. AMR is a desired feature of the future simulation software, as it gives possibility to increase the accuracy of numerical simulations at minimal computational cost by resolving particular region of the domain. At the same time it increases complexity of the communication pattern and introduces load imbalance, that can have negative effect on the code scalability. In this work we concentrate on the parallel performance of different tools required by AMR and the resulting algorithm limitations. Our implementation is based on available libraries for parallel mesh management (p4est) and partitioning (ParMetis) that provide necessary information for grid refinement/coarsening and redistribution performed within nonconforming version of Nek5000. For simplicity we consider advection-diffusion problem instead of the full Navies-Stokes equations and study both strong and weak scalability for the convected-cone problem. It is a synthetic test case allowing to test AMR with frequent dynamic mesh adjustments.},
   author = {Adam Peplinski and Paul F Fischer and Philipp Schlatter},
   city = {New York, NY, USA},
   doi = {10.1145/2938615.2938620},
   isbn = {9781450341226},
   journal = {Proceedings of the Exascale Applications and Software Conference 2016},
   keywords = {large-scale scientific computing,nonconforming methods,parallel adaptive mesh refinement,spectral elements},
   publisher = {Association for Computing Machinery},
   title = {Parallel Performance of H-Type Adaptive Mesh Refinement for Nek5000},
   url = {https://doi.org/10.1145/2938615.2938620},
   year = {2016},
}

@inproceedings{Offermans2019,
   abstract = {When performing computational fluid dynamics (CFD) simulations of complex flows, the a priori knowledge of the flow physics and the location of the dominant flow features are usually unknown. For this reason, the development of adaptive remeshing techniques is crucial for large-scale computational problems. Some work has been made recently to provide Nek5000 with adaptive mesh refinement (AMR) capabilities in order to facilitate the generation of the grid and push forward the limit in terms of problem size and complexity [10].},
   author = {Nicolas Offermans and Adam Peplinski and Oana Marin and P F Fischer and Philipp Schlatter},
   city = {Cham},
   editor = {Vincenzo
and Fröhlich Jochen
and Geurts Bernard J.
and Kuerten Hans Salvetti Maria Vittoria
and Armenio},
   isbn = {978-3-030-04915-7},
   journal = {Direct and Large-Eddy Simulation XI},
   pages = {9-15},
   publisher = {Springer International Publishing},
   title = {Towards Adaptive Mesh Refinement for the Spectral Element Solver Nek5000},
   year = {2019},
}

@article{MacNeice2000,
   abstract = {In this paper we describe a community toolkit which is designed to provide parallel support with adaptive mesh capability for a large and important class of computational models, those using structured, logically Cartesian meshes. The package of Fortran 90 subroutines, called PARAMESH, is designed to provide an application developer with an easy route to extend an existing serial code which uses a logically Cartesian structured mesh into a parallel code with adaptive mesh refinement. Alternatively, in its simplest use, and with minimal effort, it can operate as a domain decomposition tool for users who want to parallelize their serial codes, but who do not wish to use adaptivity. The package can provide them with an incremental evolutionary path for their code, converting it first to uniformly refined parallel code, and then later if they so desire, adding adaptivity.},
   author = {Peter MacNeice and Kevin M Olson and Clark Mobarry and Rosalinda de Fainchtein and Charles Packer},
   doi = {https://doi.org/10.1016/S0010-4655(99)00501-9},
   issn = {0010-4655},
   issue = {3},
   journal = {Computer Physics Communications},
   pages = {330-354},
   title = {PARAMESH: A parallel adaptive mesh refinement community toolkit},
   volume = {126},
   url = {https://www.sciencedirect.com/science/article/pii/S0010465599005019},
   year = {2000},
}

@article{Chalmers2019,
   abstract = {We present a parallel hp-adaptive high order (spectral) discontinuous Galerkin method for approximation of the incompressible Navier-Stokes equations. The spatial discretization consists of equal-order polynomial approximations of the fluid velocity and pressure via discontinuous Galerkin spatial discretizations. For the nonlinear convective term we select the local Lax-Friedrichs flux, while for the divergence and gradient operators central fluxes are chosen. For the diffusive term, we use an interior penalty discontinuous Galerkin method to ensure stability and invertibility. The temporal discretization is an implicit-explicit Runge-Kutta method paired with a high-order splitting procedure to efficiently enforce the incompressibility condition at each time step. The compact stencil size, explicit time stepping of nonlinear terms, and inversion of sparse linear systems make the resulting method simple to parallelize while the local nature of the discontinuous Galerkin approximation makes hp-adaptive refinement natural to implement. We detail our implementation consisting of a tensor product basis of high order polynomials on quadrilateral elements, and implement hp-adaptivity using an inexpensive a posteriori error estimator to determine where refinement is necessary. p-Multigrid and pressure projection techniques are used to precondition the conjugate gradient linear solvers. We present several numerical tests to demonstrate the efficacy of the method, in particular in reducing the number of degrees of freedom needed and allocating computing resources to regions of sharp variation in transient incompressible Navier-Stokes flows.},
   author = {N Chalmers and G Agbaglah and M Chrust and C Mavriplis},
   doi = {https://doi.org/10.1016/j.jcpx.2019.100023},
   issn = {2590-0552},
   journal = {Journal of Computational Physics: X},
   keywords = {Adaptive,Discontinuous Galerkin method,High-order,Incompressible flow,Navier-Stokes equations,Spectral element method},
   pages = {100023},
   title = {A parallel hp-adaptive high order discontinuous Galerkin method for the incompressible Navier-Stokes equations},
   volume = {2},
   url = {https://www.sciencedirect.com/science/article/pii/S2590055219300393},
   year = {2019},
}

@article{Karypis1997,
   author = {George Karypis and Vipin Kumar},
   title = {METIS: A software package for partitioning unstructured graphs, partitioning meshes, and computing fill-reducing orderings of sparse matrices},
   year = {1997},
}

@article{Karypis1997P,
   author = {George Karypis and Kirk Schloegel and Vipin Kumar},
   title = {Parmetis: Parallel graph partitioning and sparse matrix ordering library},
   year = {1997},
}

@article{Bryan2014,
   abstract = {This paper describes the open-source code Enzo, which uses block-structured adaptive mesh refinement to provide high spatial and temporal resolution for modeling astrophysical fluid flows. The code is Cartesian, can be run in one, two, and three dimensions, and supports a wide variety of physics including hydrodynamics, ideal and non-ideal magnetohydrodynamics, N-body dynamics (and, more broadly, self-gravity of fluids and particles), primordial gas chemistry, optically thin radiative cooling of primordial and metal-enriched plasmas (as well as some optically-thick cooling models), radiation transport, cosmological expansion, and models for star formation and feedback in a cosmological context. In addition to explaining the algorithms implemented, we present solutions for a wide range of test problems, demonstrate the code's parallel performance, and discuss the Enzo collaboration's code development methodology.},
   author = {Greg L Bryan and Michael L Norman and Brian W OShea and Tom Abel and John H Wise and Matthew J Turk and Daniel R Reynolds and David C Collins and Peng Wang and Samuel W Skillman and Britton Smith and Robert P Harkness and James Bordner and Ji-hoon Kim and Michael Kuhlen and Hao Xu and Nathan Goldbaum and Cameron Hummels and Alexei G Kritsuk and Elizabeth Tasker and Stephen Skory and Christine M Simpson and Oliver Hahn and Jeffrey S Oishi and Geoffrey C So and Fen Zhao and Renyue Cen and Yuan Li},
   doi = {10.1088/0067-0049/211/2/19},
   issue = {2},
   journal = {The Astrophysical Journal Supplement Series},
   month = {3},
   pages = {19},
   publisher = {American Astronomical Society},
   title = {ENZO: AN ADAPTIVE MESH REFINEMENT CODE FOR ASTROPHYSICS},
   volume = {211},
   url = {https://doi.org/10.1088/0067-0049/211/2/19},
   year = {2014},
}

@article{Schive2010,
   abstract = {We present the newly developed code, GPU-accelerated Adaptive-MEsh-Refinement code (GAMER), which adopts a novel approach in improving the performance of adaptive-mesh-refinement (AMR) astrophysical simulations by a large factor with the use of the graphic processing unit (GPU). The AMR implementation is based on a hierarchy of grid patches with an oct-tree data structure. We adopt a three-dimensional relaxing total variation diminishing scheme for the hydrodynamic solver and a multi-level relaxation scheme for the Poisson solver. Both solvers have been implemented in GPU, by which hundreds of patches can be advanced in parallel. The computational overhead associated with the data transfer between the CPU and GPU is carefully reduced by utilizing the capability of asynchronous memory copies in GPU, and the computing time of the ghost-zone values for each patch is diminished by overlapping it with the GPU computations. We demonstrate the accuracy of the code by performing several standard test problems in astrophysics. GAMER is a parallel code that can be run in a multi-GPU cluster system. We measure the performance of the code by performing purely baryonic cosmological simulations in different hardware implementations, in which detailed timing analyses provide comparison between the computations with and without GPU(s) acceleration. Maximum speed-up factors of 12.19 and 10.47 are demonstrated using one GPU with 40963 effective resolution and 16 GPUs with 81923 effective resolution, respectively.},
   author = {Hsi-Yu Schive and Yu-Chih Tsai and Tzihong Chiueh},
   doi = {10.1088/0067-0049/186/2/457},
   issue = {2},
   journal = {The Astrophysical Journal Supplement Series},
   month = {2},
   pages = {457-484},
   publisher = {American Astronomical Society},
   title = {GAMER
: A GRAPHIC PROCESSING UNIT ACCELERATED ADAPTIVE-MESH-REFINEMENT CODE FOR ASTROPHYSICS},
   volume = {186},
   url = {https://doi.org/10.1088/0067-0049/186/2/457},
   year = {2010},
}

@article{Wang2020,
   author = {Feng Wang and Nathan Marshak and Will Usher and Carsten Burstedde and Aaron Knoll and Timo Heister and Chris R Johson},
   doi = {10.1111/cgf.13958},
   journal = {Computer Graphics Forum},
   title = {CPU Ray Tracing of Tree-Based Adaptive Mesh Refinement Data},
   year = {2020},
}

@article{Burstedde2011,
   abstract = { We present scalable algorithms for parallel adaptive mesh refinement and coarsening (AMR), partitioning, and 2:1 balancing on computational domains composed of multiple connected two-dimensional quadtrees or three-dimensional octrees, referred to as a forest of octrees. By distributing the union of octants from all octrees in parallel, we combine the high scalability proven previously for adaptive single-octree algorithms with the geometric flexibility that can be achieved by arbitrarily connected hexahedral macromeshes, in which each macroelement is the root of an adapted octree. A key concept of our approach is an encoding scheme of the interoctree connectivity that permits arbitrary relative orientations between octrees. Based on this encoding we develop interoctree transformations of octants. These form the basis for high-level parallel octree algorithms, which are designed to interact with an application code such as a numerical solver for partial differential equations. We have implemented and tested these algorithms in the p4est software library. We demonstrate the parallel scalability of p4est on its own and in combination with two geophysics codes. Using p4est we generate and adapt multioctree meshes with up to $5.1310^11$ octants on as many as 220,320 CPU cores and execute the 2:1 balance algorithm in less than 10 seconds per million octants per process. },
   author = {Carsten Burstedde and Lucas C Wilcox and Omar Ghattas},
   doi = {10.1137/100791634},
   issue = {3},
   journal = {SIAM Journal on Scientific Computing},
   pages = {1103-1133},
   title = {p4est: Scalable Algorithms for Parallel Adaptive Mesh Refinement on Forests of Octrees},
   volume = {33},
   url = {https://doi.org/10.1137/100791634},
   year = {2011},
}
