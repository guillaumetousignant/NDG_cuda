@inproceedings{Mavriplis1990,
   abstract = {Aspects of adaptive spectral element methods are presented with emphasis on the a posteriori error estimators used in the automatic mesh refinement process. The nonconforming formulation of the method is reviewed in an effort to illustrate the various mesh refinement options available. Single mesh a posteriori error estimators are developed for the spectral element method. These estimate the actual error incurred by the discretization on a local per element basis and predict the convergence behaviour of the numerical solution as well. As a result the error estimators serve as criteria in the choice of refinement options. The usefulness of the estimators is demonstrated through examples of Navier-Stokes calculations.},
   author = {Catherine Mavriplis},
   city = {Wiesbaden},
   editor = {Pieter Wesseling},
   isbn = {978-3-663-13975-1},
   booktitle = {Proceedings of the Eighth GAMM-Conference on Numerical Methods in Fluid Mechanics},
   pages = {333-342},
   publisher = {Vieweg+Teubner Verlag},
   title = {A Posteriori Error Estimators for Adaptive Spectral Element Techniques},
   year = {1990},
}

@inproceedings{Maday1989,
   author = {Yvon Maday and Cathy Mavriplis and Anthony T Patera},
   booktitle = {Domain Decomposition Methods},
   keywords = {Decomposition,Discrete Functions,Domains,Finite Element Method,Fluid Mechanics and Heat Transfer,Grid Generation (Mathematics),Navier-Stokes Equation,Partial Differential Equations,Spectral Methods,Weighting Functions},
   month = {1},
   pages = {392-418},
   title = {Nonconforming mortar element methods - Application to spectral discretizations},
   year = {1989},
}

@book{Kopriva2009,
   abstract = {This book offers a systematic and self-contained approach to solvepartial differential
equations numerically using single and multidomain spectralmethods. It contains detailed
algorithms in pseudocode for the applicationof spectral approximations to both one
and two dimensional PDEsof mathematical physics describing potentials,transport, and
wave propagation. David Kopriva, a well-known researcherin the field with extensive
practical experience, shows how only a fewfundamental algorithms form the building
blocks of any spectral code, evenfor problems with complex geometries. The book addresses
computationaland applications scientists, as it emphasizes thepractical derivation
and implementation of spectral methods over abstract mathematics. It is divided into
two parts: First comes a primer on spectralapproximation and the basic algorithms,
including FFT algorithms, Gaussquadrature algorithms, and how to approximate derivatives.
The secondpart shows how to use those algorithms to solve steady and time dependent
PDEs in one and two space dimensions. Exercises and questions at theend of each chapter
encourage the reader to experiment with thealgorithms.},
   author = {David A Kopriva},
   edition = {1st},
   isbn = {9048122600},
   publisher = {Springer Publishing Company, Incorporated},
   title = {Implementing Spectral Methods for Partial Differential Equations: Algorithms for Scientists and Engineers},
   year = {2009},
}

@article{Harris2007,
   author = {Mark Harris and others},
   issue = {4},
   journal = {Nvidia Developer Technology},
   pages = {70},
   publisher = {NVIDIA Corp. California},
   title = {Optimizing parallel reduction in CUDA},
   volume = {2},
   year = {2007},
}

@online{Nvidia2021,
  author = {Nvidia},
  title = {CUDA C++ Programming Guide},
  month = {11},
  year = {2021},
  url = {https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html},
  urldate = {2021-11-29}
}

@article{Giuliani2019,
   author = {Andrew Giuliani and Lilia Krivodonova},
   doi = {10.1016/j.jcp.2018.12.019},
   journal = {Journal of Computational Physics},
   month = {11},
   title = {Adaptive mesh refinement on graphics processing units for applications in gas dynamics},
   volume = {381},
   year = {2019},
}

@online{Clucas1999,
   author = {Jean Clucas and Pam Walatka},
   month = {8},
   title = {FAST User Guide},
   url = {https://www.nas.nasa.gov/Software/FAST/RND-93-010.walatka-clucas/htmldocs/chp_16.surferu.html},
   year = {1999},
   urldate = {2021-12-13}
}

@article{Hilbert1891,
   author = {David Hilbert},
   issue = {38},
   journal = {Mathematische Annalen},
   pages = {459-460},
   title = {Über die stetige Abbildung einer Linie auf ein Flächenstück.},
   url = {http://www.digizeitschriften.de/dms/img/?PPN=PPN235181684_0038&DMDID=dmdlog40},
   year = {1891},
}

@article{Peano1890,
   author = {Giuseppe Peano},
   issue = {36},
   journal = {Mathematische Annalen},
   pages = {157-160},
   title = {Sur une courbe, qui remplit toute une aire plane.},
   url = {http://www.digizeitschriften.de/dms/img/?PPN=PPN235181684_0036&DMDID=dmdlog13},
   year = {1890},
}

@article{Haverkort2011,
   author = {Herman J Haverkort},
   journal = {CoRR},
   title = {An inventory of three-dimensional Hilbert space-filling curves},
   volume = {abs/1109.2323},
   url = {http://arxiv.org/abs/1109.2323},
   year = {2011},
}

@article{Karypis1998,
   author = {George Karypis and Vipin Kumar},
   journal = {SIAM Journal on Scientific Computing},
   pages = {359-392},
   title = {A fast and high quality multilevel scheme for partitioning irregular graphs},
   volume = {20},
   year = {1998},
}

@article{Pinar2004,
   abstract = {The one-dimensional decomposition of nonuniform workload arrays with optimal load balancing is investigated. The problem has been studied in the literature as the “chains-on-chains partitioning” problem. Despite the rich literature on exact algorithms, heuristics are still used in parallel computing community with the “hope” of good decompositions and the “myth” of exact algorithms being hard to implement and not runtime efficient. We show that exact algorithms yield significant improvements in load balance over heuristics with negligible overhead. Detailed pseudocodes of the proposed algorithms are provided for reproducibility. We start with a literature review and propose improvements and efficient implementation tips for these algorithms. We also introduce novel algorithms that are asymptotically and runtime efficient. Our experiments on sparse matrix and direct volume rendering datasets verify that balance can be significantly improved by using exact algorithms. The proposed exact algorithms are 100 times faster than a single sparse-matrix vector multiplication for 64-way decompositions on the average. We conclude that exact algorithms with proposed efficient implementations can effectively replace heuristics.},
   author = {Ali Pınar and Cevdet Aykanat},
   doi = {10.1016/j.jpdc.2004.05.003},
   issn = {0743-7315},
   issue = {8},
   journal = {Journal of Parallel and Distributed Computing},
   keywords = {Chains-on-chains partitioning,Dynamic programming,Image-space parallel volume rendering,Iterative refinement,One-dimensional partitioning,Optimal load balancing,Parallel sparse matrix vector multiplication,Parametric search},
   pages = {974-996},
   title = {Fast optimal load balancing algorithms for 1D partitioning},
   volume = {64},
   year = {2004},
}

@article{Patera1984,
   abstract = {A spectral element method that combines the generality of the finite element method with the accuracy of spectral techniques is proposed for the numerical solution of the incompressible Navier-Stokes equations. In the spectral element discretization, the computational domain is broken into a series of elements, and the velocity in each element is represented as a high-order Lagrangian interpolant through Chebyshev collocation points. The hyperbolic piece of the governing equations is then treated with an explicit collocation scheme, while the pressure and viscous contributions are treated implicitly with a projection operator derived from a variational principle. The implementation of the technique is demonstrated on a one-dimensional inflow-outflow advection-diffusion equation, and the method is then applied to laminar two-dimensional (separated) flow in a channel expansion. Comparisons are made with experiment and previous numerical work.},
   author = {Anthony T Patera},
   doi = {10.1016/0021-9991(84)90128-1},
   issn = {0021-9991},
   issue = {3},
   journal = {Journal of Computational Physics},
   pages = {468-488},
   title = {A spectral element method for fluid dynamics: Laminar flow in a channel expansion},
   volume = {54},
   year = {1984},
}

@book{Gottlieb1977,
   author = {David Gottlieb and Steven A Orszag},
   publisher = {SIAM},
   title = {Numerical analysis of spectral methods: theory and applications},
   year = {1977},
   isbn = {978-0-89871-023-6},
}

@book{Toro2009,
   author = {Eleuterio Toro},
   doi = {10.1007/b79761},
   month = {1},
   title = {Riemann Solvers and Numerical Methods for Fluid Dynamics: A Practical Introduction},
   year = {2009},
   publisher={Springer Science \& Business Media}
}

@article{Williamson1980,
   abstract = {All second-order, many third-order, and a few fourth-order Runge-Kutta schemes can be arranged to require only two storage locations per variable, compared with three needed by Gill's method.},
   author = {J H Williamson},
   doi = {10.1016/0021-9991(80)90033-9},
   issn = {0021-9991},
   issue = {1},
   journal = {Journal of Computational Physics},
   pages = {48-56},
   title = {Low-storage Runge-Kutta schemes},
   volume = {35},
   year = {1980},
}

@article{Gottlieb2001,
   author = {Sigal Gottlieb and Chi-Wang Shu and Eitan Tadmor},
   doi = {10.1137/S003614450036757X},
   journal = {SIAM Review},
   month = {1},
   title = {Strong Stability-Preserving High-Order Time Discretization Methods},
   volume = {43},
   year = {2001},
}

@report{Slotnick2014,
   author = {Jeffrey Slotnick and Abdollah Khodadoust and Juan Alonso and William Gropp and Dimitri Mavriplis},
   title = {CFD Vision 2030 Study: A Path to Revolutionary Computational Aerosciences},
   url = {http://www.sti.nasa.gov},
   year = {2014},
}

@article{Stern2001,
   author = {Fred Stern and Robert V Wilson and Hugh W Coleman and Eric G Paterson},
   issue = {4},
   journal = {J. Fluids Eng.},
   pages = {793-802},
   title = {Comprehensive approach to verification and validation of CFD simulations—part 1: methodology and procedures},
   volume = {123},
   year = {2001},
}

@book{Karniadakis2005,
   author = {George Karniadakis and Spencer Sherwin},
   doi = {10.1093/acprof:oso/9780198528692.001.0001},
   month = {1},
   title = {Spectral/HP Element Methods for Computational Fluid Dynamics},
   year = {2005},
   isbn = {9780198528692},
   publisher={Oxford University Press on Demand},
}

@inproceedings{Fan2004,
   abstract = {Inspired by the attractive Flops/dollar ratio and the incredible growth in the speed of modern graphics processing units (GPUs), we propose to use a cluster of GPUs for high performance scientific computing. As an example application, we have developed a parallel flow simulation using the lattice Boltzmann model (LBM) on a GPU cluster and have simulated the dispersion of airborne contaminants in the Times Square area of New York City. Using 30 GPU nodes, our simulation can compute a 480x400x80 LBM in 0.31 second/step, a speed which is 4.6 times faster than that of our CPU cluster implementation. Besides the LBM, we also discuss other potential applications of the GPU cluster, such as cellular automata, PDE solvers, and FEM.},
   author = {Zhe Fan and Feng Qiu and A Kaufman and S Yoakum-Stover},
   doi = {10.1109/SC.2004.26},
   booktitle = {SC '04: Proceedings of the 2004 ACM/IEEE Conference on Supercomputing},
   month = {11},
   pages = {47},
   title = {GPU Cluster for High Performance Computing},
   year = {2004},
}

@article{Owens2008,
   abstract = {The graphics processing unit (GPU) has become an integral part of today's mainstream computing systems. Over the past six years, there has been a marked increase in the performance and capabilities of GPUs. The modern GPU is not only a powerful graphics engine but also a highly parallel programmable processor featuring peak arithmetic and memory bandwidth that substantially outpaces its CPU counterpart. The GPU's rapid increase in both programmability and capability has spawned a research community that has successfully mapped a broad range of computationally demanding, complex problems to the GPU. This effort in general-purpose computing on the GPU, also known as GPU computing, has positioned the GPU as a compelling alternative to traditional microprocessors in high-performance computer systems of the future. We describe the background, hardware, and programming model for GPU computing, summarize the state of the art in tools and techniques, and present four GPU computing successes in game physics and computational biophysics that deliver order-of-magnitude performance gains over optimized CPU applications.},
   author = {John D Owens and Mike Houston and David Luebke and Simon Green and John E Stone and James C Phillips},
   doi = {10.1109/JPROC.2008.917757},
   issn = {1558-2256},
   issue = {5},
   journal = {Proceedings of the IEEE},
   month = {5},
   pages = {879-899},
   title = {GPU Computing},
   volume = {96},
   year = {2008},
}

@inproceedings{Lee2010,
   abstract = {Recent advances in computing have led to an explosion in the amount of data being generated. Processing the ever-growing data in a timely manner has made throughput computing an important aspect for emerging applications. Our analysis of a set of important throughput computing kernels shows that there is an ample amount of parallelism in these kernels which makes them suitable for today's multi-core CPUs and GPUs. In the past few years there have been many studies claiming GPUs deliver substantial speedups (between 10X and 1000X) over multi-core CPUs on these kernels. To understand where such large performance difference comes from, we perform a rigorous performance analysis and find that after applying optimizations appropriate for both CPUs and GPUs the performance gap between an Nvidia GTX280 processor and the Intel Core i7-960 processor narrows to only 2.5x on average. In this paper, we discuss optimization techniques for both CPU and GPU, analyze what architecture features contributed to performance differences between the two architectures, and recommend a set of architectural features which provide significant improvement in architectural efficiency for throughput kernels.},
   author = {Victor W Lee and Changkyu Kim and Jatin Chhugani and Michael Deisher and Daehyun Kim and Anthony D Nguyen and Nadathur Satish and Mikhail Smelyanskiy and Srinivas Chennupaty and Per Hammarlund and Ronak Singhal and Pradeep Dubey},
   city = {New York, NY, USA},
   doi = {10.1145/1815961.1816021},
   booktitle = {Proceedings of the 37th Annual International Symposium on Computer Architecture},
   isbn = {9781450300537},
   keywords = {cpu architecture,gpu architecture,performance analysis,performance measurement,software optimization,throughput computing},
   pages = {451-460},
   publisher = {Association for Computing Machinery},
   title = {Debunking the 100X GPU vs. CPU Myth: An Evaluation of Throughput Computing on CPU and GPU},
   year = {2010},
}

@article{Nayfeh1997,
   abstract = {Presents the case for billion-transistor processor architectures that will consist of chip multiprocessors (CMPs): multiple (four to 16) simple, fast processors on one chip. In their proposal, each processor is tightly coupled to a small, fast, level-one cache, and all processors share a larger level-two cache. The processors may collaborate on a parallel job or run independent tasks (as in the SMT proposal). The CMP architecture lends itself to simpler design, faster validation, cleaner functional partitioning, and higher theoretical peak performance. However for this architecture to realize its performance potential, either programmers or compilers will have to make code explicitly parallel. Old ISAs will be incompatible with this architecture (although they could run slowly on one of the small processors).},
   author = {B A Nayfeh and K Olukotun},
   doi = {10.1109/2.612253},
   issn = {1558-0814},
   issue = {9},
   journal = {Computer},
   month = {9},
   pages = {79-85},
   title = {A single-chip multiprocessor},
   volume = {30},
   year = {1997},
}

@article{Barroso2005,
   abstract = {In the late 1990s, our research group at DEC was one of a growing number of teams advocating the CMP (chip multiprocessor) as an alternative to highly complex single-threaded CPUs. We were designing the Piranha system,1 which was a radical point in the CMP design space in that we used very simple cores (similar to the early RISC designs of the late ’80s) to provide a higher level of thread-level parallelism. Our main goal was to achieve the best commercial workload performance for a given silicon budget. Today, in developing Google’s computing infrastructure, our focus is broader than performance alone. The merits of a particular architecture are measured by answering the following question: Are you able to afford the computational capacity you need? The high-computational demands that are inherent in most of Google’s services have led us to develop a deep understanding of the overall cost of computing, and continually to look for hardware/software designs that optimize performance per unit of cost.},
   author = {Luiz André Barroso},
   city = {New York, NY, USA},
   doi = {10.1145/1095408.1095420},
   issn = {1542-7730},
   issue = {7},
   journal = {Queue},
   month = {9},
   pages = {48-53},
   publisher = {Association for Computing Machinery},
   title = {The Price of Performance: An Economic Case for Chip Multiprocessing},
   volume = {3},
   year = {2005},
}

@inproceedings{Parkhurst2006,
   abstract = {In the past, processor design trends were dominated by increasingly complex feature sets, higher clock speeds, growing thermal envelopes and increasing power dissipation. Recently, clock speeds have tapered and thermal and power dissipation envelopes have remained flat. However, the demand for increasing performance continues which has fueled the move to integrated multiple processor (multi-core) designs. This paper discusses this trend towards multi-core processor designs, the design challenges that accompany it and a view of the research required to support it.},
   author = {Jeff Parkhurst and John Darringer and Bill Grundmann},
   city = {New York, NY, USA},
   doi = {10.1145/1233501.1233516},
   booktitle = {Proceedings of the 2006 IEEE/ACM International Conference on Computer-Aided Design},
   isbn = {1595933891},
   pages = {67-72},
   publisher = {Association for Computing Machinery},
   title = {From Single Core to Multi-Core: Preparing for a New Exponential},
   year = {2006},
}

@article{Garland2008,
   abstract = {The CUDA programming model provides a straightforward means of describing inherently parallel computations, and NVIDIA's Tesla GPU architecture delivers high computational throughput on massively parallel problems. This article surveys experiences gained in applying CUDA to a diverse set of problems and the parallel speedups over sequential codes running on traditional CPU architectures attained by executing key computations on the GPU.},
   author = {Michael Garland and Scott Le Grand and John Nickolls and Joshua Anderson and Jim Hardwick and Scott Morton and Everett Phillips and Yao Zhang and Vasily Volkov},
   doi = {10.1109/MM.2008.57},
   issn = {1937-4143},
   issue = {4},
   journal = {IEEE Micro},
   month = {7},
   pages = {13-27},
   title = {Parallel Computing Experiences with CUDA},
   volume = {28},
   year = {2008},
}

@book{Deville2003,
   author = {M O Deville and P Fischer and E H Mund},
   doi = {10.1115/1.1566402},
   title = {High-Order Methods for Incompressible Fluid Flow},
   year={2002},
   publisher={Cambridge University Press},
   address={Cambridge},
   series={Cambridge Monographs on Applied and Computational Mathematics},
   doi={10.1017/CBO9780511546792},
   collection={Cambridge Monographs on Applied and Computational Mathematics},
}

@thesis{Mavriplis1989,
   author = {Cathy Mavriplis},
   institution = {Massachusetts Institute of Technology},
   title = {Nonconforming discretizations and a posteriori error estimators for adaptive spectral element techniques},
   year = {1989},
}

@report{Reed1973,
   author = {William H Reed and Thomas R Hill},
   institution = {Los Alamos Scientific Lab., N. Mex.(USA)},
   title = {Triangular mesh methods for the neutron transport equation},
   year = {1973},
}

@book{Hesthaven2007,
    author = {Jan Hesthaven and Tim Warburton},
    title = {Nodal Discontinuous Galerkin Methods: Algorithms, Analysis, and Applications},
    year = {2007},
    month = {2},
    isbn = {0387720650},
    publisher = {Springer Publishing Company, Incorporated},
    edition = {1st},
    abstract = {The text offers an introduction to the key ideas, basic analysis, and efficient implementation of discontinuous Galerkin finite element methods (DG-FEM) for the solution of partial differential equations. All key theoretical results are either derived or discussed, including an overview of relevant results from approximation theory, convergence theory for numerical PDEs, orthogonal polynomials etc. Through embedded Matlab codes, the algorithms are discussed and implemented for a number of classic systems of PDEs, e.g., Maxwells equations, Euler equations, incompressible Navier-Stokes equations, and Poisson- and Helmholtz equations. These developments are done in detail inone and two dimensions on general unstructured grids with high-order elements and all essential routines for 3D extensions are also included and discussed briefly. The three appendices contain an overview of orthogonal polynomials and associated library routines used throughout, a brief introduction to grid generation, and an overview of the associated software (where to get it, list of variables etc). A variety of exercises are included at the end of most chapters.}
}

@inproceedings{Amdahl1967,
   abstract = {For over a decade prophets have voiced the contention that the organization of a single computer has reached its limits and that truly significant advances can be made only by interconnection of a multiplicity of computers in such a manner as to permit cooperative solution. Variously the proper direction has been pointed out as general purpose computers with a generalized interconnection of memories, or as specialized computers with geometrically related memory interconnections and controlled by one or more instruction streams.},
   author = {Gene M Amdahl},
   city = {New York, NY, USA},
   doi = {10.1145/1465482.1465560},
   booktitle = {AFIPS '67 (Spring): Proceedings of the April 18-20, 1967, Spring Joint Computer Conference},
   isbn = {9781450378956},
   pages = {483-485},
   publisher = {Association for Computing Machinery},
   title = {Validity of the Single Processor Approach to Achieving Large Scale Computing Capabilities},
   year = {1967},
}

@article{Gustafson1988,
   author = {John L Gustafson},
   city = {New York, NY, USA},
   doi = {10.1145/42411.42415},
   issn = {0001-0782},
   issue = {5},
   journal = {Commun. ACM},
   month = {5},
   pages = {532-533},
   publisher = {Association for Computing Machinery},
   title = {Reevaluating Amdahl's Law},
   volume = {31},
   year = {1988},
}

@article{Berger1984,
   abstract = {An adaptive method based on the idea of multiple component grids for the solution of hyperbolic partial differential equations using finite difference techniques is presented. Based upon Richardson-type estimates of the truncation error, refined grids are created or existing ones removed to attain a given accuracy for a minimum amount of work. The approach is recursive in that fine grids can contain even finer grids. The grids with finer mesh width in space also have a smaller mesh width in time, making this a mesh refinement algorithm in time and space. We present the algorithm, error estimation procedure, and the data structures, and conclude with numerical examples in one and two space dimensions.},
   author = {Marsha J Berger and Joseph Oliger},
   doi = {10.1016/0021-9991(84)90073-1},
   issn = {0021-9991},
   issue = {3},
   journal = {Journal of Computational Physics},
   pages = {484-512},
   title = {Adaptive mesh refinement for hyperbolic partial differential equations},
   volume = {53},
   year = {1984},
}

@article{Khokhlov1998,
   abstract = {A fully threaded tree (FTT) for adaptive mesh refinement (AMR) of regular meshes is described. By using a tree threaded at all levels, tree traversals for finding nearest neighbors are avoided. All operations on a tree including tree modifications areO(N), whereNis a number of cells and can be performed in parallel. An implementation of the tree requires 2Nwords of memory. In this paper, FTT is applied to the integration of the Euler equations of fluid dynamics. The integration on a tree can utilize flux evaluation algorithms used for grids, but requires a different time-stepping strategy to be computationally efficient. An adaptive-mesh time-stepping algorithm is described in which different time steps are used at different levels of the tree. Time stepping and mesh refinement are interleaved to avoid extensive buffer layers of fine mesh which were otherwise required ahead of moving shocks. A filtering algorithm for removing high-frequency noise during mesh refinement is described. Test examples are presented, and the FTT performance is evaluated.},
   author = {A M Khokhlov},
   doi = {10.1006/jcph.1998.9998},
   issn = {0021-9991},
   issue = {2},
   journal = {Journal of Computational Physics},
   pages = {519-543},
   title = {Fully Threaded Tree Algorithms for Adaptive Refinement Fluid Dynamics Simulations},
   volume = {143},
   year = {1998},
}

@article{Cantwell2015,
   author = {C D Cantwell and D Moxey and A Comerford and A. Bolis and G Rocco and G Mengaldo and D De Grazia and S Yakovlev and J-E. Lombard and D Ekelschot and B. Jordi and H Xu and Y Mohamied and C Eskilsson and B Nelson and P Vos and C Biotto and R M Kirby and S J Sherwin},
   doi = {10.1016/j.cpc.2015.02.008},
   issn = {0010-4655},
   journal = {Computer Physics Communications},
   month = {7},
   pages = {205-219},
   publisher = {ELSEVIER SCIENCE BV},
   title = {Nektar plus plus : An open-source spectral/hp element framework},
   volume = {192},
   year = {2015},
}

@online{Siemens2020,
   author = {Siemens},
   journal = {Siemens Simcenter},
   month = {1},
   title = {Whistle while you mesh: Simcenter STAR-CCM+ model-driven adaptive mesh refinement (AMR)},
   year = {2020},
   url = {https://blogs.sw.siemens.com/simcenter/whistle-while-you-mesh-simcenter-star-ccm-model-driven-adaptive-mesh-refinement-amr},
   urldate = {2022-02-11}
}

@article{Schive2018,
   abstract = {We present gamer-2, a GPU-accelerated adaptive mesh refinement (AMR) code for astrophysics. It provides a rich set of features, including adaptive time-stepping, several hydrodynamic schemes, magnetohydrodynamics, self-gravity, particles, star formation, chemistry, and radiative processes with grackle, data analysis with yt, and memory pool for efficient object allocation. gamer-2 is fully bitwise reproducible. For the performance optimization, it adopts hybrid OpenMP/MPI/GPU parallelization and utilizes overlapping CPU computation, GPU computation, and CPU–GPU communication. Load balancing is achieved using a Hilbert space-filling curve on a level-by-level basis without the need to duplicate the entire AMR hierarchy on each MPI process. To provide convincing demonstrations of the accuracy and performance of gamer-2, we directly compare with enzo on isolated disc galaxy simulations and with flash on galaxy cluster merger simulations. We show that the physical results obtained by different codes are in very good agreement, and gamer-2 outperforms enzo and flash by nearly one and two orders of magnitude, respectively, on the Blue Waters supercomputers using 1–256 nodes. More importantly, gamer-2 exhibits similar or even better parallel scalability compared to the other two codes. We also demonstrate good weak and strong scaling using up to 4096 GPUs and 65 536 CPU cores, and achieve a uniform resolution as high as \\10\\, 240^3\\ cells. Furthermore, gamer-2 can be adopted as an AMR + GPUs framework and has been extensively used for the wave dark matter simulations. gamer-2 is open source (available at https://github.com/gamer-project/gamer) and new contributions are welcome.},
   author = {Hsi-Yu Schive and John A ZuHone and Nathan J Goldbaum and Matthew J Turk and Massimo Gaspari and Chin-Yu Cheng},
   doi = {10.1093/mnras/sty2586},
   issn = {0035-8711},
   issue = {4},
   journal = {Monthly Notices of the Royal Astronomical Society},
   month = {2},
   pages = {4815-4840},
   title = {gamer-2: a GPU-accelerated adaptive mesh refinement code – accuracy, performance, and scalability},
   volume = {481},
   year = {2018},
}

@article{Offermans2017,
   author = {Nicolas Offermans and Oana Marin and Michel Schanen and Jing Gong and Paul F Fischer and Philipp Schlatter and Aleks Obabko and Adam Peplinski and Maxwell Hutchinson and Elia Merzari},
   journal = {CoRR},
   title = {On the Strong Scaling of the Spectral Element Solver Nek5000 on Petascale Systems},
   volume = {abs/1706.02970},
   url = {http://arxiv.org/abs/1706.02970},
   year = {2017},
   urldate = {2022-02-21},
}

@article{Fischer2021,
   author = {Paul Fischer and Stefan Kerkemeier and Misun Min and Yu-Hsiang Lan and Malachi Phillips and Thilina Rathnayake and Elia Merzari and Ananias Tomboulides and Ali Karakus and Noel Chalmers and Tim Warburton},
   journal = {CoRR},
   title = {NekRS, a GPU-Accelerated Spectral Element Navier-Stokes Solver},
   volume = {abs/2104.05829},
   url = {https://arxiv.org/abs/2104.05829},
   year = {2021},
   urldate = {2022-02-21},
}

@article{Medina2014,
   author = {David S Medina and Amik St.-Cyr and Timothy Warburton},
   journal = {CoRR},
   title = {OCCA: A unified approach to multi-threading languages},
   volume = {abs/1403.0968},
   url = {http://arxiv.org/abs/1403.0968},
   year = {2014},
   urldate = {2022-02-21},
}

@inproceedings{Krawezik2009,
   author = {Geraud Krawezik and Gene Poole},
   month = {2},
   title = {Accelerating the ANSYS Direct Sparse Solver with GPUs},
   year = {2009},
   booktitle = {Symposium on Application Accelerators in High Performance Computing},
}

@article{Naumov2015,
   abstract = { The solution of large sparse linear systems arises in many applications, such as computational fluid dynamics and oil reservoir simulation. In realistic cases the matrices are often so large that they require large scale distributed parallel computing to obtain the solution of interest in a reasonable time. In this paper we discuss the design and implementation of the AmgX library, which provides drop-in GPU acceleration of distributed algebraic multigrid (AMG) and preconditioned iterative methods. The AmgX library implements both classical and aggregation-based AMG methods with different selector and interpolation strategies, along with a variety of smoothers and preconditioners, including block-Jacobi, Gauss–Seidel, and incomplete-LU factorization. The library contains many of the standard and flexible preconditioned Krylov subspace iterative methods, which can be combined with any of the available multigrid methods or simpler preconditioners. The parallelism in the aggregation scheme exploits parallel graph matching techniques, while the smoothers and preconditioners often rely on parallel graph coloring algorithms. The AMG algorithm implemented in the AmgX library achieves $2$–$5$ speedup on a single GPU against a competitive implementation on the CPU. As will be shown in the numerical experiments section, both setup and solve phases scale well across multiple nodes, sustaining this performance advantage. },
   author = {M Naumov and M Arsaev and P Castonguay and J Cohen and J Demouth and J Eaton and S Layton and N Markovskiy and I Reguly and N Sakharnykh and V Sellappan and R Strzodka},
   doi = {10.1137/140980260},
   issue = {5},
   journal = {SIAM Journal on Scientific Computing},
   pages = {S602-S626},
   title = {AmgX: A Library for GPU Accelerated Algebraic Multigrid and Preconditioned Iterative Methods},
   volume = {37},
   year = {2015},
}

@article{Alonazi2015,
   author = {Amani AlOnazi and David Keyes and Alexey Lastovetsky and Vladimir Rychkov},
   month = {2},
   title = {Design and Optimization of OpenFOAM-based CFD Applications for Hybrid and Heterogeneous HPC Platforms},
   year = {2015},
   journal={arXiv preprint arXiv:1505.07630},
   doi = {10.25781/KAUST-9M51I},
}

@online{SimFlow2020,
   author = {SimFlow},
   journal = {SimFlow},
   pages = {1-1},
   title = {RapidCFD GPU - OpenFOAM on GPU},
   year = {2020},
   url = {https://sim-flow.com/rapid-cfd-gpu/},
   urldate = {2022-02-21}
}

@thesis{He2021,
   author = {Shiqi He},
   city = {75 Laurier Ave. E, Ottawa},
   doi = {http://dx.doi.org/10.20381/ruor-26256},
   institution = {University of Ottawa thesis},
   month = {2},
   title = {Dynamic Load Balancing for a hp-adaptive Discontinuous Galerkin Wave Equation Solver via Spacing-Filling Curve and Advanced Data Structure},
   year = {2021},
}

@article{Plewa2005,
   author = {Tomasz Plewa and Timur Linde and V Gregory Weirs and others},
   publisher = {Springer},
   title = {Adaptive mesh refinement-theory and applications},
   year = {2005},
}

@inproceedings{Obabko2017,
   author = {Aleksandr Obabko and Eduard Tsyrulnykov and Robert Rainsberger and Alvaro V Torreira and Hassan Nagib and Anil Agarwal and Paul F Fischer},
   booktitle = {APS Division of Fluid Dynamics Meeting Abstracts},
   pages = {A11–008},
   title = {Large-Eddy Simulation of Flows Through a Novel Vascular Access Device for Hemodialysis Access},
   year = {2017},
}

@article{Ameen2020,
   author = {Muhsin Ameen and Saumil Patel and Juan Colmenares and Tanmoy Chatterjee and Jackie Chen},
   journal = {DOE Vehicle Technologies Office Annual Merit Review},
   pages = {1-4},
   title = {Direct Numerical Simulation (DNS) and high-fidelity large-eddy simulations for improved prediction of in-cylinder flow and combustion processes},
   year = {2020},
}

@article{Merzari2020,
   author = {Elia Merzari and Paul Fischer and Misun Min and Stefan Kerkemeier and Aleksandr Obabko and Dillon Shaver and Haomin Yuan and Yiqi Yu and Javier Martinez and Landon Brockmeyer and others},
   issue = {9},
   journal = {Nuclear Technology},
   pages = {1308-1324},
   publisher = {Taylor & Francis},
   title = {Toward exascale: overview of large eddy simulations and direct numerical simulations of nuclear reactor flows with the spectral element method in Nek5000},
   volume = {206},
   year = {2020},
}

@article{Mengaldo2020,
   author = {Gianmarco Mengaldo and David Moxey and Michael Turner and Rodrigo Costa Moura and Ayad Jassim and Mark Taylor and Joaquim Peiró and Spencer J Sherwin},
   journal = {CoRR},
   title = {Industry-Relevant Implicit Large-Eddy Simulation of a High-Performance Road Car via Spectral/hp Element Methods},
   volume = {abs/2009.10178},
   url = {https://arxiv.org/abs/2009.10178},
   year = {2020},
}

@article{Busch2011,
   abstract = {Abstract This article reviews the state of the recently developed discontinuous Galerkin finite element method for the efficient numerical treatment of nanophotonic systems. This approach combines the accurate and flexible spatial discretisation of classical finite elements with efficient time stepping capabilities. We describe in detail the underlying principles of the discontinuous Galerkin technique and its application to the simulation of complex nanophotonic structures. In addition, formulations for both time- and frequency-domain solvers are provided and specific advantages and limitations of the technique are discussed. The potential of the discontinuous Galerkin approach is illustrated by modelling and simulating several experimentally relevant systems.},
   author = {K Busch and M König and J Niegemann},
   doi = {10.1002/lpor.201000045},
   issue = {6},
   journal = {Laser \& Photonics Reviews},
   keywords = {Maxwell's equations,Nanophotonics,discontinuous Galerkin methods.,frequency-domain simulations,light-matter interaction,plasmonics,time-domain simulations},
   pages = {773-809},
   title = {Discontinuous Galerkin methods in nanophotonics},
   volume = {5},
   year = {2011},
}

@article{Klockner2009,
   abstract = {Discontinuous Galerkin (DG) methods for the numerical solution of partial differential equations have enjoyed considerable success because they are both flexible and robust: They allow arbitrary unstructured geometries and easy control of accuracy without compromising simulation stability. Lately, another property of DG has been growing in importance: The majority of a DG operator is applied in an element-local way, with weak penalty-based element-to-element coupling. The resulting locality in memory access is one of the factors that enables DG to run on off-the-shelf, massively parallel graphics processors (GPUs). In addition, DG’s high-order nature lets it require fewer data points per represented wavelength and hence fewer memory accesses, in exchange for higher arithmetic intensity. Both of these factors work significantly in favor of a GPU implementation of DG. Using a single US400 Nvidia GTX 280 GPU, we accelerate a solver for Maxwell’s equations on a general 3D unstructured grid by a factor of around 50 relative to a serial computation on a current-generation CPU. In many cases, our algorithms exhibit full use of the device’s available memory bandwidth. Example computations achieve and surpass 200gigaflops/s of net application-level floating point work. In this article, we describe and derive the techniques used to reach this level of performance. In addition, we present comprehensive data on the accuracy and runtime behavior of the method.},
   author = {A Klöckner and T Warburton and J Bridge and J S Hesthaven},
   doi = {10.1016/j.jcp.2009.06.041},
   issn = {0021-9991},
   issue = {21},
   journal = {Journal of Computational Physics},
   keywords = {Discontinuous Galerkin,GPU,High order,Many-core,Maxwell’s equations,Parallel computation},
   pages = {7863-7882},
   title = {Nodal discontinuous Galerkin methods on graphics processors},
   volume = {228},
   year = {2009},
}

@article{Gassner2016,
   abstract = {In this work, we design an arbitrary high order accurate nodal discontinuous Galerkin spectral element type method for the one dimensional shallow water equations. The novel method uses a skew-symmetric formulation of the continuous problem. We prove that this discretisation exactly preserves the local mass and momentum. Furthermore, we show that combined with a special numerical interface flux function, the method exactly preserves the entropy, which is also the total energy for the shallow water equations. Finally, we prove that the surface fluxes, the skew-symmetric volume integrals, and the source term are well balanced. Numerical tests are performed to demonstrate the theoretical findings.},
   author = {Gregor J Gassner and Andrew R Winters and David A Kopriva},
   doi = {10.1016/j.amc.2015.07.014},
   issn = {0096-3003},
   journal = {Applied Mathematics and Computation},
   keywords = {Discontinuous Galerkin spectral element method,Entropy conservation,Gauss–Lobatto Legendre,Skew-symmetric shallow water equations,Summation-by-parts,Well balanced},
   note = {Recent Advances in Numerical Methods for Hyperbolic Partial Differential Equations},
   pages = {291-308},
   title = {A well balanced and entropy conservative discontinuous Galerkin spectral element method for the shallow water equations},
   volume = {272},
   year = {2016},
}

@article{Godel2010,
   abstract = {A multirate Adams-Bashforth (AB) scheme for simulation of electromagnetic wave propagation using the discontinuous Galerkin finite element method (DG-FEM) is presented. The algorithm is adapted such that single-instruction multiple-thread (SIMT) characteristic for the implementation on a graphics processing unit (GPU) is preserved. A domain decomposition strategy respecting the multirate classification for computation on multiple GPUs is presented. Accuracy and performance is analyzed with help of suitable benchmarks.},
   author = {Nico Gödel and Steffen Schomann and Tim Warburton and Markus Clemens},
   doi = {10.1109/TMAG.2010.2043655},
   issn = {1941-0069},
   issue = {8},
   journal = {IEEE Transactions on Magnetics},
   month = {8},
   pages = {2735-2738},
   title = {GPU Accelerated Adams–Bashforth Multirate Discontinuous Galerkin FEM Simulation of High-Frequency Electromagnetic Fields},
   volume = {46},
   year = {2010},
}

@inproceedings{Peplinski2016,
   abstract = {We discuss parallel performance of h-type Adaptive Mesh Refinement (AMR) developed for the high-order spectral element solver Nek5000 within CRESTA project. AMR is a desired feature of the future simulation software, as it gives possibility to increase the accuracy of numerical simulations at minimal computational cost by resolving particular region of the domain. At the same time it increases complexity of the communication pattern and introduces load imbalance, that can have negative effect on the code scalability. In this work we concentrate on the parallel performance of different tools required by AMR and the resulting algorithm limitations. Our implementation is based on available libraries for parallel mesh management (p4est) and partitioning (ParMetis) that provide necessary information for grid refinement/coarsening and redistribution performed within nonconforming version of Nek5000. For simplicity we consider advection-diffusion problem instead of the full Navies-Stokes equations and study both strong and weak scalability for the convected-cone problem. It is a synthetic test case allowing to test AMR with frequent dynamic mesh adjustments.},
   author = {Adam Peplinski and Paul F Fischer and Philipp Schlatter},
   city = {New York, NY, USA},
   doi = {10.1145/2938615.2938620},
   booktitle = {Proceedings of the Exascale Applications and Software Conference 2016},
   isbn = {9781450341226},
   keywords = {large-scale scientific computing,nonconforming methods,parallel adaptive mesh refinement,spectral elements},
   publisher = {Association for Computing Machinery},
   title = {Parallel Performance of H-Type Adaptive Mesh Refinement for Nek5000},
   year = {2016},
}

@inproceedings{Offermans2019,
   abstract = {When performing computational fluid dynamics (CFD) simulations of complex flows, the a priori knowledge of the flow physics and the location of the dominant flow features are usually unknown. For this reason, the development of adaptive remeshing techniques is crucial for large-scale computational problems. Some work has been made recently to provide Nek5000 with adaptive mesh refinement (AMR) capabilities in order to facilitate the generation of the grid and push forward the limit in terms of problem size and complexity [10].},
   author = {Nicolas Offermans and Adam Peplinski and Oana Marin and P F Fischer and Philipp Schlatter},
   city = {Cham},
   editor = {Vincenzo
and Fröhlich Jochen
and Geurts Bernard J.
and Kuerten Hans Salvetti Maria Vittoria
and Armenio},
   booktitle = {Direct and Large-Eddy Simulation XI},
   isbn = {978-3-030-04915-7},
   pages = {9-15},
   publisher = {Springer International Publishing},
   title = {Towards Adaptive Mesh Refinement for the Spectral Element Solver Nek5000},
   year = {2019},
}

@article{MacNeice2000,
   abstract = {In this paper we describe a community toolkit which is designed to provide parallel support with adaptive mesh capability for a large and important class of computational models, those using structured, logically Cartesian meshes. The package of Fortran 90 subroutines, called PARAMESH, is designed to provide an application developer with an easy route to extend an existing serial code which uses a logically Cartesian structured mesh into a parallel code with adaptive mesh refinement. Alternatively, in its simplest use, and with minimal effort, it can operate as a domain decomposition tool for users who want to parallelize their serial codes, but who do not wish to use adaptivity. The package can provide them with an incremental evolutionary path for their code, converting it first to uniformly refined parallel code, and then later if they so desire, adding adaptivity.},
   author = {Peter MacNeice and Kevin M Olson and Clark Mobarry and Rosalinda de Fainchtein and Charles Packer},
   doi = {10.1016/S0010-4655(99)00501-9},
   issn = {0010-4655},
   issue = {3},
   journal = {Computer Physics Communications},
   pages = {330-354},
   title = {PARAMESH: A parallel adaptive mesh refinement community toolkit},
   volume = {126},
   year = {2000},
}

@article{Chalmers2019,
   abstract = {We present a parallel hp-adaptive high order (spectral) discontinuous Galerkin method for approximation of the incompressible Navier-Stokes equations. The spatial discretization consists of equal-order polynomial approximations of the fluid velocity and pressure via discontinuous Galerkin spatial discretizations. For the nonlinear convective term we select the local Lax-Friedrichs flux, while for the divergence and gradient operators central fluxes are chosen. For the diffusive term, we use an interior penalty discontinuous Galerkin method to ensure stability and invertibility. The temporal discretization is an implicit-explicit Runge-Kutta method paired with a high-order splitting procedure to efficiently enforce the incompressibility condition at each time step. The compact stencil size, explicit time stepping of nonlinear terms, and inversion of sparse linear systems make the resulting method simple to parallelize while the local nature of the discontinuous Galerkin approximation makes hp-adaptive refinement natural to implement. We detail our implementation consisting of a tensor product basis of high order polynomials on quadrilateral elements, and implement hp-adaptivity using an inexpensive a posteriori error estimator to determine where refinement is necessary. p-Multigrid and pressure projection techniques are used to precondition the conjugate gradient linear solvers. We present several numerical tests to demonstrate the efficacy of the method, in particular in reducing the number of degrees of freedom needed and allocating computing resources to regions of sharp variation in transient incompressible Navier-Stokes flows.},
   author = {N Chalmers and G Agbaglah and M Chrust and C Mavriplis},
   doi = {10.1016/j.jcpx.2019.100023},
   issn = {2590-0552},
   journal = {Journal of Computational Physics: X},
   keywords = {Adaptive,Discontinuous Galerkin method,High-order,Incompressible flow,Navier-Stokes equations,Spectral element method},
   pages = {100023},
   title = {A parallel hp-adaptive high order discontinuous Galerkin method for the incompressible Navier-Stokes equations},
   volume = {2},
   year = {2019},
}

@article{Karypis1997,
   author = {George Karypis and Vipin Kumar},
   title = {METIS: A software package for partitioning unstructured graphs, partitioning meshes, and computing fill-reducing orderings of sparse matrices},
   year = {1997},
}

@article{Karypis1997P,
   author = {George Karypis and Kirk Schloegel and Vipin Kumar},
   title = {Parmetis: Parallel graph partitioning and sparse matrix ordering library},
   year = {1997},
}

@article{Bryan2014,
   abstract = {This paper describes the open-source code Enzo, which uses block-structured adaptive mesh refinement to provide high spatial and temporal resolution for modeling astrophysical fluid flows. The code is Cartesian, can be run in one, two, and three dimensions, and supports a wide variety of physics including hydrodynamics, ideal and non-ideal magnetohydrodynamics, N-body dynamics (and, more broadly, self-gravity of fluids and particles), primordial gas chemistry, optically thin radiative cooling of primordial and metal-enriched plasmas (as well as some optically-thick cooling models), radiation transport, cosmological expansion, and models for star formation and feedback in a cosmological context. In addition to explaining the algorithms implemented, we present solutions for a wide range of test problems, demonstrate the code's parallel performance, and discuss the Enzo collaboration's code development methodology.},
   author = {Greg L Bryan and Michael L Norman and Brian W OShea and Tom Abel and John H Wise and Matthew J Turk and Daniel R Reynolds and David C Collins and Peng Wang and Samuel W Skillman and Britton Smith and Robert P Harkness and James Bordner and Ji-hoon Kim and Michael Kuhlen and Hao Xu and Nathan Goldbaum and Cameron Hummels and Alexei G Kritsuk and Elizabeth Tasker and Stephen Skory and Christine M Simpson and Oliver Hahn and Jeffrey S Oishi and Geoffrey C So and Fen Zhao and Renyue Cen and Yuan Li},
   doi = {10.1088/0067-0049/211/2/19},
   issue = {2},
   journal = {The Astrophysical Journal Supplement Series},
   month = {3},
   pages = {19},
   publisher = {American Astronomical Society},
   title = {Enzo: An adaptive mesh refinement code for astrophysics},
   volume = {211},
   year = {2014},
}

@article{Schive2010,
   abstract = {We present the newly developed code, GPU-accelerated Adaptive-MEsh-Refinement code (GAMER), which adopts a novel approach in improving the performance of adaptive-mesh-refinement (AMR) astrophysical simulations by a large factor with the use of the graphic processing unit (GPU). The AMR implementation is based on a hierarchy of grid patches with an oct-tree data structure. We adopt a three-dimensional relaxing total variation diminishing scheme for the hydrodynamic solver and a multi-level relaxation scheme for the Poisson solver. Both solvers have been implemented in GPU, by which hundreds of patches can be advanced in parallel. The computational overhead associated with the data transfer between the CPU and GPU is carefully reduced by utilizing the capability of asynchronous memory copies in GPU, and the computing time of the ghost-zone values for each patch is diminished by overlapping it with the GPU computations. We demonstrate the accuracy of the code by performing several standard test problems in astrophysics. GAMER is a parallel code that can be run in a multi-GPU cluster system. We measure the performance of the code by performing purely baryonic cosmological simulations in different hardware implementations, in which detailed timing analyses provide comparison between the computations with and without GPU(s) acceleration. Maximum speed-up factors of 12.19 and 10.47 are demonstrated using one GPU with 40963 effective resolution and 16 GPUs with 81923 effective resolution, respectively.},
   author = {Hsi-Yu Schive and Yu-Chih Tsai and Tzihong Chiueh},
   doi = {10.1088/0067-0049/186/2/457},
   issue = {2},
   journal = {The Astrophysical Journal Supplement Series},
   month = {2},
   pages = {457-484},
   publisher = {American Astronomical Society},
   title = {GAMER: A graphic processing unit accelerated adaptive-mesh-refinement code for astrophysics},
   volume = {186},
   year = {2010},
}

@article{Wang2020,
   author = {Feng Wang and Nathan Marshak and Will Usher and Carsten Burstedde and Aaron Knoll and Timo Heister and Chris R Johson},
   doi = {10.1111/cgf.13958},
   journal = {Computer Graphics Forum},
   title = {CPU Ray Tracing of Tree-Based Adaptive Mesh Refinement Data},
   year = {2020},
}

@article{Burstedde2011,
   abstract = { We present scalable algorithms for parallel adaptive mesh refinement and coarsening (AMR), partitioning, and 2:1 balancing on computational domains composed of multiple connected two-dimensional quadtrees or three-dimensional octrees, referred to as a forest of octrees. By distributing the union of octants from all octrees in parallel, we combine the high scalability proven previously for adaptive single-octree algorithms with the geometric flexibility that can be achieved by arbitrarily connected hexahedral macromeshes, in which each macroelement is the root of an adapted octree. A key concept of our approach is an encoding scheme of the interoctree connectivity that permits arbitrary relative orientations between octrees. Based on this encoding we develop interoctree transformations of octants. These form the basis for high-level parallel octree algorithms, which are designed to interact with an application code such as a numerical solver for partial differential equations. We have implemented and tested these algorithms in the p4est software library. We demonstrate the parallel scalability of p4est on its own and in combination with two geophysics codes. Using p4est we generate and adapt multioctree meshes with up to $5.1310^11$ octants on as many as 220,320 CPU cores and execute the 2:1 balance algorithm in less than 10 seconds per million octants per process. },
   author = {Carsten Burstedde and Lucas C Wilcox and Omar Ghattas},
   doi = {10.1137/100791634},
   issue = {3},
   journal = {SIAM Journal on Scientific Computing},
   pages = {1103-1133},
   title = {p4est: Scalable Algorithms for Parallel Adaptive Mesh Refinement on Forests of Octrees},
   volume = {33},
   year = {2011},
}

@inproceedings{Chen2010,
   abstract = {The computational power provided by many-core graphics processing units (GPUs) has been exploited in many applications. The programming techniques currently employed on these GPUs are not sufficient to address problems exhibiting irregular, and unbalanced workload. The problem is exacerbated when trying to effectively exploit multiple GPUs concurrently, which are commonly available in many modern systems. In this paper, we propose a task-based dynamic load-balancing solution for single-and multi-GPU systems. The solution allows load balancing at a finer granularity than what is supported in current GPU programming APIs, such as NVIDIA's CUDA. We evaluate our approach using both micro-benchmarks and a molecular dynamics application that exhibits significant load imbalance. Experimental results with a single-GPU configuration show that our fine-grained task solution can utilize the hardware more efficiently than the CUDA scheduler for unbalanced workload. On multi-GPU systems, our solution achieves near-linear speedup, load balance, and significant performance improvement over techniques based on standard CUDA APIs.},
   author = {Long Chen and Oreste Villa and Sriram Krishnamoorthy and Guang R Gao},
   doi = {10.1109/IPDPS.2010.5470413},
   booktitle = {2010 IEEE International Symposium on Parallel Distributed Processing (IPDPS)},
   issn = {1530-2075},
   month = {4},
   pages = {1-12},
   title = {Dynamic load balancing on single- and multi-GPU systems},
   year = {2010},
}

@article{Anzt2020,
   abstract = {Efficient processing of Irregular Matrices on Single Instruction, Multiple Data (SIMD)-type architectures is a persistent challenge. Resolving it requires innovations in the development of data formats, computational techniques, and implementations that strike a balance between thread divergence, which is inherent for Irregular Matrices, and padding, which alleviates the performance-detrimental thread divergence but introduces artificial overheads. To this end, in this article, we address the challenge of designing high performance sparse matrix-vector product (SpMV) kernels designed for Nvidia Graphics Processing Units (GPUs). We present a compressed sparse row (CSR) format suitable for unbalanced matrices. We also provide a load-balancing kernel for the coordinate (COO) matrix format and extend it to a hybrid algorithm that stores part of the matrix in SIMD-friendly Ellpack format (ELL) format. The ratio between the ELL- and the COO-part is determined using a theoretical analysis of the nonzeros-per-row distribution. For the over 2,800 test matrices available in the Suite Sparse matrix collection, we compare the performance against SpMV kernels provided by NVIDIA’s cuSPARSE library and a heavily-tuned sliced ELL (SELL-P) kernel that prevents unnecessary padding by considering the irregular matrices as a combination of matrix blocks stored in ELL format.},
   author = {Hartwig Anzt and Terry Cojean and Chen Yen-Chen and Jack Dongarra and Goran Flegar and Pratik Nayak and Stanimire Tomov and Yuhsiang M Tsai and Weichung Wang},
   city = {New York, NY, USA},
   doi = {10.1145/3380930},
   issn = {2329-4949},
   issue = {1},
   journal = {ACM Trans. Parallel Comput.},
   keywords = {GPUs,Sparse Matrix Vector Product (SpMV),irregular matrices},
   month = {3},
   publisher = {Association for Computing Machinery},
   title = {Load-Balancing Sparse Matrix Vector Product Kernels on GPUs},
   volume = {7},
   year = {2020},
}

@inproceedings{Kijsipongse2012,
   abstract = {K-Means is the clustering algorithm which is widely used in many areas such as information retrieval, computer vision and pattern recognition. With the recent advance in General Purpose Graphics Processing Unit (GPGPU), we can use a modern GPU which is capable to do computation up to Tflops to calculate K-Means clustering on average problems. However, due to the exponential growth of data, the K-Means clustering on a single GPU will not be adequate for large datasets in the near future. In this paper, we present the design and implementation of an efficient large-scale parallel K-Means on GPU clusters. We utilize the massive parallelism in GPUs to speed up the most time consuming part of K-Means clustering in each node. We employ the dynamic load balancing to distribute workload equally on different GPUs installed in the clusters so as to improve the performance of the parallel K-Means at the inter-node level. We also take advantage from software distributed shared memory to simplify the communication and collaboration among nodes. The result of the evaluation shows the performance improvement of the parallel K-Means by maintaining load balance on GPU clusters.},
   author = {Ekasit Kijsipongse and Suriya U-ruekolan},
   doi = {10.1109/JCSSE.2012.6261977},
   booktitle = {2012 Ninth International Conference on Computer Science and Software Engineering (JCSSE)},
   month = {5},
   pages = {346-350},
   title = {Dynamic load balancing on GPU clusters for large-scale K-Means clustering},
   year = {2012},
}

@article{Cardellini1999,
   abstract = {Popular Web sites cannot rely on a single powerful server nor on independent mirrored-servers to support the ever-increasing request load. Distributed Web server architectures that transparently schedule client requests offer a way to meet dynamic scalability and availability requirements. The authors review the state of the art in load balancing techniques on distributed Web-server systems, and analyze the efficiencies and limitations of the various approaches.},
   author = {V Cardellini and M Colajanni and P S Yu},
   doi = {10.1109/4236.769420},
   issn = {1941-0131},
   issue = {3},
   journal = {IEEE Internet Computing},
   month = {5},
   pages = {28-39},
   title = {Dynamic load balancing on Web-server systems},
   volume = {3},
   year = {1999},
}

@article{Willebeek1993,
   abstract = {Dynamic load balancing strategies for minimizing the execution time of single applications running in parallel on multicomputer systems are discussed. Dynamic load balancing (DLB) is essential for the efficient use of highly parallel systems when solving non-uniform problems with unpredictable load estimates. With the evolution of more highly parallel systems, centralized DLB approaches which make use of a high degree of knowledge become less feasible due to the load balancing communication overhead. Five DLB strategies are presented which illustrate the tradeoff between 1) knowledge - the accuracy of each balancing decision, and 2) overhead - the amount of added processing and communication incurred by the balancing process. All five strategies have been implemented on an Inter iPSC/2 hypercube.&lt;<ETX>&gt;</ETX>},
   author = {M H Willebeek-LeMair and A P Reeves},
   doi = {10.1109/71.243526},
   issn = {1558-2183},
   issue = {9},
   journal = {IEEE Transactions on Parallel and Distributed Systems},
   month = {9},
   pages = {979-993},
   title = {Strategies for dynamic load balancing on highly parallel computers},
   volume = {4},
   year = {1993},
}

@article{Kobayashi1988,
   abstract = {Static and dynamic load balancing strategies for a multiprocessor system for a ray tracing algorithm based on constant subdivision are presented. An object space is divided into regular cubes (subspaces), whose boundary planes are perpendicular to the coordinate axes, and these are allocated to the processors in the system. Here, load balancing among the processors is the most important problem. Firstly, in a category of static load balancing, strategies for mapping the subspaces into the processors are evaluated by simulation. Moreover, we propose a hierarchical multiprocessor system in order to realize dynamic load balancing with the static one. Its architecture can overcome the limitation of the static load balancing in a large scale multiprocessor system.},
   author = {Hiroaki Kobayashi and Satoshi Nishimura and Hideyuki Kubota and Tadao Nakamura and Yoshiharu Shigei},
   doi = {10.1007/BF01887592},
   issn = {1432-2315},
   issue = {4},
   journal = {The Visual Computer},
   pages = {197-209},
   title = {Load balancing strategies for a parallel ray-tracing system based on constant subdivision},
   volume = {4},
   year = {1988},
}

@article{Vadsola2021,
   author = {M Vadsola and G G Agbaglah and C Mavriplis},
   doi = {10.1063/5.0036088},
   issue = {3},
   journal = {Physics of Fluids},
   pages = {33607},
   title = {Slat cove dynamics of low Reynolds number flow past a 30P30N high lift configuration},
   volume = {33},
   year = {2021},
}

@article{Merzari2017,
   abstract = {Numerical simulation has been an intrinsic part of nuclear engineering research since its inception. In recent years a transition is occurring toward predictive, first-principle-based tools such as computational fluid dynamics. Even with the advent of petascale computing, however, such tools still have significant limitations. In the present work some of these issues, and in particular the presence of massive multiscale separation, are discussed, as well as some of the research conducted to mitigate them. Petascale simulations at high fidelity (large eddy simulation/direct numerical simulation) were conducted with the massively parallel spectral element code Nek5000 on a series of representative problems. These simulations shed light on the requirements of several types of simulation: (1) axial flow around fuel rods, with particular attention to wall effects; (2) natural convection in the primary vessel; and (3) flow in a rod bundle in the presence of spacing devices. The focus of the work presented here is on the lessons learned and the requirements to perform these simulations at exascale. Additional physical insight gained from these simulations is also emphasized.},
   author = {Elia Merzari and Aleks Obabko and Paul Fischer and Noah Halford and Justin Walker and Andrew Siegel and Yiqi Yu},
   doi = {10.1016/j.nucengdes.2016.09.028},
   issn = {0029-5493},
   journal = {Nuclear Engineering and Design},
   note = {16th International Topical Meeting on Nuclear Reactor Thermal Hydraulics},
   pages = {86-98},
   title = {Large-scale large eddy simulation of nuclear reactor flows: Issues and perspectives},
   volume = {312},
   year = {2017},
}

@inproceedings{Wu2021,
   author = {Sicong Wu and Saumil S Patel and Muhsin M Ameen},
   booktitle = {Internal Combustion Engine Division Fall Technical Conference},
   pages = {V001T06A003},
   title = {Investigating the Origins of Cyclic Variability in Internal Combustion Engines Using Wall-Resolved Large Eddy Simulations},
   volume = {85512},
   year = {2021},
}

@article{Molcard2002,
   abstract = {This work is an attempt to simulate the Mediterranean Sea general circulation with a Spectral Finite Element Model. This numerical technique associates the geometrical flexibility of the finite elements for the proper coastline definition with the precision offered by spectral methods. The model is reduced gravity and we study the wind-driven ocean response in order to explain the large scale sub-basin gyres and their variability. The study period goes from January 1987 to December 1993 and two forcing data sets are used. The effect of wind variability in space and time is analyzed and the relationship between wind stress curl and ocean response is stressed. Some of the main permanent structures of the general circulation (Gulf of Lions cyclonic gyre, Rhodes gyre, Gulf of Syrte anticylone) are shown to be induced by permanent wind stress curl structures. The magnitude and spatial variability of the wind is important in determining the appearance or disappearance of some gyres (Tyrrhenian anticyclonic gyre, Balearic anticyclonic gyre, Ionian cyclonic gyre). An EOF analysis of the seasonal variability indicates that the weakening and strengthening of the Levantine basin boundary currents is a major component of the seasonal cycle in the basin. The important discovery is that seasonal and interannual variability peak at the same spatial scales in the ocean response and that the interannual variability includes the change in amplitude and phase of the seasonal cycle in the sub-basin scale gyres and boundary currents. The Coriolis term in the vorticity balance seems to be responsible for the weakening of anticyclonic structures and their total disappearance when they are close to a boundary. The process of adjustment to winds produces a train of coastally trapped gravity waves which travel around the eastern and western basins, respectively in approximately 6 months. This corresponds to a phase velocity for the wave of about 1m/s, comparable to an average velocity of an internal Kelvin wave in the area.},
   author = {A Molcard and N Pinardi and M Iskandarani and D B Haidvogel},
   doi = {10.1016/S0377-0265(01)00080-X},
   issn = {0377-0265},
   issue = {2},
   journal = {Dynamics of Atmospheres and Oceans},
   keywords = {Empirical orthogonal functions,Interannual and seasonal variability,Numerical modelling},
   pages = {97-130},
   title = {Wind driven general circulation of the Mediterranean Sea simulated with a Spectral Element Ocean Model},
   volume = {35},
   year = {2002},
}

@article{Haidvogel1997,
   abstract = { ABSTRACT The use of spectral methods now has a long history in global atmospheric modelling wherein the attractive properties of Fourier series on spheres, including higher-order convergence rates and efficient implementation via the transform method, have proven advantageous. Partially offsetting these advantages, however, are several competing disadvantages. Two of these, the appearance of Gibbs oscillations for localized processes (e.g., orographic interactions) and the difficulty of mapping spectral techniques onto parallel computer architectures, are inherent to the global nature of these techniques. A third drawback, the restriction of these methods to regular geometries, has severely limited their application to the modelling of the large-scale ocean circulation. We describe a global circulation model that has, in principle, none of these limitations. The model utilizes the spectral element method that combines the geometrical flexibility of traditional finite element methods with the rapid convergence rates of spectral approximation techniques. Simple test problems drawn from both oceanic and atmospheric modelling are used to demonstrate that the resulting model is exponentially convergent, yet allows effective representation of irregular geometry and efficient grid refinement in regions of dynamical interest. Lastly, performance characteristics on the nCUBE/2 and Cray T3D architectures confirm that the element model is ideally suited to the parallel computing environment. },
   author = {Dale B Haidvogel and Enrique Curchitser and Mohamed Iskandarani and Rowan Hughes and Mark Taylor},
   doi = {10.1080/07055900.1997.9687363},
   issue = {sup1},
   journal = {Atmosphere-Ocean},
   pages = {505-531},
   publisher = {Taylor & Francis},
   title = {Global Modelling of the Ocean and Atmosphere Using the Spectral Element Method},
   volume = {35},
   year = {1997},
}

@article{Chan2016,
   author = {Jesse Chan and Zheng Wang and Axel Modave and Jean-Francois Remacle and T Warburton},
   doi = {10.1016/j.jcp.2016.04.003},
   journal = {Journal of Computational Physics},
   month = {8},
   pages = {142-168},
   publisher = {Elsevier BV},
   title = {GPU-accelerated discontinuous Galerkin methods on hybrid meshes},
   volume = {318},
   year = {2016},
}

@article{Hesthaven2002,
   abstract = {We present a convergent high-order accurate scheme for the solution of linear conservation laws in geometrically complex domains. As our main example we include a detailed development and analysis of a scheme for the time-domain solution of Maxwell's equations in a three-dimensional domain. The fully unstructured spatial discretization is made possible by the use of a high-order nodal basis, employing multivariate Lagrange polynomials defined on the triangles and tetrahedra, while the equations themselves are satisfied in a discontinuous Galerkin form with the boundary conditions being enforced weakly through a penalty term. Accuracy, stability, and convergence of the semidiscrete approximation to Maxwell's equations is established rigorously and bounds on the growth of the global divergence error are provided. Concerns related to efficient implementations are discussed in detail. This sets the stage for the presentation of examples, verifying the theoretical results, and illustrating the versatility, flexibility, and robustness when solving two- and three-dimensional benchmark problems in computational electromagnetics. Pure scattering as well as penetration is discussed and high parallel performance of the scheme is demonstrated.},
   author = {J S Hesthaven and T Warburton},
   doi = {https://doi.org/10.1006/jcph.2002.7118},
   issn = {0021-9991},
   issue = {1},
   journal = {Journal of Computational Physics},
   pages = {186-221},
   title = {Nodal High-Order Methods on Unstructured Grids: I. Time-Domain Solution of Maxwell's Equations},
   volume = {181},
   year = {2002},
}

@article{Beck2014,
   abstract = {In this paper, we investigate the accuracy and efficiency of discontinuous Galerkin spectral method simulations of under-resolved transitional and turbulent flows at moderate Reynolds numbers, where the accurate prediction of closely coupled laminar regions, transition and developed turbulence presents a great challenge to large eddy simulation modelling. We take full advantage of the low numerical errors and associated superior scale resolving capabilities of high-order spectral methods by using high-order ansatz functions up to 12th order. We employ polynomial de-aliasing techniques to prevent instabilities arising from inexact quadrature of nonlinearities. Without the need for any additional filtering, explicit or implicit modelling, or artificial dissipation, our high-order schemes capture the turbulent flow at the considered Reynolds number range very well. Three classical large eddy simulation benchmark problems are considered: a circular cylinder flow at ReD=3900, a confined periodic hill flow at Reh=2800 and the transitional flow over a SD7003 airfoil at Rec=60,000. For all computations, the total number of degrees of freedom used for the discontinuous Galerkin spectral method simulations is chosen to be equal or considerably less than the reported data in literature. In all three cases, we achieve an equal or better match to direct numerical simulation results, compared with other schemes of lower order with explicitly or implicitly added subgrid scale models. Copyright © 2014 John Wiley & Sons, Ltd.},
   author = {Andrea D Beck and Thomas Bolemann and David Flad and Hannes Frank and Gregor J Gassner and Florian Hindenlang and Claus-Dieter Munz},
   doi = {https://doi.org/10.1002/fld.3943},
   issue = {8},
   journal = {International Journal for Numerical Methods in Fluids},
   keywords = {aliasing,discontinuous Galerkin,large eddy simulation,over-integration,polynomial dealiasing,stability,transitional and detached flows,under-resolved turbulence},
   pages = {522-548},
   title = {High-order discontinuous Galerkin spectral element methods for transitional and turbulent flow simulations},
   volume = {76},
   year = {2014},
}

@inproceedings{Garai2015,
    author = {Garai, Anirban and Diosady, Laslo and Murman, Scott and Madavan, Nateri},
    title = {DNS of Flow in a Low-Pressure Turbine Cascade Using a Discontinuous-Galerkin Spectral-Element Method},
    volume = {2B: Turbomachinery},
    booktitle = {Turbo Expo: Power for Land, Sea, and Air},
    year = {2015},
    month = {06},
    abstract = {A new computational capability under development for accurate and efficient high-fidelity direct numerical simulation (DNS) and large-eddy simulation (LES) of turbomachinery is described. This capability is based on an entropy-stable Discontinuous-Galerkin spectral-element approach that extends to arbitrarily high orders of spatial and temporal accuracy, and is implemented in a computationally efficient manner on a modern high performance computer architecture. A validation study using this method to perform DNS of flow in a low-pressure turbine airfoil cascade is presented. The results indicate that the method captures the main features of the flow.},
    doi = {10.1115/GT2015-42773},
}

@article{Gong2016,
   abstract = {We present a hybrid GPU implementation and performance analysis of Nekbone, which represents one of the core kernels of the incompressible Navier–Stokes solver Nek5000. The implementation is based on OpenACC and CUDA Fortran for local parallelization of the compute-intensive matrix–matrix multiplication part, which significantly minimizes the modification of the existing CPU code while extending the simulation capability of the code to GPU architectures. Our discussion includes the GPU results of OpenACC interoperating with CUDA Fortran and the gather–scatter operations with GPUDirect communication. We demonstrate performance of up to 552 Tflops on 16, 384 GPUs of the OLCF Cray XK7 Titan.},
   author = {Jing Gong and Stefano Markidis and Erwin Laure and Matthew Otten and Paul Fischer and Misun Min},
   doi = {10.1007/s11227-016-1744-5},
   issn = {1573-0484},
   issue = {11},
   journal = {The Journal of Supercomputing},
   pages = {4160-4180},
   title = {Nekbone performance on GPUs with OpenACC and CUDA Fortran implementations},
   volume = {72},
   year = {2016},
}

@article{Chalmers2022,
   author = {Noel Chalmers and Abhishek Mishra and Damon McDougall and Tim Warburton},
   doi = {10.48550/ARXIV.2202.12477},
   keywords = {Distributed,FOS: Computer and information sciences,Parallel,Performance (cs.PF),and Cluster Computing (cs.DC)},
   publisher = {arXiv},
   title = {HipBone: A performance-portable GPU-accelerated C++ version of the NekBone benchmark},
   url = {https://arxiv.org/abs/2202.12477},
   year = {2022},
}

@inproceedings{Rahman2015,
   author = {Md Rahman and Ramesh Agarwal and Markku Lampinen and Timo Siikonen},
   doi = {10.2514/6.2015-2786},
   month = {4},
   title = {An Improved Version of One-Equation RAS Turbulence Model},
   year = {2015},
   booktitle = {45th AIAA Fluid Dynamics Conference},
}
