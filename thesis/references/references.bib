@inproceedings{Mavriplis1990,
   abstract = {Aspects of adaptive spectral element methods are presented with emphasis on the a posteriori error estimators used in the automatic mesh refinement process. The nonconforming formulation of the method is reviewed in an effort to illustrate the various mesh refinement options available. Single mesh a posteriori error estimators are developed for the spectral element method. These estimate the actual error incurred by the discretization on a local per element basis and predict the convergence behaviour of the numerical solution as well. As a result the error estimators serve as criteria in the choice of refinement options. The usefulness of the estimators is demonstrated through examples of Navier-Stokes calculations.},
   author = {Catherine Mavriplis},
   city = {Wiesbaden},
   editor = {Pieter Wesseling},
   isbn = {978-3-663-13975-1},
   booktitle = {Proceedings of the Eighth GAMM-Conference on Numerical Methods in Fluid Mechanics},
   pages = {333-342},
   publisher = {Vieweg+Teubner Verlag},
   title = {A Posteriori Error Estimators for Adaptive Spectral Element Techniques},
   year = {1990},
}

@inproceedings{Maday1989,
   author = {Yvon Maday and Cathy Mavriplis and Anthony T Patera},
   booktitle = {Domain Decomposition Methods},
   keywords = {Decomposition,Discrete Functions,Domains,Finite Element Method,Fluid Mechanics and Heat Transfer,Grid Generation (Mathematics),Navier-Stokes Equation,Partial Differential Equations,Spectral Methods,Weighting Functions},
   month = {1},
   pages = {392-418},
   title = {Nonconforming mortar element methods - Application to spectral discretizations},
   year = {1989},
}

@book{Kopriva2009,
   abstract = {This book offers a systematic and self-contained approach to solvepartial differential
equations numerically using single and multidomain spectralmethods. It contains detailed
algorithms in pseudocode for the applicationof spectral approximations to both one
and two dimensional PDEsof mathematical physics describing potentials,transport, and
wave propagation. David Kopriva, a well-known researcherin the field with extensive
practical experience, shows how only a fewfundamental algorithms form the building
blocks of any spectral code, evenfor problems with complex geometries. The book addresses
computationaland applications scientists, as it emphasizes thepractical derivation
and implementation of spectral methods over abstract mathematics. It is divided into
two parts: First comes a primer on spectralapproximation and the basic algorithms,
including FFT algorithms, Gaussquadrature algorithms, and how to approximate derivatives.
The secondpart shows how to use those algorithms to solve steady and time dependent
PDEs in one and two space dimensions. Exercises and questions at theend of each chapter
encourage the reader to experiment with thealgorithms.},
   author = {David A Kopriva},
   edition = {1st},
   isbn = {9048122600},
   publisher = {Springer Publishing Company, Incorporated},
   title = {Implementing Spectral Methods for Partial Differential Equations: Algorithms for Scientists and Engineers},
   year = {2009},
}

@article{Harris2007,
   author = {Mark Harris and others},
   issue = {4},
   journal = {Nvidia developer technology},
   pages = {70},
   publisher = {NVIDIA Corp. California},
   title = {Optimizing parallel reduction in CUDA},
   volume = {2},
   year = {2007},
}

@online{Nvidia2021,
  author = {Nvidia},
  title = {CUDA C++ Programming Guide},
  month = {11},
  year = {2021},
  url = {https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html},
  urldate = {2021-11-29}
}

@article{Giuliani2019,
   author = {Andrew Giuliani and Lilia Krivodonova},
   doi = {10.1016/j.jcp.2018.12.019},
   journal = {Journal of Computational Physics},
   month = {11},
   title = {Adaptive mesh refinement on graphics processing units for applications in gas dynamics},
   volume = {381},
   year = {2019},
}

@online{Clucas1999,
   author = {Jean Clucas and Pam Walatka},
   month = {8},
   title = {FAST User Guide},
   url = {https://www.nas.nasa.gov/Software/FAST/RND-93-010.walatka-clucas/htmldocs/chp_16.surferu.html},
   year = {1999},
   urldate = {2021-12-13}
}

@article{Hilbert1891,
   author = {David Hilbert},
   issue = {38},
   journal = {Mathematische Annalen},
   pages = {459-460},
   title = {Über die stetige Abbildung einer Linie auf ein Flächenstück.},
   url = {http://www.digizeitschriften.de/dms/img/?PPN=PPN235181684_0038&DMDID=dmdlog40},
   year = {1891},
}

@article{Peano1890,
   author = {Giuseppe Peano},
   issue = {36},
   journal = {Mathematische Annalen},
   pages = {157-160},
   title = {Sur une courbe, qui remplit toute une aire plane.},
   url = {http://www.digizeitschriften.de/dms/img/?PPN=PPN235181684_0036&DMDID=dmdlog13},
   year = {1890},
}

@article{Haverkort2011,
   author = {Herman J Haverkort},
   journal = {CoRR},
   title = {An inventory of three-dimensional Hilbert space-filling curves},
   volume = {abs/1109.2323},
   url = {http://arxiv.org/abs/1109.2323},
   year = {2011},
}

@article{Karypis1998,
   author = {George Karypis and Vipin Kumar},
   journal = {SIAM Journal on Scientific Computing},
   pages = {359-392},
   title = {A fast and high quality multilevel scheme for partitioning irregular graphs},
   volume = {20},
   year = {1998},
}

@article{Pinar2004,
   abstract = {The one-dimensional decomposition of nonuniform workload arrays with optimal load balancing is investigated. The problem has been studied in the literature as the “chains-on-chains partitioning” problem. Despite the rich literature on exact algorithms, heuristics are still used in parallel computing community with the “hope” of good decompositions and the “myth” of exact algorithms being hard to implement and not runtime efficient. We show that exact algorithms yield significant improvements in load balance over heuristics with negligible overhead. Detailed pseudocodes of the proposed algorithms are provided for reproducibility. We start with a literature review and propose improvements and efficient implementation tips for these algorithms. We also introduce novel algorithms that are asymptotically and runtime efficient. Our experiments on sparse matrix and direct volume rendering datasets verify that balance can be significantly improved by using exact algorithms. The proposed exact algorithms are 100 times faster than a single sparse-matrix vector multiplication for 64-way decompositions on the average. We conclude that exact algorithms with proposed efficient implementations can effectively replace heuristics.},
   author = {Ali Pınar and Cevdet Aykanat},
   doi = {https://doi.org/10.1016/j.jpdc.2004.05.003},
   issn = {0743-7315},
   issue = {8},
   journal = {Journal of Parallel and Distributed Computing},
   keywords = {Chains-on-chains partitioning,Dynamic programming,Image-space parallel volume rendering,Iterative refinement,One-dimensional partitioning,Optimal load balancing,Parallel sparse matrix vector multiplication,Parametric search},
   pages = {974-996},
   title = {Fast optimal load balancing algorithms for 1D partitioning},
   volume = {64},
   url = {https://www.sciencedirect.com/science/article/pii/S0743731504000851},
   year = {2004},
}

@article{Patera1984,
   abstract = {A spectral element method that combines the generality of the finite element method with the accuracy of spectral techniques is proposed for the numerical solution of the incompressible Navier-Stokes equations. In the spectral element discretization, the computational domain is broken into a series of elements, and the velocity in each element is represented as a high-order Lagrangian interpolant through Chebyshev collocation points. The hyperbolic piece of the governing equations is then treated with an explicit collocation scheme, while the pressure and viscous contributions are treated implicitly with a projection operator derived from a variational principle. The implementation of the technique is demonstrated on a one-dimensional inflow-outflow advection-diffusion equation, and the method is then applied to laminar two-dimensional (separated) flow in a channel expansion. Comparisons are made with experiment and previous numerical work.},
   author = {Anthony T Patera},
   doi = {https://doi.org/10.1016/0021-9991(84)90128-1},
   issn = {0021-9991},
   issue = {3},
   journal = {Journal of Computational Physics},
   pages = {468-488},
   title = {A spectral element method for fluid dynamics: Laminar flow in a channel expansion},
   volume = {54},
   url = {https://www.sciencedirect.com/science/article/pii/0021999184901281},
   year = {1984},
}

@book{Gottlieb1977,
   author = {David Gottlieb and Steven A Orszag},
   publisher = {SIAM},
   title = {Numerical analysis of spectral methods: theory and applications},
   year = {1977},
}

@inbook{Toro2009,
   author = {Eleuterio Toro},
   doi = {10.1007/b79761},
   journal = {Riemann Solvers and Numerical Methods for Fluid Dynamics},
   month = {1},
   title = {Riemann Solvers and Numerical Methods for Fluid Dynamics: A Practical Introduction},
   year = {2009},
}

@article{Williamson1980,
   abstract = {All second-order, many third-order, and a few fourth-order Runge-Kutta schemes can be arranged to require only two storage locations per variable, compared with three needed by Gill's method.},
   author = {J H Williamson},
   doi = {https://doi.org/10.1016/0021-9991(80)90033-9},
   issn = {0021-9991},
   issue = {1},
   journal = {Journal of Computational Physics},
   pages = {48-56},
   title = {Low-storage Runge-Kutta schemes},
   volume = {35},
   url = {https://www.sciencedirect.com/science/article/pii/0021999180900339},
   year = {1980},
}

@article{Gottlieb2001,
   author = {Sigal Gottlieb and Chi-Wang Shu and Eitan Tadmor},
   doi = {10.1137/S003614450036757X},
   journal = {SIAM Review},
   month = {1},
   title = {Strong Stability-Preserving High-Order Time Discretization Methods},
   volume = {43},
   year = {2001},
}

@report{Slotnick2014,
   author = {Jeffrey Slotnick and Abdollah Khodadoust and Juan Alonso and William Gropp and Dimitri Mavriplis},
   title = {CFD Vision 2030 Study: A Path to Revolutionary Computational Aerosciences},
   url = {http://www.sti.nasa.gov},
   year = {2014},
}

@article{Stern2001,
   author = {Fred Stern and Robert V Wilson and Hugh W Coleman and Eric G Paterson},
   issue = {4},
   journal = {J. Fluids Eng.},
   pages = {793-802},
   title = {Comprehensive approach to verification and validation of CFD simulations—part 1: methodology and procedures},
   volume = {123},
   year = {2001},
}

@inbook{Karniadakis2005,
   author = {George Karniadakis and Spencer Sherwin},
   doi = {10.1093/acprof:oso/9780198528692.001.0001},
   month = {1},
   title = {Spectral/HP Element Methods for Computational Fluid Dynamics},
   year = {2005},
}

@inproceedings{Fan2004,
   abstract = {Inspired by the attractive Flops/dollar ratio and the incredible growth in the speed of modern graphics processing units (GPUs), we propose to use a cluster of GPUs for high performance scientific computing. As an example application, we have developed a parallel flow simulation using the lattice Boltzmann model (LBM) on a GPU cluster and have simulated the dispersion of airborne contaminants in the Times Square area of New York City. Using 30 GPU nodes, our simulation can compute a 480x400x80 LBM in 0.31 second/step, a speed which is 4.6 times faster than that of our CPU cluster implementation. Besides the LBM, we also discuss other potential applications of the GPU cluster, such as cellular automata, PDE solvers, and FEM.},
   author = {Zhe Fan and Feng Qiu and A Kaufman and S Yoakum-Stover},
   doi = {10.1109/SC.2004.26},
   journal = {SC '04: Proceedings of the 2004 ACM/IEEE Conference on Supercomputing},
   month = {11},
   pages = {47},
   title = {GPU Cluster for High Performance Computing},
   year = {2004},
}

@article{Owens2008,
   abstract = {The graphics processing unit (GPU) has become an integral part of today's mainstream computing systems. Over the past six years, there has been a marked increase in the performance and capabilities of GPUs. The modern GPU is not only a powerful graphics engine but also a highly parallel programmable processor featuring peak arithmetic and memory bandwidth that substantially outpaces its CPU counterpart. The GPU's rapid increase in both programmability and capability has spawned a research community that has successfully mapped a broad range of computationally demanding, complex problems to the GPU. This effort in general-purpose computing on the GPU, also known as GPU computing, has positioned the GPU as a compelling alternative to traditional microprocessors in high-performance computer systems of the future. We describe the background, hardware, and programming model for GPU computing, summarize the state of the art in tools and techniques, and present four GPU computing successes in game physics and computational biophysics that deliver order-of-magnitude performance gains over optimized CPU applications.},
   author = {John D Owens and Mike Houston and David Luebke and Simon Green and John E Stone and James C Phillips},
   doi = {10.1109/JPROC.2008.917757},
   issn = {1558-2256},
   issue = {5},
   journal = {Proceedings of the IEEE},
   month = {5},
   pages = {879-899},
   title = {GPU Computing},
   volume = {96},
   year = {2008},
}

@inproceedings{Lee2010,
   abstract = {Recent advances in computing have led to an explosion in the amount of data being generated. Processing the ever-growing data in a timely manner has made throughput computing an important aspect for emerging applications. Our analysis of a set of important throughput computing kernels shows that there is an ample amount of parallelism in these kernels which makes them suitable for today's multi-core CPUs and GPUs. In the past few years there have been many studies claiming GPUs deliver substantial speedups (between 10X and 1000X) over multi-core CPUs on these kernels. To understand where such large performance difference comes from, we perform a rigorous performance analysis and find that after applying optimizations appropriate for both CPUs and GPUs the performance gap between an Nvidia GTX280 processor and the Intel Core i7-960 processor narrows to only 2.5x on average. In this paper, we discuss optimization techniques for both CPU and GPU, analyze what architecture features contributed to performance differences between the two architectures, and recommend a set of architectural features which provide significant improvement in architectural efficiency for throughput kernels.},
   author = {Victor W Lee and Changkyu Kim and Jatin Chhugani and Michael Deisher and Daehyun Kim and Anthony D Nguyen and Nadathur Satish and Mikhail Smelyanskiy and Srinivas Chennupaty and Per Hammarlund and Ronak Singhal and Pradeep Dubey},
   city = {New York, NY, USA},
   doi = {10.1145/1815961.1816021},
   isbn = {9781450300537},
   journal = {Proceedings of the 37th Annual International Symposium on Computer Architecture},
   keywords = {cpu architecture,gpu architecture,performance analysis,performance measurement,software optimization,throughput computing},
   pages = {451-460},
   publisher = {Association for Computing Machinery},
   title = {Debunking the 100X GPU vs. CPU Myth: An Evaluation of Throughput Computing on CPU and GPU},
   url = {https://doi.org/10.1145/1815961.1816021},
   year = {2010},
}

@article{Nayfeh1997,
   abstract = {Presents the case for billion-transistor processor architectures that will consist of chip multiprocessors (CMPs): multiple (four to 16) simple, fast processors on one chip. In their proposal, each processor is tightly coupled to a small, fast, level-one cache, and all processors share a larger level-two cache. The processors may collaborate on a parallel job or run independent tasks (as in the SMT proposal). The CMP architecture lends itself to simpler design, faster validation, cleaner functional partitioning, and higher theoretical peak performance. However for this architecture to realize its performance potential, either programmers or compilers will have to make code explicitly parallel. Old ISAs will be incompatible with this architecture (although they could run slowly on one of the small processors).},
   author = {B A Nayfeh and K Olukotun},
   doi = {10.1109/2.612253},
   issn = {1558-0814},
   issue = {9},
   journal = {Computer},
   month = {9},
   pages = {79-85},
   title = {A single-chip multiprocessor},
   volume = {30},
   year = {1997},
}

@article{Barroso2005,
   abstract = {In the late 1990s, our research group at DEC was one of a growing number of teams advocating the CMP (chip multiprocessor) as an alternative to highly complex single-threaded CPUs. We were designing the Piranha system,1 which was a radical point in the CMP design space in that we used very simple cores (similar to the early RISC designs of the late ’80s) to provide a higher level of thread-level parallelism. Our main goal was to achieve the best commercial workload performance for a given silicon budget. Today, in developing Google’s computing infrastructure, our focus is broader than performance alone. The merits of a particular architecture are measured by answering the following question: Are you able to afford the computational capacity you need? The high-computational demands that are inherent in most of Google’s services have led us to develop a deep understanding of the overall cost of computing, and continually to look for hardware/software designs that optimize performance per unit of cost.},
   author = {Luiz André Barroso},
   city = {New York, NY, USA},
   doi = {10.1145/1095408.1095420},
   issn = {1542-7730},
   issue = {7},
   journal = {Queue},
   month = {9},
   pages = {48-53},
   publisher = {Association for Computing Machinery},
   title = {The Price of Performance: An Economic Case for Chip Multiprocessing},
   volume = {3},
   url = {https://doi.org/10.1145/1095408.1095420},
   year = {2005},
}

@inproceedings{Parkhurst2006,
   abstract = {In the past, processor design trends were dominated by increasingly complex feature sets, higher clock speeds, growing thermal envelopes and increasing power dissipation. Recently, clock speeds have tapered and thermal and power dissipation envelopes have remained flat. However, the demand for increasing performance continues which has fueled the move to integrated multiple processor (multi-core) designs. This paper discusses this trend towards multi-core processor designs, the design challenges that accompany it and a view of the research required to support it.},
   author = {Jeff Parkhurst and John Darringer and Bill Grundmann},
   city = {New York, NY, USA},
   doi = {10.1145/1233501.1233516},
   isbn = {1595933891},
   journal = {Proceedings of the 2006 IEEE/ACM International Conference on Computer-Aided Design},
   pages = {67-72},
   publisher = {Association for Computing Machinery},
   title = {From Single Core to Multi-Core: Preparing for a New Exponential},
   url = {https://doi.org/10.1145/1233501.1233516},
   year = {2006},
}

@article{Garland2008,
   abstract = {The CUDA programming model provides a straightforward means of describing inherently parallel computations, and NVIDIA's Tesla GPU architecture delivers high computational throughput on massively parallel problems. This article surveys experiences gained in applying CUDA to a diverse set of problems and the parallel speedups over sequential codes running on traditional CPU architectures attained by executing key computations on the GPU.},
   author = {Michael Garland and Scott Le Grand and John Nickolls and Joshua Anderson and Jim Hardwick and Scott Morton and Everett Phillips and Yao Zhang and Vasily Volkov},
   doi = {10.1109/MM.2008.57},
   issn = {1937-4143},
   issue = {4},
   journal = {IEEE Micro},
   month = {7},
   pages = {13-27},
   title = {Parallel Computing Experiences with CUDA},
   volume = {28},
   year = {2008},
}

@article{Deville2003,
   author = {Michel Deville and P Fischer and E H Mund},
   doi = {10.1115/1.1566402},
   journal = {Applied Mechanics Reviews},
   month = {1},
   title = {High-Order Methods for Incompressible Fluid Flow},
   volume = {56},
   year = {2003},
}

@thesis{Mavriplis1989,
   author = {Cathy Mavriplis},
   institution = {Massachusetts Institute of Technology},
   title = {Nonconforming discretizations and a posteriori error estimators for adaptive spectral element techniques},
   year = {1989},
}

@report{Reed1973,
   author = {William H Reed and Thomas R Hill},
   institution = {Los Alamos Scientific Lab., N. Mex.(USA)},
   title = {Triangular mesh methods for the neutron transport equation},
   year = {1973},
}

@inbook{Hesthaven2007,
   author = {Jan Hesthaven and Tim Warburton},
   month = {2},
   title = {Nodal Discontinuous Galerkin Methods: Algorithms, Analysis, and Applications},
   volume = {54},
   year = {2007},
}

@inproceedings{Amdahl1967,
   abstract = {For over a decade prophets have voiced the contention that the organization of a single computer has reached its limits and that truly significant advances can be made only by interconnection of a multiplicity of computers in such a manner as to permit cooperative solution. Variously the proper direction has been pointed out as general purpose computers with a generalized interconnection of memories, or as specialized computers with geometrically related memory interconnections and controlled by one or more instruction streams.},
   author = {Gene M Amdahl},
   city = {New York, NY, USA},
   doi = {10.1145/1465482.1465560},
   isbn = {9781450378956},
   journal = {Proceedings of the April 18-20, 1967, Spring Joint Computer Conference},
   pages = {483-485},
   publisher = {Association for Computing Machinery},
   title = {Validity of the Single Processor Approach to Achieving Large Scale Computing Capabilities},
   url = {https://doi.org/10.1145/1465482.1465560},
   year = {1967},
}

@article{Gustafson1988,
   author = {John L Gustafson},
   city = {New York, NY, USA},
   doi = {10.1145/42411.42415},
   issn = {0001-0782},
   issue = {5},
   journal = {Commun. ACM},
   month = {5},
   pages = {532-533},
   publisher = {Association for Computing Machinery},
   title = {Reevaluating Amdahl's Law},
   volume = {31},
   url = {https://doi.org/10.1145/42411.42415},
   year = {1988},
}

@article{Berger1984,
   abstract = {An adaptive method based on the idea of multiple component grids for the solution of hyperbolic partial differential equations using finite difference techniques is presented. Based upon Richardson-type estimates of the truncation error, refined grids are created or existing ones removed to attain a given accuracy for a minimum amount of work. The approach is recursive in that fine grids can contain even finer grids. The grids with finer mesh width in space also have a smaller mesh width in time, making this a mesh refinement algorithm in time and space. We present the algorithm, error estimation procedure, and the data structures, and conclude with numerical examples in one and two space dimensions.},
   author = {Marsha J Berger and Joseph Oliger},
   doi = {https://doi.org/10.1016/0021-9991(84)90073-1},
   issn = {0021-9991},
   issue = {3},
   journal = {Journal of Computational Physics},
   pages = {484-512},
   title = {Adaptive mesh refinement for hyperbolic partial differential equations},
   volume = {53},
   url = {https://www.sciencedirect.com/science/article/pii/0021999184900731},
   year = {1984},
}
