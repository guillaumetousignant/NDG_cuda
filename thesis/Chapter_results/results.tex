\chapter{Results}\label{chapter:results}
% Setup, supercomputers etc
% Profiler
% Memory usage maybe?
% Stream performance
% Mixed precision maybe?
% Hybrid solver?
% Different block sizes!!

This chapter presents results of the implementation of the adaptive \acrshort{acr:DG-SEM} wave
equation solver on \acrshort{acr:GPU} architectures and studies its effectiveness. Three main
components are studied. First, the performance of the core spectral element method code on
\acrshortpl{acr:GPU}, with a comparison with the same code running on \acrshortpl{acr:CPU} is
presented in Section~\ref{section:results:scaling_tests}. Then, the performance of the
\acrlong{acr:AMR} is studied in Section~\ref{section:results:adaptivity_performance}. Dynamic load
balancing is studied in Section~\ref{section:results:load_balancing_performance}. Finally more
complex applications are shown in Sections~\ref{section:results:complex_application}
and~\ref{section:results:complex_meshes}.

\section{Platforms}\label{section:results:platforms}

The implementation we have developed aims to perform large scale computations, and as such targets
\acrshort{acr:HPC} platforms. The program has been designed to scale to multiple levels of
parallelism, the highest being splitting the workload between several \acrshort{acr:HPC} nodes, each
having multiple \acrshortpl{acr:CPU} and \acrshortpl{acr:GPU}. To test this use case, most of the
following tests were run on clusters graciously offered by Compute
Canada\footnote{https://www.computecanada.ca/}. Two such clusters were used for our experiments,
Béluga and Narval.

\subsection{Béluga}\label{subsection:results:platforms:beluga}

Béluga is a heterogeneous cluster suitable for a variety of purposes, from traditional
\acrshort{acr:CPU} computing to massively parallel \acrshort{acr:GPU} computing. It is located at
the École de technologie supérieure in Montréal. Béluga is made up of 977 compute nodes of different
types, totaling 39,120 \acrshort{acr:CPU} cores and 696 \acrshortpl{acr:GPU}. For our testing, we
use Béluga's general usage \acrshort{acr:GPU} nodes. Each one of the 172 \acrshort{acr:GPU} nodes
contains 40 \acrshort{acr:CPU} cores and 186 GB of memory across two Intel Gold 6148
\acrshortpl{acr:CPU}, and four Nvidia V100 SXM2 \acrshortpl{acr:GPU} with 16 GB of memory each. One
such node totals about 2 TFlops of double precision \acrshort{acr:CPU} computing power, and 13
TFlops of double precision \acrshort{acr:GPU} computing power. The nodes are connected by Infiniband
HDR (100 Gb/s) interconnections.

\subsection{Narval}\label{subsection:results:platforms:narval}

Narval is a heterogeneous cluster suitable for a variety of purposes, from traditional
\acrshort{acr:CPU} computing to massively parallel \acrshort{acr:GPU} computing and artificial
intelligence workloads. It is located at the École de technologie supérieure in Montréal. Narval is
made up of 1301 compute nodes of different types, totaling 80,720 \acrshort{acr:CPU} cores and 636
\acrshortpl{acr:GPU}. For our testing, we use Narval's \acrshort{acr:GPU} nodes. Each one of the 159
\acrshort{acr:GPU} nodes contains 48 \acrshort{acr:CPU} cores and 498 GB of memory across two AMD
Milan 7413 \acrshortpl{acr:CPU}, and four Nvidia A100 \acrshortpl{acr:GPU} with 40 GB of memory
each. One such node totals about 2.2 TFlops of double precision \acrshort{acr:CPU} computing power,
and 38.8 TFlops of double precision \acrshort{acr:GPU} computing power. The nodes are connected by
Infiniband EDR (100 Gb/s) interconnections.

\subsection{Consumer hardware}\label{subsection:results:platforms:consumer}

Some smaller scale tests or tests that did not involve performance were run on consumer hardware.
This is important, as it may not always be possible to access \acrshort{acr:HPC} systems when flow
simulations have to be performed. The program should also have reasonable performance on regular
systems. The system used in these cases is a single computer containing 16 \acrshort{acr:CPU} cores
and 128 GB of memory across two Intel E5\textendash2650 V2 \acrshortpl{acr:CPU}, and one Nvidia GTX
1070 \acrshort{acr:GPU} with 8 GB of memory. This totals about 0.5 TFlops of double precision
\acrshort{acr:CPU} computing power, and 0.25 TFlops of double precision \acrshort{acr:GPU} computing
power. This is an interesting comparison, as consumer \acrshortpl{acr:GPU} are not geared towards
high precision compute loads. \Acrshortpl{acr:GPU} found in \acrshort{acr:HPC} systems typically
have a 1:2 ratio between their double precision and single precision performance, whereas consumer
\acrshortpl{acr:GPU} usually have 1:32 ratio. This means that computations using double precision
floating point numbers will execute at one sixteenth of the speed of compute-oriented
\acrshortpl{acr:GPU}, all other factors being equal.

\section{Test Case}\label{section:results:test_case}

The test case used in this chapter is shown in Figure~\ref{fig:problem}. We solve the 2D wave
equation from Section~\ref{section:spectral_element_method:equation} on a square domain of dimension
1 in each dimension. A linear wave starts just outside of the domain, and traverses the domain
diagonally. Figure~\ref{fig:problem} shows the pressure distribution at \(0.5 s\). The number of
elements \( K \) varies depending on the problem, as does the polynomial order \( N \). This problem
should help demonstrate key aspects of the program, as the solution is steeper near the wave and
needs refinement for optimal results.

\begin{figure}[H]
	\centering
	\includegraphics[width=0.6\textwidth]{Chapter_results/media/problem_1}
	\caption{Test case: A wave travels through a square domain at a 45° angle.}\label{fig:problem}
\end{figure}

\section{Scaling Tests}\label{section:results:scaling_tests}

Parallel scaling is an important aspect of the performance of a program. It describes how the
execution time varies when more resources are assigned to the problem. This is an optimisation
problem, as each problem will have an optimal amount of resources dedicated to it. 

A completely parallelisable program, often called embarrassingly parallel, should scale linearly
with the amount of resources. The speedup \(S\) of a workload split between \(W\) workers should be
equal to the number of workers in Equation~\ref{equ:scaling}. In this work, workers are a single
\acrshort{acr:GPU} or a single \acrshort{acr:CPU} core for comparative tests.

\begin{equation} \label{equ:scaling}
	S = \frac{t_1}{t_W}
\end{equation}

Unfortunately, it is often not possible to make a program entirely parallel. In such cases, dividing
a problem into more and more smaller tasks obeys the well-known Amdahl's law~\cite{Amdahl1967}, as
seen in Equation~\ref{equ:strong_scaling}. It splits the program into a sequential part \(s\) and a
parallel part \(p\), with only the parallel part scaling with the number of workers.

\begin{equation} \label{equ:strong_scaling}
	S = \frac{1}{s + \frac{p}{W}}
\end{equation}

This is called \textit{strong scaling}, where a fixed problem is solved using varying amount of
resources. This is discussed in Subsection~\ref{subsection:results:scaling_tests:strong}.

On the other hand, increased available resources can enable working on bigger problems. Gustafson
argues that modern highly parallel computers will be used to solve more complex problems instead of
solving smaller problems faster. Such a scaling is described with Equation~\ref{equ:weak_scaling}.

\begin{equation} \label{equ:weak_scaling}
	S = s + p \times W
\end{equation}

This is called \textit{weak scaling}. It describes solving a problem whose size increases with the
amount of resources, with the task size per worker stays constant. Weak scaling is tested in
Subsection~\ref{subsection:results:scaling_tests:weak}. With weak scaling a program will scale
indefinitely, with the slope of the scaling being influenced by the parallel proportion of the task.

This section will also serve as a general comparison between \acrshortpl{acr:CPU} and
\acrshortpl{acr:GPU}. All scaling tests have been performed on the Béluga supercomputer. Scaling is
evaluated by varying the number of \textit{nodes}, where a node in this context is a single computer
from Béluga, sporting 40 \acrshort{acr:CPU} cores and 4 \acrshortpl{acr:GPU}, as stated in
Subsection~\ref{subsection:results:platforms:beluga}. The four-node execution time of those tests
therefore represents the program running on 16 \acrshortpl{acr:GPU} or 160 \acrshort{acr:CPU} cores,
for the \acrshort{acr:GPU} and \acrshort{acr:CPU} computations respectively.

Both scaling tests evaluate the core solver part of the program, as the meshes start with enough
elements to have a well resolved solution all along the simulation. The \acrlong{acr:AMR} and
dynamic load balancing subroutines are still enabled. The subroutines still run their evaluation
phases, the error estimation and load imbalance computation respectively, but do not deem the
estimated error and load imbalance high enough to trigger the whole subroutine. 

\subsection{Strong scaling}\label{subsection:results:scaling_tests:strong}

The test case for this section is described in Section~\ref{section:results:test_case}. It is
divided into 256 elements in the \(x\) and \(y\) direction, for a total of \(K = 65536\). The
problem is then solved in parallel from one quarter node to eight nodes. One quarter node contains 1
\acrshort{acr:GPU} and 10 \acrshort{acr:CPU} cores, while eight nodes contain 16
\acrshortpl{acr:GPU} and 160 \acrshort{acr:CPU} cores. The mesh is split into blocks accordingly,
one block per \acrshort{acr:GPU} or one block per \acrshort{acr:CPU} core. The first test has a
polynomial order \(N\) of four for all elements, and a block size \(B\) of 32. 

The block size, as explained in
Subsection~\ref{subsection:graphics_processing_units:architecture:programming_model}, is the number
of threads in a block of threads. Having a higher block size can speed up execution by reducing the
number of instructions to dispatch by the control flow unit of the \acrshort{acr:GPU}, because the
instructions are dispatched by block of threads. However, if the threads within a block diverge,
having a higher block size can actually lower performance. A single thread diverging will have all
the threads in its block waiting until it is finished, therefore a higher block size will have more
threads stalled when divergence occurs.

All tests have an ``ideal'' line both for the \acrshort{acr:GPU} and \acrshort{acr:CPU}
calculations. This line represents the optimal split for that case, with the number of nodes with
the best scaling, extrapolated linearly to the other splits.

\begin{figure}[H]
	\centering
	\includesvg[width=0.6\textwidth]{Chapter_results/media/strong_scaling_N4_K65536_W32}
	\caption{Weak scaling: The problem size increases with the number of workers. N = 4, K = 65536, B = 32}\label{fig:strong_scaling_N4_W32}
\end{figure}

Figure~\ref{fig:strong_scaling_N4_W32} shows that the \acrshort{acr:GPU} implementation, initially
slower than the \acrshort{acr:CPU} implementation, overtakes it when one full node or more is used.
The higher number of nodes show the \acrshort{acr:GPU} implementation to be \(2.9 \times \) faster
that the \acrshort{acr:CPU} implementation.

At low numbers of \acrshortpl{acr:GPU} working on the same problem, the \acrshortpl{acr:GPU} are
more heavily loaded. It is possible that the higher memory usage, or the increased cache eviction
rate lowers the performance of \acrshortpl{acr:GPU} in that regime. With a lower number of elements
being worked on by a \acrshort{acr:GPU}, cache has to be emptied less often to make room for more
elements to use for computations.

At the end of the curve, we see that the scaling lowers slightly. This could be a hint that the
\acrshortpl{acr:GPU} are then not loaded enough. These \acrshortpl{acr:GPU} have many
\acrshort{acr:CUDA} cores, 5120 each in the case of Béluga. If the workload does not saturate
the \acrshort{acr:GPU}, some of those cores will be idle, leading to worse performance. The optimal
number of elements per \acrshort{acr:GPU} seems to be around 4096.

The next test, shown on Figure~\ref{fig:strong_scaling_N6_W32}, shows the same test case but with an
increased polynomial order of \(N = 6\) for all elements. The rationale is that the
\acrshort{acr:GPU} should spend more time on the computations, since the computations will be
heavier, while spending the same time on the overhead of kernel calls and other housekeeping.

\begin{figure}[H]
	\centering
	\includesvg[width=0.6\textwidth]{Chapter_results/media/strong_scaling_N6_K65536_W32}
	\caption{Weak scaling: The problem size increases with the number of workers. N = 6, K = 65536, B = 32}\label{fig:strong_scaling_N6_W32}
\end{figure}

Figure~\ref{fig:strong_scaling_N6_W32} shows that the increased polynomial order has not increased
the performance of the \acrshort{acr:GPU} implementation compared to the \acrshort{acr:CPU}
implementation, rather shifting to the right the crossover point where the \acrshort{acr:GPU}
implementation is faster. It is possible that the reduced relative performance is linked to the
increased memory used by the solution, as the needed storage is relative to \(2 N + 1\), where \(N\)
is the polynomial order of elements.

Next, this last case is computer again with increased block size \(B\), first 64 on
Figure~\ref{fig:strong_scaling_N6_W64}, then 128 on Figure~\ref{fig:strong_scaling_N6_W128}. The
\acrshort{acr:CPU} implementation is not modified. This should assess the incidence of block size on
the solution time.

\begin{figure}[H]
	\centering
	\includesvg[width=0.6\textwidth]{Chapter_results/media/strong_scaling_N6_K65536_W64}
	\caption{Weak scaling: The problem size increases with the number of workers. N = 6, K = 65536, B = 64}\label{fig:strong_scaling_N6_W64}
\end{figure}

\begin{figure}[H]
	\centering
	\includesvg[width=0.6\textwidth]{Chapter_results/media/strong_scaling_N6_K65536_W128}
	\caption{Weak scaling: The problem size increases with the number of workers. N = 6, K = 65536, B = 128}\label{fig:strong_scaling_N6_W128}
\end{figure}

A the last two figures suggest, increasing the block size \(B\) does incur a performance penalty.
This is possible caused by a high amount of thread divergence within blocks. This is consistent with
how the program is written, for example
Section~\ref{section:adaptive_mesh_refinement:implementation} shows how the projection back and
forth between the elements and faces have a low of branching code paths. A lot of changes in the
architecture of the program would have to be made to reduce this branching, and switch to a more
data-driven approach.

One thing to note is that the curve of the \acrshort{acr:CPU} implementation on
Figures~\ref{fig:strong_scaling_N4_W32} and~\ref{fig:strong_scaling_N6_W32} is similar, whereas the
\acrshort{acr:GPU} curves from all the figures differ significantly. This is a recurring theme of
all the tests in this chapter, code running on \acrshortpl{acr:GPU} seems to be harder to optimise
and some factors have unexpected influences on the performance.

\subsection{Weak scaling}\label{subsection:results:scaling_tests:weak}

The weak scaling test uses the text case from Section~\ref{section:results:test_case}. The domain is
split up into elements such that each node contains 16384 elements. This amounts to 4096 elements
per \acrshort{acr:GPU}, or 410 elements per \acrshort{acr:CPU} core. The simulation is first
computed with the highest number of nodes, therefore the smallest elements and time step size. The
\acrshort{acr:CFL} number is adjusted for all other simulations to get the same time step size,
therefore the same total number of iterations. An ideal result is a flat curve, indicating that by
increasing the problem size and resources by the same amount, the simulation time stays the same.
The problem is studied from ¼ node to 16 nodes.

\begin{figure}[H]
	\centering
	\includesvg[width=0.6\textwidth]{Chapter_results/media/weak_scaling_N4_K4096_W32}
	\caption{Weak scaling: The problem size increases with the number of workers. N = 4, K = 16384/node}\label{fig:weak_scaling}
\end{figure}

Figure~\ref{fig:weak_scaling} shows good weak scaling from the \acrshort{acr:GPU} implementation.
Keeping in mind that the quarter node and single node cases have a reduced workload by not having to
do any \acrshort{acr:MPI} communication over the network, and the quarter node \acrshort{acr:GPU}
case has no \acrshort{acr:MPI} communication to do at all, the \acrshort{acr:GPU} curve is
reasonably flat. The \acrshort{acr:CPU} curve is not as good, indicating that there may be a
bottleneck in the communication part of the program. The \acrshort{acr:CPU} implementation has a
much higher number of workers than the \acrshort{acr:GPU} one, 40 per node instead of four,
increasing the inter-process communication needed. The slope of the \acrshort{acr:CPU} curve is
about \(0.2\) with regards to the number of \acrshort{acr:CPU} cores. 

\section{Adaptive Mesh Refinement Performance}\label{section:results:adaptivity_performance}

This section establishes the performance of the \acrlong{acr:AMR}. The problem from
Section~\ref{section:results:test_case} is used, initially split into four elements in the \(x\) and
\(y\) direction, for \(K = 16\). The initial polynomial order \(N\) of four. The system is allowed
to refine up to a split level of five, meaning a cell can split five times and become \(
\frac{1}{32}\) the size of the original cell, and up to a polynomial order \(N\) of 16. The error
estimation threshold is set to \num{1e-6}, meaning it will refine elements whose estimated error is
greater than that number.

\Acrlong{acr:AMR} is a costly process, as the \acrshort{acr:CPU} must reallocate arrays on the
\acrshort{acr:GPU} for the different objects making up the mesh, and then schedule kernels to move
objects to the new arrays. We test here two strategies to reduce the performance overhead of
\acrlong{acr:AMR}, while reducing error. 

First, \acrlong{acr:AMR} will be performed at an interval.
Figures~\ref{fig:adaptivity_efficiency_C5},~\ref{fig:adaptivity_efficiency_C20},~\ref{fig:adaptivity_efficiency_C100}
and~\ref{fig:adaptivity_efficiency_C500} show the results of refining the mesh every 5, 20, 100 and
500 timesteps, respectively.

Secondly, the simulations are performed with an increasing number of refinement pre-conditioning
steps as described in Section~\ref{section:adaptive_mesh_refinement:pre_conditioning}. The analysis
ranges from no pre-conditioning step to five steps. The total simulation time is shown in blue. It
is the sum of the time taken up by the pre-condition of the mesh, shown in green, and the
computation time to solve the problem, shown in yellow. 

We compare those simulation to two non-adaptive cases. In the first case, the initial mesh is fully
refined uniformly up to the maximum level attained by the adaptive cases. The mesh is split into 128
elements in the \(x\) and \(y\) dimensions, for  \(K = 16384\), with a polynomial order  \(N\) of
10. It is shown as a dashed line on the figures. The second non-adaptive case aims to obtain a
maximum error similar to the adaptive case. The mesh refined uniformly to 32 elements in the \(x\)
and \(y\) dimensions, for  \(K = 1024\), with a polynomial order  \(N\) of 10. It is shown as a
dotted line on the figures. Both the simulation time and the error against the analytical solution
are compared.

% say on what it was run

\begin{figure}[H]
	\centering
	\subfloat[Solution time]
	{\includesvg[width=0.45\textwidth]{Chapter_results/media/adaptivity_time_N4_K16_C5}\label{fig:adaptivity_efficiency_C5_time}}
	\subfloat[Maximum error]
	{\includesvg[width=0.45\textwidth]{Chapter_results/media/adaptivity_error_N4_K16_C5}\label{fig:adaptivity_efficiency_C5_error}}
	\caption{Adaptivity efficiency: Simulation time and analytical error with adaptivity and increasing pre-processing steps. N = 4, K = 16, adaptivity interval = 5}\label{fig:adaptivity_efficiency_C5}
\end{figure}

\begin{figure}[H]
	\centering
	\subfloat[Solution time]
	{\includesvg[width=0.45\textwidth]{Chapter_results/media/adaptivity_time_N4_K16_C20}\label{fig:adaptivity_efficiency_C20_time}}
	\hfill
	\subfloat[Maximum error]
	{\includesvg[width=0.45\textwidth]{Chapter_results/media/adaptivity_error_N4_K16_C20}\label{fig:adaptivity_efficiency_C20_error}}
	\caption{Adaptivity efficiency: Simulation time and analytical error with adaptivity and increasing pre-processing steps. N = 4, K = 16, adaptivity interval = 20}\label{fig:adaptivity_efficiency_C20}
\end{figure}

\begin{figure}[H]
	\centering
	\subfloat[Solution time]
	{\includesvg[width=0.45\textwidth]{Chapter_results/media/adaptivity_time_N4_K16_C100}\label{fig:adaptivity_efficiency_C100_time}}
	\hfill
	\subfloat[Maximum error]
	{\includesvg[width=0.45\textwidth]{Chapter_results/media/adaptivity_error_N4_K16_C100}\label{fig:adaptivity_efficiency_C100_error}}
	\caption{Adaptivity efficiency: Simulation time and analytical error with adaptivity and increasing pre-processing steps. N = 4, K = 16, adaptivity interval = 100}\label{fig:adaptivity_efficiency_C100}
\end{figure}

\begin{figure}[H]
	\centering
	\subfloat[Solution time]
	{\includesvg[width=0.45\textwidth]{Chapter_results/media/adaptivity_time_N4_K16_C500}\label{fig:adaptivity_efficiency_C500_time}}
	\hfill
	\subfloat[Maximum error]
	{\includesvg[width=0.45\textwidth]{Chapter_results/media/adaptivity_error_N4_K16_C500}\label{fig:adaptivity_efficiency_C500_error}}
	\caption{Adaptivity efficiency: Simulation time and analytical error with adaptivity and increasing pre-processing steps. N = 4, K = 16, adaptivity interval = 500}\label{fig:adaptivity_efficiency_C500}
\end{figure}

From
Figures~\ref{fig:adaptivity_efficiency_C5},~\ref{fig:adaptivity_efficiency_C20},~\ref{fig:adaptivity_efficiency_C100}
and~\ref{fig:adaptivity_efficiency_C500}, we can see that this is an optimisation problem, with
tradeoffs between the simulation time and the error generated.

An important part of the error comes from the initial conditions being applied on too coarse a mesh.
Even if the mesh is refined, this error propagates through the domain. Pre-conditioning, as
described in Section~\ref{section:adaptive_mesh_refinement:pre_conditioning}, aims to alleviate this
problem. Indeed, with enough pre-condition steps all cases converge to an acceptable error. Adding
more pre-condition steps does not improve the results, as the error estimation threshold of
\num{1e-6} is likely reached.

It can be observed that the fully refined non-adaptive case, shown in dashed lines on
Figures~\ref{fig:adaptivity_efficiency_C5},~\ref{fig:adaptivity_efficiency_C20},~\ref{fig:adaptivity_efficiency_C100}
and~\ref{fig:adaptivity_efficiency_C500}, is prohibitively time consuming to compute. The solution
is well resolved as shown on Figure~\ref{fig:adaptivity_efficiency_C20_error}, with the analytical
solution error being \(19 \times \) smaller than the adaptive case. On the other hand, it takes
\(271 \times \) as much time to compute. The difference is even more pronounced when the adaptivity
interval is increased to 100, as on Figure~\ref{fig:adaptivity_efficiency_C100_time}, where the
adaptive solution with three to five pre-condition steps is around \(750 \times \) faster, at the
expense of a worse error compared to the 50 adaptivity interval case.

As for the similar maximum error non-adaptive case, shown with dotted lines, the results are closer.
The case with an adaptivity interval of 20 on Figure~\ref{fig:adaptivity_efficiency_C20} was used to
set the target error for that non-adaptive case. In this comparison, it can be observed that the
adaptive case starting from a coarse mesh is able to reach a maximum error only 14\% greater than
the non-adaptive case, while being 34\% faster. The other cases also attain the error estimation
target of \num{1e-6} while being faster.

It is to be noted that the non-adaptive cases are closer to being optimal for the \acrshort{acr:GPU}
architecture. The
% Something about non-adaptive cases being a best-case (lol) for GPUs. No branching and divergence,
% cite non-conforming interfaces, N, same number of neighbours

% Compare the different intervals and steps, select best and add wrap up plot

\section{Dynamic Load Balancing Performance}\label{section:results:load_balancing_performance}

This section is dedicated to benchmarking the dynamic load balancing module of the program in
adaptive cases where varying levels of load imbalance are generated. The sample problem is the same
as described in Section~\ref{section:results:test_case}, except that the domain size is increased
from a size of 1 to 100 in either dimension. The wave stays the same, effectively only staying in
the lower left corner during the program runtime. The program advances the solution to a simulation
time of two seconds. The mesh is split into 128 elements in the \(x\) and \(y\) directions, for a
total of \(K = 16384\) elements. The initial polynomial order is \(N = 4\). The problem is solved
using four \acrshort{acr:GPU} nodes from the Narval supercomputer, for a total of 16
\acrshortpl{acr:GPU}. This initially amounts to 1024 elements per \acrshort{acr:GPU}. The mesh is
refined every 20 timesteps. In order to prescribe a different load imbalance for different cases,
they have a different maximum split level \(S\). This parameter controls how many times an element
can h-refine. Three cases are presented, a low load imbalance case with \(S = 3\) shown in
Figures~\ref{fig:load_imbalance_case_low_p} and~\ref{fig:load_imbalance_case_low_s}, a medium load
imbalance case with \(S = 5\) shown in Figures~\ref{fig:load_imbalance_case_medium_p}
and~\ref{fig:load_imbalance_case_medium_s}, and a high load imbalance case with \(S = 7\) shown in
Figures~\ref{fig:load_imbalance_case_high_p} and~\ref{fig:load_imbalance_case_high_s}. The different
cases are allowed to refine up to their respective maximum split level \(S\), and up to a polynomial
order of \(N = 12\). 

\begin{figure}[H]
	\centering
	\subfloat[Full domain]
	{\includegraphics[width=0.45\textwidth]{Chapter_results/media/problem_low_far}\label{fig:load_imbalance_case_low_p_far}}
	\hfill
	\subfloat[Detail]
	{\includegraphics[width=0.45\textwidth]{Chapter_results/media/problem_low_near}\label{fig:load_imbalance_case_low_p_near}}
	\caption{Low load imbalance (\(S = 3\)) test case pressure: A wave passes through a very big domain. \(K_{initial} = 16384\), \(K_{final} = 17752\) (a) Complete domain (b) Area of interest}\label{fig:load_imbalance_case_low_p}
\end{figure}

\begin{figure}[H]
	\centering
	\subfloat[Full domain]
	{\includegraphics[width=0.45\textwidth]{Chapter_results/media/split_level_low_far}\label{fig:load_imbalance_case_low_s_far}}
	\hfill
	\subfloat[Detail]
	{\includegraphics[width=0.45\textwidth]{Chapter_results/media/split_level_low_near}\label{fig:load_imbalance_case_low_s_near}}
	\caption{Low load imbalance (\(S = 3\)) test case split level: Split level, indicating how many times the elements have split, only the bottom right refines. \(K_{initial} = 16384\), \(K_{final} = 17752\) (a) Complete domain (b) Area of interest}\label{fig:load_imbalance_case_low_s}
\end{figure}

\begin{figure}[H]
	\centering
	\subfloat[Full domain]
	{\includegraphics[width=0.45\textwidth]{Chapter_results/media/problem_medium_far}\label{fig:load_imbalance_case_medium_p_far}}
	\hfill
	\subfloat[Detail]
	{\includegraphics[width=0.45\textwidth]{Chapter_results/media/problem_medium_near}\label{fig:load_imbalance_case_medium_p_near}}
	\caption{Medium load imbalance (\(S = 5\)) test case pressure: A wave passes through a very big domain. \(K_{initial} = 16384\), \(K_{final} = 26734\) (a) Complete domain (b) Area of interest}\label{fig:load_imbalance_case_medium_p}
\end{figure}

\begin{figure}[H]
	\centering
	\subfloat[Full domain]
	{\includegraphics[width=0.45\textwidth]{Chapter_results/media/split_level_medium_far}\label{fig:load_imbalance_case_s_far}}
	\hfill
	\subfloat[Detail]
	{\includegraphics[width=0.45\textwidth]{Chapter_results/media/split_level_medium_near}\label{fig:load_imbalance_case_s_near}}
	\caption{Medium load imbalance (\(S = 5\)) test case split level: Split level, indicating how many times the elements have split, only the bottom right refines. \(K_{initial} = 16384\), \(K_{final} = 26734\) (a) Complete domain (b) Area of interest}\label{fig:load_imbalance_case_medium_s}
\end{figure}

\begin{figure}[H]
	\centering
	\subfloat[Full domain]
	{\includegraphics[width=0.45\textwidth]{Chapter_results/media/problem_high_far}\label{fig:load_imbalance_case_high_p_far}}
	\hfill
	\subfloat[Detail]
	{\includegraphics[width=0.45\textwidth]{Chapter_results/media/problem_high_near}\label{fig:load_imbalance_case_high_p_near}}
	\caption{High load imbalance (\(S = 7\)) test case pressure: A wave passes through a very big domain. \(K_{initial} = 16384\), \(K_{final} = 54837\) (a) Complete domain (b) Area of interest}\label{fig:load_imbalance_case_high_p}
\end{figure}

\begin{figure}[H]
	\centering
	\subfloat[Full domain]
	{\includegraphics[width=0.45\textwidth]{Chapter_results/media/split_level_high_far}\label{fig:load_imbalance_case_high_s_far}}
	\hfill
	\subfloat[Detail]
	{\includegraphics[width=0.45\textwidth]{Chapter_results/media/split_level_high_near}\label{fig:load_imbalance_case_high_s_near}}
	\caption{High load imbalance (\(S = 7\)) test case split level: Split level, indicating how many times the elements have split, only the bottom right refines. \(K_{initial} = 16384\), \(K_{final} = 54837\) (a) Complete domain (b) Area of interest}\label{fig:load_imbalance_case_high_s}
\end{figure}

% ADD add plot of the rank for S = 7

% Describe table
Where \(K_p\) is the number of elements in process \(p\).

\begin{table}[H]
	\centering
	\begin{tabular}{ c c c c c c c }
		Case & S & K & Max \(K_p\) & \(w_{ideal}\) & Load imbalance \\
		\midrule
		Low & \(3\) & \(17752\) & \(2392\) & \(1109.5\) & \(2.16\) \\
		Medium & \(5\) & \(26734\) & \(11374\) & \(1670.875\) & \(6.81\) \\
		High & \(7\) & \(70197\) & \(54837\) & \(4387.3125\) & \(12.50\) \\
	\end{tabular}
	\caption{Load imbalance cases.}\label{table:load_imbalance}
\end{table}

% Describe two strategies

\subsection{Load balancing interval}\label{subsection:results:load_balancing_performance:interval}

\begin{figure}[H]
	\centering
	\includesvg[width=0.6\textwidth]{Chapter_results/media/load_balancing_interval_N4_K16384_A20_P16_S3}
	\caption{Load balancing efficiency interval test: Simulation time with adaptivity, load balancing and increasing load balancing interval. N = 4, K = 16384, adaptivity interval = 20, W = 16, max split level = 3}\label{fig:load_balancing_efficiency_interval}
\end{figure}

\begin{figure}[H]
	\centering
	\includesvg[width=0.6\textwidth]{Chapter_results/media/load_balancing_interval_N4_K16384_A20_P16_S5}
	\caption{Load balancing efficiency interval test: Simulation time with adaptivity, load balancing and increasing load balancing interval. N = 4, K = 16384, adaptivity interval = 20, W = 16, max split level = 5}\label{fig:load_balancing_efficiency_interval_s5}
\end{figure}

\begin{figure}[H]
	\centering
	\includesvg[width=0.6\textwidth]{Chapter_results/media/load_balancing_interval_N4_K16384_A20_P16_S7}
	\caption{Load balancing efficiency interval test: Simulation time with adaptivity, load balancing and increasing load balancing interval. N = 4, K = 16384, adaptivity interval = 20, W = 16, max split level = 7}\label{fig:load_balancing_efficiency_interval_s7}
\end{figure}

% Say that the interval depends on the problem size

\subsection{Load balancing threshold}\label{subsection:results:load_balancing_performance:threshold}

\begin{figure}[H]
	\centering
	\includesvg[width=0.6\textwidth]{Chapter_results/media/load_balancing_threshold_N4_K16384_A20_L20_P16_S3}
	\caption{Load balancing efficiency threshold test: Simulation time with adaptivity, load balancing and increasing load balancing threshold. N = 4, K = 16384, adaptivity interval = 20, load balancing interval = 20, W = 16, max split level = 3}\label{fig:load_balancing_efficiency_threshold_s3}
\end{figure}

\begin{figure}[H]
	\centering
	\includesvg[width=0.6\textwidth]{Chapter_results/media/load_balancing_threshold_N4_K16384_A20_L20_P16_S5}
	\caption{Load balancing efficiency threshold test: Simulation time with adaptivity, load balancing and increasing load balancing threshold. N = 4, K = 16384, adaptivity interval = 20, load balancing interval = 20, W = 16, max split level = 5}\label{fig:load_balancing_efficiency_threshold_s5}
\end{figure}

\subsection{Polynomial order influence}\label{subsection:results:load_balancing_performance:polynomial_order}

\begin{figure}[H]
	\centering
	\includesvg[width=0.6\textwidth]{Chapter_results/media/N_iteration_time}
	\caption{Polynomial order influence: Iteration time with increasing polynomial order.}\label{fig:N_influence}
\end{figure}

\section{Complex Case}\label{section:results:complex_application}

\begin{figure}[H]
	\centering
	\subfloat[\( t = 0 s\)]
	{\includegraphics[width=0.45\textwidth]{Chapter_results/media/cloud_t0}\label{fig:cloud_p_t0}}
	\hfill
	\subfloat[\( t = 0.2 s\)]
	{\includegraphics[width=0.45\textwidth]{Chapter_results/media/cloud_t0_2}\label{fig:cloud_p_t0_2}}
	\caption{Complex case: A wave goes through a cloud of high pressure. (a) Initial conditions (b) After the wave and cloud collide}\label{fig:cloud_p}
\end{figure}

\begin{figure}[H]
	\centering
	\subfloat[\( t = 0 s\)]
	{\includegraphics[width=0.45\textwidth]{Chapter_results/media/cloud_split_level_t0}\label{fig:cloud_s_t0}}
	\hfill
	\subfloat[\( t = 0.2 s\)]
	{\includegraphics[width=0.45\textwidth]{Chapter_results/media/cloud_split_level_t0_2}\label{fig:cloud_s_t0_2}}
	\caption{Complex case split level: The elements split more where the cloud meets the wave. (a) Start of computation (b) After the wave and cloud collide}\label{fig:cloud_s}
\end{figure}

\begin{figure}[H]
	\centering
	\subfloat[\( t = 0 s\)]
	{\includegraphics[width=0.45\textwidth]{Chapter_results/media/cloud_N_t0}\label{fig:cloud_N_t0}}
	\hfill
	\subfloat[\( t = 0.2 s\)]
	{\includegraphics[width=0.45\textwidth]{Chapter_results/media/cloud_N_t0_2}\label{fig:cloud_N_t0_2}}
	\caption{Complex case N:\@The polynomial order increases more at the center of the cloud and where cloud meets the wave. (a) Start of computation (b) After the wave and cloud collide}\label{fig:cloud_N}
\end{figure}

\begin{figure}[H]
	\centering
	\subfloat[\( t = 0 s\)]
	{\includegraphics[width=0.45\textwidth]{Chapter_results/media/cloud_rank_t0}\label{fig:cloud_rank_t0}}
	\hfill
	\subfloat[\( t = 0.2 s\)]
	{\includegraphics[width=0.45\textwidth]{Chapter_results/media/cloud_rank_t0_2}\label{fig:cloud_rank_t0_2}}
	\caption{Complex case rank: As the mesh is refined load imbalance can occur, and elements are sent from one \acrshort{acr:GPU} to another when load balancing. (a) Start of computation (b) After the wave and cloud collide}\label{fig:cloud_rank}
\end{figure}

\section{Complex Meshes}\label{section:results:complex_meshes}

% explain case
% explain what it shows
% add profiling table

\begin{figure}[H]
	\centering
	\subfloat[Circular domain]
	{\includegraphics[width=0.45\textwidth]{Chapter_results/media/airfoil_mesh_far}\label{fig:complex_mesh_far}}
	\hfill
	\subfloat[Airfoil]
	{\includegraphics[width=0.45\textwidth]{Chapter_results/media/airfoil_mesh_near}\label{fig:complex_mesh_near}}
	\caption{Complex mesh: An airfoil in an unstructured circular domain. (a) Complete domain (b) Up close}\label{fig:complex_mesh}
\end{figure}

\begin{figure}[H]
	\centering

	\subfloat[Pressure]
	{\includegraphics[width=0.45\textwidth]{Chapter_results/media/airfoil_pressure_near_t1_5}\label{fig:complex_mesh_solution_close}}
	\hfill
	\subfloat[Pressure \(\sigma \)]
	{\includegraphics[width=0.45\textwidth]{Chapter_results/media/airfoil_pressure_sigma_near_t1_5}\label{fig:complex_mesh_solution_far}}
	
	\smallskip

	\subfloat[Split level]
	{\includegraphics[width=0.45\textwidth]{Chapter_results/media/airfoil_split_level_t1_5}\label{fig:complex_mesh_split_level}}
	\hfill
	\subfloat[N]
	{\includegraphics[width=0.45\textwidth]{Chapter_results/media/airfoil_N_t1_5}\label{fig:complex_mesh_N}}

	\caption{Complex mesh: A wave going over an airfoil. (a) Pressure (b) Pressure \(\sigma \), denoting if h-refinement or p-refinement would be chosen (c) Split level, denoting how many times elements have split (d) Polynomial order \(N\)}\label{fig:complex_mesh_solution}
\end{figure}
