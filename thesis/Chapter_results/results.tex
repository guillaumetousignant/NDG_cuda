\chapter{Results} \label{chapter:results}
% Setup, supercomputers etc
% Profiler
% Memory usage maybe?
% Stream performance
% Mixed precision maybe?
% Hybrid solver?
% Different block sizes!!

Following are results studying the effectiveness of the program. Three main components are studied.
First, the performance of the core spectral element method code on GPUs, with a comparison with the
same code running on CPUs is presented in Section~\ref{section:results:scaling_tests}. Then, the
performance of the adaptive mesh refinement is studied in
Section~\ref{section:results:adaptivity_performance}. Finally, dynamic load balancing is studied in
Section~\ref{section:results:load_balancing_performance}.

\section{Platforms} \label{section:results:platforms}

This program aims to perform large scale computations, and as such targets HPC platforms. The
program has been designed to scale to multiple parallel level, the highest being splitting the
workload between several HPC nodes, each having multiple CPUs and GPUs. To test this use case,
most of the following tests were run on clusters graciously offered by Compute
Canada\footnote{https://www.computecanada.ca/}. Two such clusters were used for our experiments,
Béluga and Narval.

\subsection{Béluga} \label{subsection:results:platforms:beluga}

Béluga is a heterogeneous cluster suitable for a variety of purposes, from traditional CPU computing
to massively parallel GPU computing. It is located at the École de technologie supérieure in
Montréal. Béluga is made up of 977 compute nodes of different types, totaling 39,120 CPU cores and
696 GPUs. For our testing, we use Béluga's GPU nodes. Each one of the 172 GPU nodes contains 40 CPU
cores and 186 GB of memory across two Intel Gold 6148 CPUs, and four NVidia V100 SXM2 GPUs with 16
GB of memory each. One such node totals about 2 TFlops of double precision CPU computing power, and
13 TFlops of double precision GPU computing power. The nodes are connected by Infiniband HDR (100
Gb/s) interconnections.

\subsection{Narval} \label{subsection:results:platforms:narval}

Narval is a heterogeneous cluster suitable for a variety of purposes, from traditional CPU computing
to massively parallel GPU computing and artificial intelligence workloads. It is located at the
École de technologie supérieure in Montréal. Narval is made up of 1301 compute nodes of different
types, totaling 80,720 CPU cores and 636 GPUs. For our testing, we use Narval's GPU nodes. Each one
of the 159 GPU nodes contains 48 CPU cores and 498 GB of memory across two AMD Milan 7413 CPUs, and
four NVidia A100 GPUs with 40 GB of memory each. One such node totals about 2.2 TFlops of double
precision CPU computing power, and 38.8 TFlops of double precision GPU computing power. The nodes
are connected by Infiniband EDR (100 Gb/s) interconnections.

\subsection{Consumer hardware} \label{subsection:results:platforms:consumer}

Some smaller scale tests or tests that didn't involve performance were run on consumer hardware.
This is important, as it may not always be possible to access HPC systems when flow simulations have
to be performed. The program should also have reasonable performance on regular systems. The system
used in these cases is a single computer containing 16 CPU cores and 128 GB of memory across two
Intel E5-2650 V2 CPUs, and one NVidia GTX 1070 GPU with 8 GB of memory. This totals about 0.5 TFlops
of double precision CPU computing power, and 0.25 TFlops of double precision GPU computing power.
This is an interesting comparison, as consumer GPUs are not geared towards high precision compute
loads. GPUs found in HPC systems typically have a 1:2 ratio between their double precision and
single precision performance, whereas consumer GPUs usually have 1:32 ratio. This means that
computations using double precision floating point numbers will execute at one sixteenth of the
speed of compute-oriented GPUS, all other factors being equal.

\section{Test case} \label{section:results:test_case}
% new case

\begin{figure}[H]
	\centering
	\includegraphics[width=0.6\textwidth]{Chapter_results/media/problem_1}
	\caption{Test case: A wave travels through a square domain at a 45° angle.}
	\label{fig:problem}
\end{figure}

\section{Scaling tests} \label{section:results:scaling_tests}
% Scaling tests (strong and weak), N tests
% CPU vs GPU
% GPU loading

Parallel scaling is an important aspect of the performance of a program. It describes how the
execution time varies when more resources are assigned to the problem. This is an optimisation
problem, as each problem will have an optimal amount of resources dedicated to it. 

A completely parallelisable program, often called embarrassingly parallel, should scale linearly
with the amount of resources. The speedup $S$ of a workload split between $W$ workers should be
equal to the number of workers in Equation~\ref{equ:scaling}. In this work, workers are a single GPU
or a single CPU core for comparative tests.

\begin{equation} \label{equ:scaling}
	S = \frac{t_1}{t_W}
\end{equation}

Unfortunately, it is often not possible to make a program entirely parallel. In such cases, dividing
a problem into more and more smaller tasks obeys the well-known Amdahl's law~\cite{Amdahl1967}, as
seen in Equation~\ref{equ:strong_scaling}. It splits the program into a sequential part $s$ and a parallel
part $p$, with only the parallel part scaling with the number of workers.

\begin{equation} \label{equ:strong_scaling}
	S = \frac{1}{s + \frac{p}{W}}
\end{equation}

This is called \textit{strong scaling}, where a fixed problem is solved using varying amount of
resources. This is discussed in Subsection~\ref{subsection:results:scaling_tests:strong}.

On the other hand, increased available resources can enable working on bigger problems. Gustafson
argues that modern highly parallel computers will be used to solve more complex problems instead of
solving smaller problems faster. Such a scaling is described with Equation~\ref{equ:weak_scaling}.

\begin{equation} \label{equ:weak_scaling}
	S = s + p \times W
\end{equation}

This is called \textit{weak scaling}. It describes solving a problem whose size increases with the
amount of resources, with the task size per worker stays constant. Weak scaling is tested in
Subsection~\ref{subsection:results:scaling_tests:weak}. With weak scaling a program will scale
indefinitely, with the slope of the scaling being influenced by the parallel proportion of the task.

This section will also serve as a general comparison between CPUs and GPUs. All scaling tests have
been performed on the Béluga supercomputer. Scaling is evaluated by varying the number of
\textit{nodes}, where a node in this context is a single computer from Béluga, sporting 40 CPU cores
and 4 GPUs, as stated in Subsection~\ref{subsection:results:platforms:beluga}. The four-node
execution time of those tests therefore represents the program running on 16 GPUs or 160 CPU cores,
for the GPU and CPU computations respectively.

Both scaling tests evaluate the core solver part of the program, as the meshes start with enough
elements to have a well resolved solution all along the simulation. The adaptive mesh refinement and
dynamic load balancing subroutines are still enabled. The subroutines still run their evaluation
phases, the error estimation and load imbalance computation respectively, but do not deem the
estimated error and load imbalance high enough to trigger the whole subroutine. 

\subsection{Strong scaling} \label{subsection:results:scaling_tests:strong}

The test case for this section is described in Section~\ref{section:results:test_case}. It is
divided into 256 elements in the $x$ and $y$ direction, for a total of $K = 65536$. The problem is
then solved in parallel from one quarter node to eight nodes. One quarter node contains 1 GPU and 10
CPU cores, while eight nodes contain 16 GPUs and 160 CPU cores. The mesh is split into blocks
accordingly, one block per GPU or one block per CPU core. The first test has a polynomial order $N$
of four for all elements, and a block size $B$ of 32. 

The block size, as explained in
Subsection~\ref{subsection:graphics_processing_units:architecture:programming_model}, is the number
of threads in a block of threads. Having a higher block size can speed up execution by reducing the
number of instructions to dispatch by the control flow unit of the GPU, because the instructions are
dispatched by block of threads. However, if the threads within a block diverge, having a higher
block size can actually lower performance. A single thread diverging will have all the threads in
its block waiting until it is finished, therefore a higher block size will have more threads stalled
when divergence occurs.

All tests have an "ideal" line both for the GPU and CPU calculations. This line represents the
optimal split for that case, with the number of nodes with the best scaling, extrapolated linearly
to the other splits.

\begin{figure}[H]
	\centering
	\includesvg[width=0.6\textwidth]{Chapter_results/media/strong_scaling_N4_K65536_W32}
	\caption{Weak scaling: The problem size increases with the number of workers. N = 4, K = 65536, B = 32}
	\label{fig:strong_scaling_N4_W32}
\end{figure}

Figure~\ref{fig:strong_scaling_N4_W32} shows that the GPU implementation, initially slower than the
CPU implementation, overtakes it when one full node or more is used. The higher number of nodes show
the GPU implementation to be $2.9 \times$ faster that the CPU implementation.

At low numbers of GPUs working on the same problem, the GPUs are more heavily loaded. It is possible
that the higher memory usage, or the increased cache eviction rate lowers the performance of GPUs in
that regime. With a lower number of elements being worked on by a GPU, cache has to be emptied less
often to make room for more elements to use for computations.

At the end of the curve, we see that the scaling lowers slightly. This could be a hint that the GPUs
are then not loaded enough. These GPUs have many CUDA cores, 5120 each in the case of Béluga. If the
workload does not saturate the GPU, some of those cores will be idle, leading to worse performance.
The optimal number of elements per GPU seems to be around 4096.

The next test, shown on Figure~\ref{fig:strong_scaling_N6_W32}, shows the same test case but with an
increased polynomial order of $N = 6$ for all elements. The rationale is that the GPU should spend
more time on the computations, since the computations will be heavier, while spending the same time
on the overhead of kernel calls and other housekeeping.

\begin{figure}[H]
	\centering
	\includesvg[width=0.6\textwidth]{Chapter_results/media/strong_scaling_N6_K65536_W32}
	\caption{Weak scaling: The problem size increases with the number of workers. N = 6, K = 65536, B = 32}
	\label{fig:strong_scaling_N6_W32}
\end{figure}

Figure~\ref{fig:strong_scaling_N6_W32} shows that the increased polynomial order has not increased
the performance of the GPU implementation compared to the CPU implementation, rather shifting to the
right the crossover point where the GPU implementation is faster. It is possible that the reduced
relative performance is linked to the increased memory used by the solution, as the needed storage
is relative to $2 N + 1$, where $N$ is the polynomial order of elements.

Next, this last case is computer again with increased block size $B$, first 64 on
Figure~\ref{fig:strong_scaling_N6_W64}, then 128 on Figure~\ref{fig:strong_scaling_N6_W128}. The CPU
implementation is not modified. This should assess the incidence of block size on the solution time.

\begin{figure}[H]
	\centering
	\includesvg[width=0.6\textwidth]{Chapter_results/media/strong_scaling_N6_K65536_W64}
	\caption{Weak scaling: The problem size increases with the number of workers. N = 6, K = 65536, B = 64}
	\label{fig:strong_scaling_N6_W64}
\end{figure}

\begin{figure}[H]
	\centering
	\includesvg[width=0.6\textwidth]{Chapter_results/media/strong_scaling_N6_K65536_W128}
	\caption{Weak scaling: The problem size increases with the number of workers. N = 6, K = 65536, B = 128}
	\label{fig:strong_scaling_N6_W128}
\end{figure}

A the last two figures suggest, increasing the block size $B$ does incur a performance penalty. This
is possible caused by a high amount of thread divergence within blocks. This is consistent with how
the program is written, for example Section~\ref{section:adaptive_mesh_refinement:implementation}
shows how the projection back and forth between the elements and faces have a low of branching code
paths. A lot of changes in the architecture of the program would have to be made to reduce this
branching, and switch to a more data-driven approach.

One thing to note is that the curve of the CPU implementation on
Figures~\ref{fig:strong_scaling_N4_W32} and~\ref{fig:strong_scaling_N6_W32} is similar, whereas the
GPU curves from all the figures differ significantly. This is a recurring theme of all the tests in
this chapter, code running on GPUs seems to be harder to optimise and some factors have unexpected
influences on the performance.

\subsection{Weak scaling} \label{subsection:results:scaling_tests:weak}

The weak scaling test uses the text case from Section~\ref{section:results:test_case}. The domain is
split up into elements such that each node contains 16384 elements. This amounts to 4096 elements
per GPU, or 410 elements per CPU core. The simulation is first computed with the highest number of
nodes, therefore the smallest elements and time step size. The CFL number is adjusted for all other
simulations to get the same time step size, therefore the same total number of iterations. An ideal
result is a flat curve, indicating that by increasing the problem size and resources by the same
amount, the simulation time stays the same. The problem is studied from ¼ node to 16 nodes.

\begin{figure}[H]
	\centering
	\includesvg[width=0.6\textwidth]{Chapter_results/media/weak_scaling_N4_K4096_W32}
	\caption{Weak scaling: The problem size increases with the number of workers. N = 4, K = 16384/node}
	\label{fig:weak_scaling}
\end{figure}

Figure~\ref{fig:weak_scaling} shows good weak scaling from the GPU implementation. Keeping in mind
that the quarter node and single node cases have a reduced workload by not having to do any MPI
communication over the network, and the quarter node GPU case has no MPI communication to do at all,
the GPU curve is reasonably flat. The CPU curve is not as good, indicating that there may be a
bottleneck in the communication part of the program. The CPU implementation has a much higher number
of workers than the GPU one, 40 per node instead of four, increasing the inter-process communication
needed. The slope of the CPU curve is about $0.2$ with regards to the number of CPU cores. 

\section{Adaptive mesh refinement performance} \label{section:results:adaptivity_performance}
% Error adaptive vs non-adaptive, time to run
% Error at same runtime

\begin{figure}[H]
	\centering
	\includesvg[width=0.6\textwidth]{Chapter_results/media/adaptivity_N4_K16_C5}
	\caption{Adaptivity efficiency: Simulation time and analytical error with adaptivity and increasing pre-processing steps. N = 4, K = 16, adaptivity interval = 5}
	\label{fig:adaptivity_efficiency_C5}
\end{figure}

\begin{figure}[H]
	\centering
	\includesvg[width=0.6\textwidth]{Chapter_results/media/adaptivity_N4_K16_C20}
	\caption{Adaptivity efficiency: Simulation time and analytical error with adaptivity and increasing pre-processing steps. N = 4, K = 16, adaptivity interval = 20}
	\label{fig:adaptivity_efficiency_C20}
\end{figure}

\begin{figure}[H]
	\centering
	\includesvg[width=0.6\textwidth]{Chapter_results/media/adaptivity_N4_K16_C100}
	\caption{Adaptivity efficiency: Simulation time and analytical error with adaptivity and increasing pre-processing steps. N = 4, K = 16, adaptivity interval = 100}
	\label{fig:adaptivity_efficiency_C100}
\end{figure}

\begin{figure}[H]
	\centering
	\includesvg[width=0.6\textwidth]{Chapter_results/media/adaptivity_N4_K16_C500}
	\caption{Adaptivity efficiency: Simulation time and analytical error with adaptivity and increasing pre-processing steps. N = 4, K = 16, adaptivity interval = 500}
	\label{fig:adaptivity_efficiency_C500}
\end{figure}

\section{Dynamic load balancing performance} \label{section:results:load_balancing_performance}
% Runtime adaptive with and without load balancing

\begin{figure}[H]
	\centering
	\subfloat[Pressure]
	{\includegraphics[width=0.45\textwidth]{Chapter_results/media/problem_big} \label{fig:load_imbalance_case_p}}
	\hfill
	\subfloat[Split level]
	{\includegraphics[width=0.45\textwidth]{Chapter_results/media/split_level_big} \label{fig:load_imbalance_case_s}}
	\caption{Load imbalance test case: A wave passes through a very big domain, only the bottom right refines. (a) Pressure (b) Split level, indicating how many times the elements have split}
	\label{fig:load_imbalance_case}
\end{figure}

% Table of imbalance

\subsection{Load balancing interval} \label{subsection:results:load_balancing_performance:interval}

\begin{figure}[H]
	\centering
	\includesvg[width=0.6\textwidth]{Chapter_results/media/load_balancing_interval_N4_K16384_A20_P16_S3}
	\caption{Load balancing efficiency interval test: Simulation time with adaptivity, load balancing and increasing load balancing interval. N = 4, K = 16384, adaptivity interval = 20, W = 16, max split level = 3}
	\label{fig:load_balancing_efficiency_interval}
\end{figure}

\subsection{Load balancing threshold} \label{subsection:results:load_balancing_performance:threshold}

\begin{figure}[H]
	\centering
	\includesvg[width=0.6\textwidth]{Chapter_results/media/load_balancing_threshold_N4_K16384_A20_L20_P16_S3}
	\caption{Load balancing efficiency threshold test: Simulation time with adaptivity, load balancing and increasing load balancing threshold. N = 4, K = 16384, adaptivity interval = 20, load balancing interval = 20, W = 16, max split level = 3}
	\label{fig:load_balancing_efficiency_threshold_s3}
\end{figure}

\begin{figure}[H]
	\centering
	\includesvg[width=0.6\textwidth]{Chapter_results/media/load_balancing_threshold_N4_K16384_A20_L20_P16_S5}
	\caption{Load balancing efficiency threshold test: Simulation time with adaptivity, load balancing and increasing load balancing threshold. N = 4, K = 16384, adaptivity interval = 20, load balancing interval = 20, W = 16, max split level = 3}
	\label{fig:load_balancing_efficiency_threshold_s5}
\end{figure}

\subsection{Polynomial order influence} \label{subsection:results:load_balancing_performance:polynomial_order}

\begin{figure}[H]
	\centering
	\includesvg[width=0.6\textwidth]{Chapter_results/media/N_iteration_time}
	\caption{Polynomial order influence: Iteration time with increasing polynomial order.}
	\label{fig:N_influence}
\end{figure}

\section{Complex meshes} \label{section:results:complex_meshes}

\begin{figure}[H]
	\centering
	\subfloat[Circular domain]
	{\includegraphics[width=0.45\textwidth]{Chapter_results/media/airfoil_mesh} \label{fig:complex_mesh_close}}
	\hfill
	\subfloat[Airfoil]
	{\includegraphics[width=0.45\textwidth]{Chapter_results/media/airfoil_mesh_close} \label{fig:complex_mesh_far}}
	\caption{Complex mesh: An airfoil in an unstructured circular domain. (a) Complete domain (b) Up close}
	\label{fig:complex_mesh}
\end{figure}

\begin{figure}[H]
	\centering
	\subfloat[Pressure]
	{\includegraphics[width=0.45\textwidth]{Chapter_results/media/airfoil_pressure_close_t1_6} \label{fig:complex_mesh_solution_close}}
	\hfill
	\subfloat[Pressure $\sigma$]
	{\includegraphics[width=0.45\textwidth]{Chapter_results/media/Airfoil 4} \label{fig:complex_mesh_solution_far}}
	\caption{Complex mesh: A wave going over an airfoil. (a) Pressure (b) Pressure $\sigma$, denoting if h-refinement or p-refinement would be chosen}
	\label{fig:complex_mesh_solution}
\end{figure}
